[
  {
    "id": "ai/맥에서-직접-ai-모델-돌려보자-ollama로-llm-서버-구축하기",
    "slug": "ai/맥에서-직접-ai-모델-돌려보자-ollama로-llm-서버-구축하기",
    "title": "맥에서 직접 AI 모델 돌려보자! Ollama로 LLM 서버 구축하기",
    "excerpt": "",
    "content": "1. 개요 최근 AI 관련 서비스를 많이 사용하면서 비용 부담이 커지는 경우가 많다. 이에 따라 로컬 환경에서 직접 을 구축하여 활용하는 방법을 소개하고자 한다. 이번 글에서는 macOS에서 을 설치하고 활용하는 방법을 다룬다. 2.  설치 2.1  설치 및 확인 우선 를 설치한다. 는 로컬에서 손쉽게 을 실행할 수 있도록 도와주는 도구이다. 설치 방법 웹 사이트에 가서 원하는 OS에 맞게 다운로드해서 설치를 진행하면 된다. 터미널에서 명령어로 설치하고 싶은 경우에는 로 설치하면 된다. 설치가 완료되면 올바르게 설치되었는지 확인한다. Ollama 구동해보기 설치가 완료되면 이제 모델을 다운로드하여 실행할 수 있다.   prompt에서 원하는 내용을 입력하면 답변을 주는 것을 볼 수 있다. 3.  사용해보기 3.1 Open WebUI CLI 환경에서만 사용하면 불편할 수 있으므로 Open WebUI를 활용할 수도 있다. Open WebUI는 를 쉽게 사용할 수 있는 웹 기반 UI이다. 설치 방법 여러 방법이 있지만, 간단하게 도커로 실행해본다. 이제 웹 브라우저에서 http://localhost:3000에 접속하면 를 웹 UI에서 사용할 수 있다. ChatGPT 웹 인터페이스와 비슷하고 잘 동작하는 걸 볼 수 있다. 3.2 API로 호출 Ollama는 API를 제공하므로, curl을 사용하여 직접 호출할 수도 있다. 3.3 Python으로 호출해보기 Python 에서는 langchain library를 제공하고 있어서 이걸 이요해서 을 사용할 수 있다. 4. Ollama 명령어 모음  명령어는 다양한 명령어를 제공하고 자주 사용하는 명령어은 다음과 같다. - 모델 리스트 확인    - 새 모델 추가   - 다운로드 가능한 모델은 library 사이트에서 확인할 수 있다.    - 모델 삭제    - 로컬에서 직접 모델 실행    5. 마무리 이제 macOS 환경에서 을 구축하고 활용하는 방법을 배웠다. 와 Open WebUI를 활용하면 로컬에서 효율적으로 AI 모델을 운영할 수 있으므로 직접 활용해보는 것을 추천한다. 6. 참고 - LLM을 local에서 돌려보자\n- Ollama 를 활용해서 개인형 LLM 서버 구성하기\n- 오픈소스 LLM 활용\n- https://github.com/open-webui/open-webui\n- https://github.com/ollama/ollama",
    "category": "ai",
    "tags": [
      "AI",
      "ollama",
      "LLM",
      "ChatGPT",
      "llama"
    ],
    "date": "2025-03-23T00:00:00.000Z"
  },
  {
    "id": "algorithm/algorithm-2개의-array에서-common-value-찾기",
    "slug": "algorithm/algorithm-2개의-array에서-common-value-찾기",
    "title": "Algorithm 2개의 array에서 common value 찾기",
    "excerpt": "",
    "content": "1. Problem 2개의 array에서 common value값을 찾아 결과를 반환하는 문제입니다. 메서드 정의는 아래와 같이 2개의 array를 받고 결과를 Set으로 반환합니다. 1.1 입력 / 결과 간단한 입력과 결과 예제입니다. 반환 결과에서는 중복된 값은 포함되지 않습니다. - [1, 1, 1, 1, 2, 2] & [3, 3, 4, 1, 2] -> [1,2]\n- [2, 7, 1, 4, 5, 6, 9, 8, 7] & [4, 6, 8, 2, 3, 5, 3, 1] -> [4, 6, 8, 2, 5, 1] unit test를 미리 작성해서 쉽게 실행하면서 로직을 짜는게 좋겠죠. 2. Solution 2.1 Approach 이 문제를 가장 쉽게 푸는 방법은 첫번째 Array의 각 요소가 두번째 Array에 존재 하는지 확인하고 있으면 결과에 추가하면 되는 문제입니다. 하지만, 이런 알고리즘의 복잡도는 O(n2)가 됩니다. 최소 O(n)으로 풀수 있는 방법은 없을 까요? 생각해보면 쉽습니다. HashTable 데이터 구조를 이용해서 Lookup 타임을 O(1)으로 하면, 전체 복잡도는 O(n)이 됩니다.\n알고리즘은 다음과 같습니다. - 두번째 array를 hashtable로 만든다 —> O(n)\n- 첫번째 array의 요소가 hashtable에 있는지 확인하고 있으면 결과값에 넣는다 —> O(n) 전체 소스코드는 github 를 참조해주세요.\n감사합니다. 3. Reference - https://codereview.stackexchange.com/questions/189504/finding-common-elements-in-two-arrays",
    "category": "algorithm",
    "tags": [
      "array",
      "common,",
      "알고리즘",
      "인터뷰",
      "면접",
      "코드면접",
      "배열",
      "공통값"
    ],
    "date": "2018-07-29T00:00:00.000Z"
  },
  {
    "id": "algorithm/algorithm-괄호-기호가-Valid한지-체크하기",
    "slug": "algorithm/algorithm-괄호-기호가-Valid한지-체크하기",
    "title": "Algorithm 괄호 기호가 Valid한지 체크하기",
    "excerpt": "",
    "content": "1. Problem\n괄호 기호가 OPEN, CLOSE 매칭이 제대로 되도록 확인하는 코드 문제입니다. 1.1 입력 / 결과\n입력 가능한 String 값은 아래와 같습니다. - ()()() —> true\n- )( —> false\n- ((()))()() —> true 2. Solution 2.1 Approach 1 이 문제를 쉽게 해결하는 방법은 스택 자료구조를 이용하는 것입니다.\n기본 아이디어는 다음과 같습니다. 1. String의 한 char씩 스킨한다\n1. OPEN\\괄호 ‘(‘ 을 만나면 스택에 push하고\n1. CLOSE\\괄호 ‘)’를 만나면 스택에서 pop을 한다.\n1. 스택에 아무것도 남아 있지 않으면, valid한 괄호인것으로 판단할 수 있다 소스코드는 github 에서도 확인할 수 있습니다. 3. Reference - https://hongku.tistory.com/251",
    "category": "algorithm",
    "tags": [
      "알고리즘",
      "인터뷰",
      "면접",
      "코드면접",
      "괄호"
    ],
    "date": "2018-07-29T00:00:00.000Z"
  },
  {
    "id": "algorithm/algorithm-정수값에서-1이-설정된-bit를-카운트하기",
    "slug": "algorithm/algorithm-정수값에서-1이-설정된-bit를-카운트하기",
    "title": "Algorithm 정수값에서 1이 설정된 bit를 카운트하기",
    "excerpt": "",
    "content": "1. Problem 정수값에서 1인 비트를 카운트하는 문제입니다. 1.1 입력 / 결과 - 7 : 111 —> 3\n- 23 : 10111 —> 4\n- 13 : 1101 —> 3 2. Solution 2.1 Approach 1 컴퓨터 공학과 수업 중에 assembly를 다루는 과목은 꼭 필수로 들었던 기억이 납니다. 매우 오래전 얘기긴 하지만, assembly로 과제를 하면서 자연스럽게 비트 연산을 익혔던 것 같습니다.\n다시 문제를 풀려고 하니, 솔직히 기억은 나지 않네요. 그래도 AND, OR만 알아도 쉽게 풀 수 있는 문제들이 많이 있습니다. 이진수에서 1이 있는지 확인하려면 맨 끝자리에서 1과 AND 연산하면서 비트를 카운트하면 쉽게 카운트를 할 수 있습니다. 이 알고리즘은 정수 값이 0이 될때까지 반복하기 때문에 복잡도는 O(n)이 됩니다. 이것보다 더 빠른 방법은 없을 까요?\n브라이언 커니핸 교수님이 고안한 알고리즘은 O(log n)으로 비트를 카운트 할 수 있습니다. 2.2 Approach 2 - Brian Kernighan’s Algorithm 처음 들어보신 분도 계실 수 있지만, 브라이언 커니핸 ( Brian Kernighan )이란 분은 초창기 UNIX를 개발하셨고 유닉스에서 자주 사용하는 AWK 명령어도 개발하신 분이십니다. 2000년 이후부터 쭉 프린스턴 대학교에서 교수직으로 일하고 계십니다. 구현은 쉽지만, 이걸 생각해냈다는 게 정말로 대단한 것 같습니다. 기본 알고리즘을 알아보죠. - N = N AND (N-1)를 정수값이 0이 될때까지 반복한다.\n- 반복하는 카운트를 세면 1인 비트를 얻을 수 있다. 알고리즘은 간단한데 어떻게 이런 결과가 되는지 스텝으로 알아보죠. 이 방법 외에도 다양한 알고리즘은 더 많이 존재 합니다 (참고: Bit Twiddling Hacks ). 누군가 정말로 열심히 연구하고 공부한 것 같아요. 3. Reference - https://www.geeksforgeeks.org/count-set-bits-in-an-integer/\n- https://www.quora.com/How-do-you-count-the-number-of-1-bits-in-a-number-using-only-bitwise-operations\n- Bit Twiddling Hacks\n    - https://graphics.stanford.edu/seander/bithacks.html",
    "category": "algorithm",
    "tags": [
      "알고리즘",
      "인터뷰",
      "면접",
      "코드면접",
      "비트",
      "카운트"
    ],
    "date": "2018-10-28T00:00:00.000Z"
  },
  {
    "id": "cloud/argo-cd",
    "slug": "cloud/argo-cd",
    "title": "Argo CD",
    "excerpt": "",
    "content": "Argo Projects 그전 발표내용은 여기를 참고해주세요. Argo CD? What? Argo CD는 GitOps 기반의 CD 도구이고 다음과 같은 여러 기능을 제공하고 있다 Feature - 타겟 환경(Git 저장소에 지정된 대로)에 application 자동 배포 지원\n- 쿠버네티스 manifest 파일을 생성해주는 여러 템플릿 포맷을 지원\n    - \n- Pull deployment 방식를 지원\n    - Argo CD는 k8s manifest의 변경을 pull하는 방식\n- 여러 클러스터를 관리하고 배포하는 기능\n- SSO 통합 인증 지원 (OIDC, OAuth2, LDAP, SAML 2.0, GitHub, GitLab, Microsoft, LinkedIn)\n- Application 자원의 Health 상태 지원\n- Web UI, CLI 지원\n- 웹훅 통합 지원 (GitHub, BitBucket, GitLab)\n- 복잡한 응용 프로그램 Rollout을 지원하기 위한 Presync, Sync, Postsync hook도 지원 Architecture Argo CD는 3가지 컨포넌트로 이루어져 있다. Argo CD가 하는 역할은 다음과 같다. - 실행 중인 애플리케이션을 지속적으로 모니터링\n- 현재 라이브 상태를 원하는 대상 상태(Git 저장소에 지정된 대로)와 비교를 주기적으로 한다\n    - 라이브 상태가 대상 상태와 다른 배포된 애플리케이션은 OutOfSync로 간주한다\n- Argo CD는 이런 차이점을 보고 UI 시각화\n- 라이브 상태를 원하는 대상 상태로 자동 또는 수동으로 다시 동기화 하는 기능을 제공 - API Server\n    - API 서버는 Web UI, CLI 및 외부에서 사용할 수 있도록 gRPC와 REST API를 제공한다\n- Repository Server\n    - Application manifest 파일을 가지고 있는 Git 저장소의 로컬 캐시를 유지 관리한다\n    - Git 저장소에 저장된 manifest 파일을 생성하는 역할을 한다\n- Application Controller\n    - 애플리케이션 컨트롤러는 실행중인 응용 프로그램을 지속적으로 모니터링\n    - live state와 target state를 비교해서 UI 상으로 알려준다 (OutOfSync) 참고 - https://argo-cd.readthedocs.io/en/stable/operator-manual/architecture/\n- https://landscape.cncf.io/card-mode?project=incubating&selected=argo When? - CD (Continuous Delivery) 도구로써 쿠너베티스 환경에 application을 자동 배포하는데 적합한 도구이다 Why? 기존 Jenkins와 Argo CD를 비교해보면서 쿠버네티스 환경에서는 어떤 도구가 더 적합한지 각자 판단해보세요. - Jenkins\n    - 를 jenkins 서버에 직접 설치해야 한다\n    - k8s cluster 접근하려면 credentials도 설정 필요하다\n    - 한번 배포되면 배포된 상태 값에 대해서 알 수 없다 (모니터링 기능 없음)\n    - Push deployment - Argo CD\n    - 대상 상태(Git 저장소)와 현재 라이브 상태를 주기적으로 모니터링하고 있어 배포가 필요한 application을 쉽게 알 수 있다\n    - 배포이후에 잘 배포가 되었는 지도 확인이 가능하다\n    - Pull deployment How? Argo CD를 설치하고 사용하려면 아래와 같은 절차로 진행하면 된다. 본 예제는 Argo CD 공식 문서를 참고하여 작성하였다. - k8s 클러스터에 Argo CD를 설치\n- Argo CD Application 생성\n    - Argo CLI\n    - ArgoCD Web UI\n    - kubernetes CRD - Docker 이미지를 버전업 시켜 Argo로 동기화 해보기 1.로컬환경에서 Argo CD 설치하기 로컬환경에서 를 설치하기 위해 을 실행한다. 명령어가 없는 경우에는 로 설치하세요. 를 생성한 이후 Argo CD 프로그램을 설치한다. 1.1 Argo CD Web 접속하기 쿠버네티스 서비스를 노출하지 않고 port forwarding하여 Web 서버에 접속해보자. admin 계정의 초기 암호는 자동으로 생성되어  시크린에서 base64 값으로 저장되어 있다.  명령어 사용하여 간단하게 암호를 확인한다. 2.Argo CD Application을 생성하기 2.1 Argo CLI으로 생성하기 Argo CLI로 application을 생성하려면 Argo CD CLI 설치가 필요하다. 2.1.1 Argo CD CLI 설치하기 CLI로 Argo를 생성하거나 조회 삭제도 할 수 있다. 먼저 으로 로그인을 한다. Web UI에서 로그인시 입력한 같은 id/password을 사용한다. > Argo 테스트하기 위해 Git 저장소에 파일을 수정하고 push하기 위해서 Argo 문서에서 제공하는 argocd-example-apps repository를 fork 하였습니다.\n>\n> https://github.com/kenshin579/analyzing-argocd-example-apps\n> - repo\n    - Argo CD에서 관리할 repository를 지정한다\n- path\n    - Repository에서 path로 application directory를 지정한다\n- dest-server\n    - 대상이 되는 쿠버네티이스 클러스터 URL를 지정한다\n- dest-namespace\n    - application을 생성할 대상 네이스페이스를 지정한다 2.1.1 Argo Application 확인 2.1.2 Sync Application OutOfSync인 경우 명령어로도 동기화를 할 수 있다 가 없어서 동기화 실패가 떨어졌다. 간단하게 해결하는 방법은 를 생성하고 다시 동기화 시키면 된다. Application 배포시 원하는 namespace를 자동으로 생성해주는 옵션도 존재를 한다. App Details 에서 Auto-Create Namespace 옵션을 클릭해서 저장후 Sync하면 argotest namespace에 k8s object 들이 생성된 것을 확인할 수 있다. 2.2 Web UI에서 생성하기 Applications > New App 버튼 클릭해서 생성할 수 있다. 2.3 kubenetes manifest 파일로 생성하기 3.Docker 이미지를 버전 업시켜 Argo로 동기화 해보기 Application 개발이후 새로운 도커 이미지를 생성했다면, Argo CD로 배포를 해보자. 3.1 Kubernetes config 설정 파일 수정 Git 저장소에서 쿠버네티스 설정 파일를 수정한다. 도커 이미지의 버전을 올려서 푸쉬를 하고 Argo CD에서 확인해보자. 3.2 Argo CD Web UI에서 동기화하기 Argo CD는 Git 저장소를 실시간으로 모니터링하지 않고 주기적으로 확인하기 때문에 UI 상에서는 바로  OutOfSync를 표시해주지 않는다. 바로 확인해고 싶은 경우 Refresh 버튼을 클릭하면 된다. Sync 버튼을 클릭하면 동기화 시킬 수 있다. FAQ 1. GitOps 기반의 다른 CD 도구는 뭐가 있나? FluxCD, JenkinsX 참고 - https://harness.io/blog/argo-cd-alternatives/\n- https://blog.container-solutions.com/fluxcd-argocd-jenkins-x-gitops-tools 2. Application 저장소에 Config 설정을 같이 관리하면 안되나? App과 config는 용도와 생명 주기가 다르기 때문에 Git 저장소를 분리하여 저장하는 걸 Best Practices로 추천하고 있다 참고 - https://kangwoo.kr/tag/argocd/\n- https://argo-cd.readthedocs.io/en/stable/user-guide/bestpractices/ Reference - https://argo-cd.readthedocs.io/en/stable/\n- https://ithub.tistory.com/345\n- https://blog.wonizz.tk/2020/06/08/kubernetes-deploy-tool-argocd/ Terms - CI (Continous Integration)\n- 개발자를 위한 자동화 프로세스인 지속적인 통합을 의미한다\n    - 지속적인 통합은 모든 개발을 완료한 뒤에 품질 관리를 적용하는 고전적인 방법을 대체하는 방법으로서 소프트웨어의 질적 향상과 소프트웨어를 배포하는데 걸리는 시간을 줄이는데 초점이 맞추어져 있다.\n    - ex. Jenkins, Github Action\n- CD (Continous Deployment)\n    - 개발자의 변경 사항을 리포지토리에서 고객이 사용 가능한 프로덕션 환경까지 자동으로 릴리스하는 것을 의미한다\n    - ex. Jenkins, Argo CD - CR (Custom Resource)\n    - 쿠버네티스에서 기본으로 제공하는 Object (ex. Service, Secret) 종류외에 사용자가 원하는 리소스를 정의해 사용할 수 있다\n    - 사용자가 정의한 CRD 기반으로 동작하는 custom controller 개발도 쉽게 할 수 있도록 쿠버네티스에서 인터페이스를 제공한다 - CRD (Custom Resource Definition)\n    - CRD는 CR의 데이터에 어떠한 항목이 정의되어 있는지 등을 선언한 Metadata Object이다\n    - 기존 쿠버네티스 manifest 파일처럼 YAML로 파일을 작성한다 - GitOps\n    - GitOps라는 개념은 Weaveworks가 처음 만든 용어이다\n    - Git을 통해 개발자에게 익숙한 방식으로 인프라 또는 어플리케이션의 선언적인 설정파일을 관리하고 배포하는 일련의 프로세스이다 참고 - CR/CRD\n    - https://blog.naver.com/PostView.naver?blogId=alicek106&logNo=221579974362&redirect=Dlo",
    "category": "cloud",
    "tags": [
      "argo",
      "argocd",
      "events",
      "workflow",
      "devops"
    ],
    "date": "2022-03-04T00:00:00.000Z"
  },
  {
    "id": "cloud/argo-projects",
    "slug": "cloud/argo-projects",
    "title": "Argo Projects",
    "excerpt": "",
    "content": "Argo Projects? Argo Project란 쿠버네티스 환경에서 application이나 job을 실행하거나 배포를 도와주는 일련의 쿠버네티스 도구 집합이다. 모든 Argo 프로그램은 와 사용자 쿠버네티스 클러스터로 구현되어 있다. 현재 4가지 대표 서브 프로젝트가 존재하고 각 프로그램은 독립적으로 사용할 수도 있지만, 함께 사용하면 더욱 강력한 도구가 되기도 한다. What? - \n    - 컨테이너 기반의 워크플로우 엔진         - Job 단위가 프로세스가 아닌 컨테이너 단위로 실행된다         - 다양한 실행 방식을 지원한다             - ex. sequence, parallel, with dependency w/ DAG, etc - \n    - 쿠버네티스를 위한 이벤트 기반 워크로플로우 자동화 프레임워크 도구         - 아래와 같은 다양한 Event와 Trigger를 제공하고 Event 발생시 Trigger하는 역할을 수행한다         - Events Source (20+):             - Github, NATS, File, NATS, MQTT, Slack, Webhooks, HDFS, K8s Resources, Kafka, Redis, etc         - Triggers (10+)             - Argo Workflow, Argo Rollouts, k8s Object, AWS Lambda, AWS Lamda, NATS message, Kafka message, Log, Slack Notification, etc - [](https://blog.advenoh.pe.kr/argo-cd/)\n    - 선언적인 GitOps 기반의 CD (Continuous Deployment) 도구 - \n    - Progress Delivery 를 지원하는 도구     - 여러 배포 방식을 지원한다     - ex. canary, blue/green, rolling updates, etc 참고 - https://github.com/terrytangyuan/awesome-argo Who? - Applatix 회사에서 Argo 를 만들고 cloud-native 개발자 커뮤니티에 오픈소스로 제공을 함\n- 2018년에 Intuit라는 회사가 Applatix를 인수를 함\n- 2020년 Argo 프로젝트가 CNCF 프로젝트 Incubator 프로젝트로 승인됨\n- 현재 여러 회사에 의해서 유지되고 있음 참고 - https://argoproj.github.io/\n- https://www.intuit.com/blog/innovation/cloud-native-computing-foundation-accepts-argo-as-an-incubator-project/\n- https://www.intuit.com/blog/innovation/welcome-applatix-to-the-intuit-team/\n- https://blog.argoproj.io/argo-goes-to-cncf-incubator-f0e9dfb40597 Where? - 180개 이상의 여러 회사에서 프로덕션에 적극적으로 사용하고 있음 - ex. Adobe, Alibaba Cloud, Data Dog, Datastax, Google, GitHub, IBM, MLB, NVIDIA, Red Hat, SAP, Tesla, Ticketmaster, 당근마켓, LINE Reference - https://argoproj.github.io/ Note > 본 내용은 저희 Platform Engineering 팀내 CNCF 스터디를 위해 준비한 자료입니다. 저희가 하는 로봇 플랫폼 개발에 관심이 있는 분들은 아래 링크를 참고해주시고 도전적이고 열정적으로 같이 일하실 분은 많이 지원해주세요.\n>\n> - 네이버는 왜 제2사옥 1784를 지었을 까요?  https://www.youtube.com/watch?v=WG7JHLfClEo\n> - 네이버 랩스 - https://www.naverlabs.com/",
    "category": "cloud",
    "tags": [
      "argo",
      "argocd",
      "events",
      "workflow",
      "cloud"
    ],
    "date": "2022-03-04T00:00:00.000Z"
  },
  {
    "id": "cloud/argocd-resource-hooks에-대해서-알아보자",
    "slug": "cloud/argocd-resource-hooks에-대해서-알아보자",
    "title": "ArgoCD Resource Hooks (PreSync, PostSync, SyncWaves)에 대해서 알아보자",
    "excerpt": "",
    "content": "1. 개요 > ArgoCD 란? 여기는 참고해주세요. 이번 포스팅에서는 ArgoCD Resource Hooks에 대해서 알아보자. Argo CD에서는 Sync 는 Git 리포지토리의 선언된 상태와 Kubernetes 클러스터의 실제 상태를 동기화하는 과정이다. 여기서 Resource Hook은 이러한 배포 프로세스 중 특정 시점에 추가적인 작업(, Sync,  등)을 실행하는 기능을 ArgoCD에서 제공을 해준다. 이를 통해 배포 전에 준비 작업 (ex. DB 스키마 마이그레이션)을 하거나 배포 후 검증 작업 (ex. notification)을 수행할 수 있다. ArgoCD에서 제공하는 Resource Hook은 다음과 같다. | Hook         | 설명                                                         |\n| ------------ | ------------------------------------------------------------ |\n|     | 매니페스트 적용 전에 실행된다                                |\n|        | 모든  훅이 완료되고 성공한 후, 매니페스트 적용과 동시에 실행된다 |\n|        | Argo CD에서 해당 매니페스트의 적용을 건너뛰도록 지정한다     |\n|    | 모든  훅이 완료되고 성공한 후, 성공적인 매니페스트 적용과 모든 리소스가 Healthy 상태인 경우에 실행된다 |\n|    | 동기화 작업이 실패했을 때 실행된다                           |\n|  | 모든 애플리케이션 리소스가 삭제된 후 실행된다 (v2.10 버전부터 사용 가능) | 2. Resource Hook 설정하는 방법 2.1 Resource Hooks 설정하기 어플리케이션에 Resource Hook 을 어떻게 적용할 수 있는지 알아보자. Resource Hook을 설정하는 방법은 간단한 어노테이션을 추가하면 된다. 다음은 에서 echo 명령어로 화면에 메시지를 출력하는 예제이다. 위  파일은 배포하려는 Application  폴더에 Job Resource로 작성하면 된다.  설정하는 방법도 유사하게 을 작성하면 된다. 아래 예제는 동기화가 실패 되었을 때 실행되는  Hook이다. > 이란? \n>\n> Kubernetes Job에서 실패한 작업을 재시도하는 횟수를 지정하는 속성이다. Job의 컨테이너가 실패할 경우, 지정된  값에 따라 재시도하며, 재시도 횟수가 이 값을 초과하면 Job은 실패한 것으로 간주된다.\n>\n> : 실패 시 두 번까지 재시도 하겠다는 의미이다 2.1.1 Hook 삭제 정책  어노테이션에 의해서 Hook이 어떻게 삭제될지 정할 수 있다. ArgoCD에서 제공하는 삭제 정책은 다음과 같다. 어노테이션을 지정하지 않은 경우에는 기본적으로 로 지정이 된다. | 정책                 | 설명                                                         |\n| -------------------- | ------------------------------------------------------------ |\n|       | 훅 리소스가 성공한 후 삭제된다 (예: Job/Workflow가 성공적으로 완료된 경우) |\n|          | 훅 리소스가 실패한 후 삭제된다                               |\n|  | 새로운 훅이 생성되기 전에 기존 훅 리소스가 삭제된다 (v1.3 버전부터 사용 가능). 이는 과 함께 사용하도록 설계된다 | 2.1.2 ArgoCD 실행 화면 실제 Sync를 실행하면 ArgoCD UI 화면에서는 아래와 같이 표시된다.  표시가 있는 블록이 생성이 되었고 클릭하고 LOGS에서 프로그램 화면에 출력된 값도 확인할 수 있다. , 로 실행된 Pod는 가 설정이 안되어서 삭제가 안되었다. 로 설명하면 실행후 ArgoCD나 Pod가 살아지기 때문에 실제 결과를 UI 상에서 확인할수가 없어서 삭제 정책 없이 실행하였다. 2.2 Sync Wave란? Sync Wave는 여러 리소스를 동기화할 때 실행 순서를 제어하는 기능이다. 예를 들어, 네트워크 설정 리소스가 먼저 적용된 후 애플리케이션이 배포되기를 원할 때, 각 리소스에 Sync Wave 값을 설정해 순차적으로 실행되게 할 수 있다. - 각 리소스는 기본적으로  을 가진다\n- Wave 번호가 낮은 리소스가 먼저 실행된다 > 여러 개의 Pre Job이 있는 경우 여러 개의  Job을 정의하는 경우, 각각의 Job에 대해 를 설정해 순서를 지정할 수 있다. 만약 동일한 Wave 값을 가진 Job이 여러 개 있으면, 병렬로 실행된다. 예를 들어 다음과 같이 설정할 수 있다. 위의 설정에서는 이 먼저 실행되고, 그 이후에 가 실행된다. 3. FAQ 3.1 가 실패가 되면 다음 Hook은 실행되지 않나? 가 실패하면 다음 Phase로 넘어가지 않고 멈춘다. 실제로 아래와 같이 강제로 실패가 떨어지게 하고 실행해 보면 , 도 실행이 안 되는 것을 확인할 수 있다. ArgoCD에서도 PreSync, PostSync가 실패로 표시되고, 마지막으로 SyncFail Hook이 실행된 것을 확인할 수 있다. 3.2  삭제 정책으로 지정을 하면 실행후 바로 삭제가 되어 결과를 확인할 수 없는데, 5분?뒤에 삭제할 방법은 없나?  어노테이션을 지정하지 않고  값으로 지정을 하면 600초 뒤에 삭제가 된다. >  란? \n> 는 Kubernetes의 Job이나 CronJob에서 사용되는 필드로, 작업이 완료된 후 해당 리소스가 삭제되기 전까지 남아있는 시간을 초 단위로 설정하는 옵션이다.\n>\n> 만약  필드를 설정하지 않으면, Job은 수동으로 삭제해야 하며 클러스터에 계속 남아있게 된다. > 를 적용했는데 삭제가 되지 않는 경우는?\n>\n> Kubernetes 버전에 따라서 동작하지 않을 수도 있다. 버전이 낮거나 기능 활성화되지 않아 동작하지 않을 수 있다.\n>\n> \n> 참고: JobSpec v1 batch (kubernetes v1.18) 3.3 실제 App 버전은 같아서 배포가 필요 없지만, 수동으로 , 를 할수는 없나?  버튼을 누르면 무조건  →  → 를 실행하게 되어 있어서 실제로 Hook이 실행이 된다 3.4 Presync가 실패가 되었는데, Deployment는 배포를 하는 경우에는 어떻게 하면 되나? Presync 실행 시 오류가 발생해서 Pod가 배포가 안된 경우에는 ArgoCD에서 deployment 블록을 선택해서 수동으로 시작하면 Pod를 배포할 수 있다.  3.5 무한 Syncing/Terminating을 계속 하고 있는 경우 강제로 terminate하는 방법은 없나? Application을 선택하고  버튼을 클릭해서 강제로 종료시킬 수 있다.  4. 마무리 Argo CD의 Resource Hook은 배포 과정에서 유연한 작업을 설정할 수 있는 강력한 도구이다다. 이를 통해 배포 이전, 배포 중, 배포 이후의 작업을 체계적으로 관리할 수 있으며, 복잡한 환경에서도 안정적인 배포를 구현할 수 있다. Hook과 Sync Wave를 적절히 활용하면 배포의 순서를 세밀하게 제어할 수 있으며, 이를 통해 팀 전체의 배포 속도와 안정성을 높일 수 있다. > 본 포스팅에서 작성한 내용은 argocd-charts-sample 여기에서 확인할 수 있다 5. 참고 - ArgoCD Resource Hooks – Explained\n- ArgoCD Phases\n- ArgoCD - Resource Hooks\n- Argo CD — Resource Hooks (PreSync, PostSync, Sync, and SyncFail) and Sync Waves",
    "category": "cloud",
    "tags": [
      "argocd",
      "argo",
      "kubernetes",
      "k8s",
      "sync",
      "hook",
      "resource hook",
      "prepost",
      "deploy",
      "gitops",
      "전후처리",
      "쿠버네티스",
      "배포"
    ],
    "date": "2024-10-21T00:00:00.000Z"
  },
  {
    "id": "cloud/aws에서-ec2로-api-서버-구축하기",
    "slug": "cloud/aws에서-ec2로-api-서버-구축하기",
    "title": "AWS에서 EC2로 API 서버 구축하기",
    "excerpt": "",
    "content": "API 서버를 구축하기 위해 사용할 수 있는 서비스는 아래와 같이 여러 서비스가 존재한다. - Heroku\n- GCP\n- PythonAnywhere\n- AWS (Amazon Web Service) 위 서비스들은 대부분 무료 플랜을 제공하고 있고 제한된 리소스와 기능을 제공한다. 개인적으로 여러 서비스 중에서 그래도 장기간 12개월간 무료로 사용할 수 있는 AWS를 선호한다. 자주 EC2를 구축하지는 않지만, EC2로 API를 구축하면 매번 구글링해서 세팅하는 과정이 시간이 걸린다. 이번에 stock-api 구축하면서 나중에 바로 참고할 수 있도록 블로그에 정리해둡니다. 1.서버 구축 사전작업 AWS 계정 생성하기 AWS 계정은 12개월 무료로 사용할 수 있지만, 이메일 주소로 계정을 생성해야 한다. 매번 새로운 이메일 주소를 생성하기보다는 구글의 별칙 기능을 사용하기를 추천한다. AWS 계정을 생성하고 콘솔에 로그인한다. 2. AWS에서 EC2 서버 구축하기 2.1 EC2 인스턴스 생성하기 AWS 서비스 중에 인스턴스를 찾아들어가 EC2 AWS에서 가상머신을 생성한다. 왼쪽 메뉴에서 인스턴스 > 인스턴스 시작 버튼을 클릭한다. 기본적으로 EC2 인스턴스 생성 시 기본값은 설정되어 있다. 사용자가 채워 넣어야 하는 값은 다음과 같다. - 이름: 원하는 이름을 넣는다 (ex. ) - 애플리케이션 및 OS 이미지     - 이미지: \n    - 아키텍처:  - 인스턴스 유형: t2 micro - 키 페어 (로그인):      - 키 페어는 나중에 EC2 인스턴스 생성후 ssh로 로그인하기 위해 필요한다\n    - 일단 새 키 페어 생성 클릭해서 생성하고 - 네트워크 설정     -  체크박스 클릭하고 를 선택하다\n    - 이 선택되어 있으면 누구나 로그인 시도를 할 수 있어 특정 네트워크나 네트워크 주소에서만 접근하도록 하는게 좋다 - 스토리지 구성 : \n    - 프리 티어는 최대 30GB까지 사용할 수 있어 으로 변경한다 2.1.1 키 페어 생성 생성하면  파일이 자동으로 다운로드된다. 키 페어는 인스턴스 생성 이후 접근할 때 사용하는 파일이다. 2.2 Elastic IP 설정하기 EC2 인스턴스를 재시작하게 되면 매번 새 IP가 할당된다. IP가 변경되면 PC에서 접근할 때마다 IP 주소를 확인해야 하는 번거로운지 존재한다. 매번 IP가 변경되지 않고 고정 IP를 할당받으려면 Elastic IP를 설정해야 한다.  >  >  >  버튼을 클릭한다. 아래와 같은 설정으로 할당한다.  >  >  >  >  선택한다. 생성한 EC2 인스턴스와 프라이빗 IP 주소를 입력한다. 2.3 EC2 서버에 ssh로 접속하기 위에서 다운로드한  파일을 가지고 ssh로 접근하려면 아래 명령어를 입력하면 된다. 2.3.1 ssh 옵션에서 PEM 파일 지정하기 IP 주소로 기억하기 쉽지 않고 매번 긴 명령어 입력을 해야 해서 ssh 설정 파일을 다음과 같이 설정하면 간단한 명령어로 접근이 가능하다. 2.3.1 ssh 설정에 미리 PEM 파일 및 서버 IP 주소 설정하기  파일을 폴더로 복사하고 권한을 변경한다.  파일을 생성해서 다음 정보를 입력한다. 이제 아래 명령을 입력하면 EC2 인스턴스에 로그인할 수 있다. 2.4. EC2 생성 후 EC2 인스턴스 추가 설정 2.4.1 타임존 변경 EC2 서버의 기본 타입 존은 UTC이다. 한국시간에 맞게 타임존을 변경한다. 2.4.2 Hostname 변경 여러 서버를 관리하고 있다면 IP 주소만으로는 어떤 서비스의 서버인지 확인이 어렵기 때문에  이름을 변경해준다. Amazon Linux AMI 2 이미지 기존으로 hostname을 변경하는 방법이다. 추가로 /etc/hosts에 hostname을 등록하면 등록한 hostname으로 접근하기 편한 부분이 있어 같이 수정을 해준다. IP 주소 대신 등록한 hostname을 사용해서 접근할 수 있다. 아직 서버에 서버를 띄우지 않아서 위 오류 메시지가 뜨지만, curl 호스트 이름으로 실행은 잘 된 것을 확인할 수 있다. 참고 - https://bbeomgeun.tistory.com/157 2.5 (옵션) EC2 보안 그룹에 추가 설정 EC2 인스턴스 생성 시 ssh 트래픽 허용하지 않았다면 보안 그룹에 추가로 설정을 해줘야 한다.  >  >  선택 >  >  선택 한다 맨 아래  버튼을 클릭해서 소스 유형 > 내 IP를 선택해서 내 컴퓨터에서만 접근되도록 한다. 3. EC2에 API 배포하기 3.1 Github 소스 코드 다운로드 ssh로 EC2 인스턴스에 접근하고 github 에서 소스 코드를 다운로드한다. 3.1 golang 설치 Echo server는 golang으로 작성이 되어 있어서 golang을 설치한다. 3.2 소스 코드 빌드 Makefile에 빌드 명령이 지정되어 있어 쉽게 make로 빌드한다. 3.3 Echo 실행하고 외부에서 테스트해보기 3.3.1 API 실행 3.3.2 외부에서 API 접근하기 먼저 EC2 공개 주소를 알아야 접근할 수 있기 때문에 EC2 인스턴스 세부 정보에서 확인한다.  >  > curl` 명령어로 API을 호출해보면 잘 되는 걸 확인할 수 있다. 끝!! 참고 - https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/go-devenv.html\n- https://ryanwoo.tistory.com/8\n- https://technoracle.com/how-to-install-git-on-amazon-linux-2/\n- https://chat.openai.com/chat\n- http://www.yes24.com/Product/Goods/83849117",
    "category": "cloud",
    "tags": [
      "aws",
      "ec2",
      "api",
      "server",
      "구축",
      "가상머신",
      "api",
      "heroku",
      "gcp",
      "amazon",
      "vm",
      "virtual",
      "rest"
    ],
    "date": "2023-03-18T00:00:00.000Z"
  },
  {
    "id": "cloud/docker-도커-명령어-모음",
    "slug": "cloud/docker-도커-명령어-모음",
    "title": "(Docker-1) Docker 도커 명령어 모음",
    "excerpt": "",
    "content": "1. 들어가며 도커를 다루는 데 있어서 크게 2가지 종류로 나뉠 수 있습니다. - 도커 이미지 다루기\n- 도커 컨테이너 다루기 도커 관련된 여러 명령어들이 많아서 자주 사용되는 명령어 위주로 정리를 해봤습니다. 전체 도커 명령어에 대한 내용은 도커 문서 사이트를 참고해주세요. 2. 도커 명령어\n2.1 도커 도움말 도커 도움말은 명령어 창에서 help로 확인할 수 있습니다. 도커를 다루는 데 있어서 여러 관리 명령어(서브 명령어)가 존재를 하고 해당 도움말도 아래 명령어로 자세하게 확인할 수 있습니다. 도커 볼륨 명령어 도움말은 다음과 같습니다. 2.2 도커 이미지 다루기 도커 컨테이너를 실행하기 전에 원하는 이미지를 검색하거나 새로운 이미지를 생성할 필요가 있습니다. 도커 이미지를 어떻게 다룰 수 있는지 알아보겠습니다. 2.2.1 도커 이미지 검색하기 Docker Hub 레지스트리에서 도커 이미지를 검색합니다. Redis 관련 이미지를 목록으로 보여줍니다. 검색된 결과는 깃허브의 스타 순으로 정렬해서 출력됩니다. 목록을 보면 대부분 ####/redis 앞에 네임스페이스가 붙지만, 첫 번째 아이템은 네임스페이스 없이 redis으로만 되어 있습니다. 공식 리포지토리의 네임스페이스는 library이고 이 네임스페이스는 생략될 수 있습니다. search 명령어로는 여러 버전의 redis를 검색할 수 없기 때문에 다음과 같은 API를 통해서 조회하면 됩니다. > jq 명령어는 JSON 포멧을 다룰 수 있는 명령어이고 맥에 설치되어 있지 않다면 brew install jq로 설치하면 된다 2.2.2 이미지 다운로드하기 도커 이미지를 도커 registry에서 내려받으려면 docker image pull 명령을 사용합니다. redis 이미지를 다운로드받으려면 다음과 같이 하면 됩니다. 태그 명을 생략하면 기본적으로 latest 태그가 적용됩니다. 2.2.3 이미지  빌드하기 docker image built 명령어로 도커 이미지를 생성하고 Dockerfile 파일에 정의된 내용에 따라서 이미지가 생성됩니다. - -t 옵션은 이미지명과 태그명을 같이 붙이는 옵션이다. 거의 필수 옵션으로 쓰인다 Dockerfile 파일을 보면 이미지 생성하는 인스트럭션이 나열되어 있습니다. | 인스트럭션 | 설명                                                         |\n| ---------- | ------------------------------------------------------------ |\n| FROM       | 지정한 이미지를 레지스트리에서 다운로드 한다 (ex. ubuntu:16.04 이미지 다운로드) |\n| COPY       | 호스트에 있는 파일을 도커 이미지로 복사한다                  |\n| RUN        | FROM 에서 다운로드 받은 이미지를 기반으로 RUN에 정의된 명령어를 실행하여 새로운 이미지가 생성이 된다 |\n| CMD        | 컨테이너가 시작되었을 때 실행되는 명령어를 정의한다          | > 참고로 이미지가 한번 다운로드되어 있으면 로컬에 저장된 이미지를 사용한다. --pull =true 옵션을 추가하면 매번 베이스 이미지를 강제로 새로 다운로드받는다. 2.2.4 이미지 목록 보기 docker image ls 명령어로 현재 보유하고 있는 이미지의 목록을 볼 수 있습니다. Docker Hub에서 내려받은 이미지나 이미지를 빌드한 것들이 목록으로 보여집니다. 2.2.5 이미지에 태그 붙이기 docker image tag 명령어로 이미지에 태그를 달 수 있습니다. 예를 들어 helloworld의 이미지에 0.1.0 태그를 추가하려면 다음과 같이 실행하면 됩니다. 이미지를 다음과 같은 상황에서 빌드를 하면 매번 새로운 이미지가 생성이 됩니다. - 이미지를 빌드하는 경우\n- Dockerfile을 편집하는 경우\n- COPY 대상이 되는 파일의 내용이 변경이 된 경우 COPY 대상이 되는 helloworld 파일을 수정하고 빌드를 해보겠습니다. 이미지를 빌드하고 조회를 하면 <none>가 새로 추가되었습니다. 새로운 이미지는 helloworld:latest로 새로운 생성이 되었고 그전 오래된 이미지들은 <none>으로 표시가 됩니다. 2.2.6 이미지 공유하기 docker image push 명령어로 도커 허브 등의 레지스트리에 등록할 수 있습니다. helloworld 이미지를 도커 허브에 올려보겠습니다. helloworld:latest 이미지는 도커 허브에 이미 등록되어 있어서 이미지의 네임스페이스를 먼저 바꾸고 push를 하면 됩니다. 도커에 로그인이 안 되어 있으면 로그인을 먼저하고 이미지를 push 하면 도커 허브에 공유가 됩니다. 2.3 도커 컨테이너 다루기 2.3.1 컨테이너 실행하기 docker container run 명령어는 컨테이너를 생성하고 실행하는 명령어입니다. redis 이미지를 백그라운드에서 실행하려면 -d 옵션을 주고 다음과 같이 실행하면 됩니다. -p 옵션으로 호스트 포트 7000 -> 컨테이너 포트 6379으로 포트 포워딩했으므로 다음과 같이 redis에 접속할  때 포트 번호를 7000으로 해서 접속해야 합니다. 컨테이너 옵션 | 옵션   | 설명                                                         |\n| ------ | ------------------------------------------------------------ |\n| -d     | 백그라운드로 실행한다                                        |\n| -p     | 외부포트:컨테이터포트 (ex. 9000:8080)<br />포트를 지정하지 않는 경우 임의의 포트가 자동으로 할당된다 |\n| -t     | 유닉스 터미널 연결 활성화를 시킨다<br />-i 옵션과 같이 많이 사용되어 -it 옵션으로 합쳐서 실행한다 |\n| -i     | 컨테이너 쪽 표준 입력(stdout)과 연결을 유지한다. 컨테이너 쪽 셀에 들어가려면 이 옵션을 추가해야 한다. |\n| -rm    | 컨테이터가 종료시 컨테이너를 파기한다.                       |\n| --name | 컨테이너에 원하는 이름을 붙일 수 있다. 이름으로 조회하거나 삭제할 수 있다. | 2.3.1.1 명령 인자로 실행하기 docker container run 명령어에 명령 인자를 추가해서 실행하면 Dockerfile에 정의된 CMD 인스트럭션은 무시되고 명령 인자로 넘어온 값으로 실행됩니다. 위 명령어는 redis 디몬이 실행되는 반면에 'uname -a' 인자를 추가한 명령어는 인자 명령어를 실행한 값이 출력됩니다. 2.3.2 실행 중인 컨테이너 조회하기 docker container ls 명령어로 현재 실행 중인 컨테이너를 확인할 수 있습니다. 20e9f060fc15 ID를 가진 컨테이너는 redistest 이름을 가진 컨테이너로 --name 옵션으로 실행된 컨테이너입니다. --name 옵션 없이 실행되면 랜덤 값의 이름이 부여됩니다. 2.3.2.1 컨테이너 목록 필터링해서 보기 --filter 옵션으로 필터링 조건을 추가하여 컨테이너 목록을 필터링해서 조회할 수 있습니다. 필터 조건으로 name을 추가하면 컨테이너 이름에 redis가 포함된 컨테이너를 조회합니다. 2.3.2.2 종료된 컨테이너 목록 보기 종료된 컨테이너는 목록에서 보여지지 않지만, -a 옵션을 추가하여 종료된 컨테이너를 확인할 수 있습니다. 2.3.3 실행중인 컨테이너 정지하기 docker container stop 명령어로 실행 중인 컨테이너를 컨테이너 ID나 컨테이너 이름으로 정지 시킬 수 있습니다. 2.3.4 정지된 컨테이너 재시작하기 정지된 컨테이너를 다시 시작하려면 docker container restart를 사용하면 됩니다. 2.3.5 실행중인 컨테이너 삭제하기 컨테이너를 정지시키면 정시된 시점의 상태를 계속 유지한 체 디스크에 남아 있습니다. 완전히 파기하려면 rm 명령어를 추가하여 삭제합니다. 컨테이너 실행하고 종료한 이후에 자동으로 파기하려면 다음 명령어와 같이 컨네이터 실행시 --rm 옵션을 추가합니다. 2.3.6 컨테이너의 stdout(표준 출력)를 호스트 stdout으로 출력하기 docker container logs 명령어로 도커 컨테이너의 표준 출력을 호스트 화면으로 볼 수 있습니다. jenkins 도커를 실행한 후 jenkins 서버에서 출력하는 내용을 보기 위해서는 다음과 명령어를 입력하면 됩니다. | 명령어 | 옵션 | 설명                                                         |\n| ------ | ---- | ------------------------------------------------------------ |\n| logs   | -f   | 출력하는 로그를 계속 화면에 출력한다. (ex. tail -f와 같은 옵션) |\n| ls     | -q   | docker container ls에서 -q 옵션은 컨테이너의 ID를 얻어온다   | 2.3.7 실행중인 컨테이너에서 명령 실행하기 docker container exec 명령어로 현재 실행 중인 컨테이너에 명령을 수행할 수 있습니다. 셀을 오픈해서 내부 파일을 확인하고 싶으면 다음 명령어로 확인하면 됩니다. 2.3.8 파일 복사하기 컨테이너<-> 호스트 간에 파일을 복사하기 위한 명령어입니다. 호스트 파일을 컨테이너에 복사하고 복사한 텍스트를 보려면 다음과 같이 하면 됩니다. 3. 부록(기타사항) 3.1 도커 축약 명령어 docker container run 명령어가 길기 때문에 축약 명령어도 제공합니다. 하지만, 축약 명령어보다는 full 명령어를 사용하는 걸 추천하는 분위기입니다. | 도커 full 명령어     | 축약 명령어  |\n| -------------------- | ------------ |\n| docker container run | docker run   |\n| docker image pull    | docker pull  |\n",
    "category": "cloud",
    "tags": [
      "도커",
      "컨테이너",
      "가상화",
      "명령어",
      "docker",
      "container",
      "vm",
      "cli"
    ],
    "date": "2019-12-08T00:00:00.000Z"
  },
  {
    "id": "cloud/heroku에-node-js-mongodb-app-배포하기",
    "slug": "cloud/heroku에-node-js-mongodb-app-배포하기",
    "title": "Heroku에 Node.js+MongoDB App 배포하기",
    "excerpt": "",
    "content": "1. Heroku란 헤로쿠(Heroku)는 PaaS(Platform as a Service)형태의 클라우드 서비스입니다. 헤로쿠는 터미널이나 웹에서 필요한 여러 티어(ex. DB)를 쉽게 생성하고 연동시킬 수 있습니다. 최초 버전에서는 Ruby 언어만 지원하였지만, 현재는 메이저급 언어 대부분을 지원하고 있습니다. 1.1 Heroku Features  Git 명령어로 앱을 배포함\n 앱 배포 시 경량 가상화 컨테이너인 Dynos에서 실행됨\n 여러 언어를 지원함 (Ruby, Java, Node.js, Scala, Clojure, Python, Php, Go)\n 애드온(Add-ons)\n     DB와 같은 여러 백엔드 서비스를 제공함(ex. redis, mongodb, mysql)\n     Elements 마켓장소에서 필요한 애드온을 찾아 설치할 수 있음  구독 가격\n     무료\n         공짜 Dyno 시간이 주어짐(ex. 기본 550 hrs부터 시작)\n         30분 동안 사용하지 않는 경우에는 sleep 모드로 전환됨\n         재 접속시에는 wakeup하는 시간으로 응답시간이 늦어질 수 있음      유료 - 3가지 구독 서비스가 있음 2. Heroku 시작하기 2.1 Heroku 계정 만들기 먼저 Heroku 사이트에 들어가 계정을 만들어야 합니다. 2.2 Heroku CLI 설치 터미널에서 Heroku 앱을 관리할 수 있도록 Heroku CLI (Command Line Interface)를 설치합니다. 이 포스팅에서는 맥 기반으로 설명되어 있습니다. 그외에 OS는 Heroku 설치 가이드를 참조해주세요. 2.3 Heroku 로그인 Heroku관련 작업을 하려면 먼저 로그인 명령어로 Heroku 계정에 로그인해야 합니다. 2.4 필수 명령어 Heroku를 배포할때 git를 사용하기 때문에 git가 있어야 합니다. 맥 OS에서는 Xcode가 설치되어 있으면 기본적으로 터미널에서 사용할 수 있습니다. 3. Sample 코드 생성, 코드 수정 및 배포 실제 개발과정을 보면 처음에 프로젝트 생성한 이후 코드 수정, 개발환경에서의 테스트, 서버로 배포하는 이런 과정을 계속 반복하게 됩니다. Heroku 클라우드에 개발하는 코드 어떻게 쉽게 반영할 수 있는지 알아보죠. 3.1 Sample 프로젝트 생성 Heroku에서 기본적으로 제공하는 샘플 프로젝트를 다운로드합니다. 3.2 개발환경에서 실행하기 앱에 필요한 Node.js 모듈를 설치하고 앱을 구동해보죠. 3.3 Heroku 클라우드에 배포하기 먼저 create 명령어로 Heroku에 관련 Git 저장소와 빈 앱을 생성합니다.  앱 주소\n     https://nameless-falls-97478.herokuapp.com/  Heroke Git 저장소\n     https://git.heroku.com/nameless-falls-97478.git create 옵션에 이름을 지정하지 않으면 임의의 이름(ex. 여기서는 nameless-falls-97478로 생성)으로 생성합니다. 앱 생성 이후에도 apps:rename 옵션으로 앱 이름 변경이 가능합니다. 이제 실제로 Heroku 클라우드에 배포해보죠. Git 명령어로 Heroku 저장소에 푸시하면 배포가 완료됩니다. 참 쉽죠? ㅋㅋ 소스가 올라가면 빌드 과정을 거치고 공개 주소에 접속해 웹 앱에 접속할 수 있습니다. 빌드 과정이후 Heroku에서 웹 앱을 시작하는 시작점은 Procfile 파일에 정의되어 있습니다. Procfile이 없은 경우에는 package.json에 정의된 start script로 시작합니다. 3.3 배포된 사이트 오픈하기 잘 배포되었는지 브라우저에서 확인해봅니다. 브라우저가 오픈되고 공개 주소로 접속되고 페이지가 정상적으로 로드되는 것을 볼수 있습니다. 3.4 코드 수정이후 다시 배포하기 이제 코드 수정한 이후에 다시 배포하는 과정을 거쳐보죠. 메인 페이지 (views/pages/index.ejs)에서 타이틀 부분을 수정합니다. 로컬환경에서 수정이 잘됐는지 확인 이후 이상없으면 코드를 commit합니다. Heroku에 배포하고 반영 잘됐는지 브라우저를 오픈해서 확인합니다. 변경한 타이틀이 잘 로드가 되네요. 웹 앱이 Heroku에서 구동될 때 로그를 확인하고 싶으면, logs 옵션으로 확인이 가능합니다. 4. Add-on MongoDB 설치후 Node.js와 연동하기 Add-on 마켓장소에는 많은 수의 데이터 저장소(ex. Postgres, Redis, MongoDB, MySQL)를 지원하고 있습니다. 이 예제에서는 MongoDB add-on을 설치하고 작성하고 있는 Node.js와 연동하는 부분을 다루어 보도록 하겠습니다. 4.1 MongoDB 설치 MongoDB add-on을 추가합니다. 명령어외에도 직접 마켓장소에서 접속해서 add-on을 추가할 수 있습니다. 4.2 MongoDB와 연동 mLab MongoDB를 추가하면 Heroku 환경변수에 MONGODBURI가 추가됩니다. MongoDB 주소는 아래와 같습니다. 명령어로 MongoDB에 접속할 수도 있지만, 개인적으로 MongoDB GUI client(Studio 3T)로 접속해보았습니다. 아래는 Studio 3T에서 새로운 연결 정보를 입력하는 화면입니다. DB에 연결이후 앱에 필요한 데이터를 입력합니다. dummy 데이터로는 기존에 작성된 데이터를 사용하였습니다. (데이터 링크) 4.3 코드 수정 앞서 MongoDB에 입력한 데이터를 브라우저에서 /timers에 접속시 json값을 가져오는 코드를 작성해보도록 하겠습니다. 먼저 MongoDB를 Node.js에서 사용하기 위해서는 mongoose 모듈이 필요합니다. Npm 명령어로 mongoose 모듈를 설치합니다. 앱에서 MongoDB로 접속하는 데 필요한 파일을 생성하고 기존 코드를 수정합니다. 원하는 document 구조에 맞게 schema 파일을 생성합니다. 메인 index.js 파일을 수정합니다. RESTful API /timers 접속 시 모든 데이터를 조회 가능하도록 구현합니다. 지금까지 작성한 코드는 github에 업로드 되어 있습니다. 4.4 재배포 확인 다시 Heroku에 배포하고 확인해보죠. 5. Appendix 5.1 기존 App Heroku에 배포하기 기존에 작성한 Node.js 프로젝트에 Heroku를 배포해보았습니다. 실제 적용 과정은 위 샘플 프로젝트와 크게 다르지 않습니다.  https://github.com/kenshin579/app-keep-countdown-timer 먼저 Procfile 파일 생성합니다. create 명령어로 Heroku 앱을 생성합니다. MongoDB add-on을 생성해서 주소를 얻어옵니다. 새로 얻은 MongoDB 주소를 코드에 반영하고 MongoDB에 dummy 데이터를 로드합니다. 로컬 환경에서 이상이 없으면, commit해서 Heroku에 배포합니다. 5.2 명령어 모음 이미 언급된 Heroku 명령어외에 사용시 알면 유용한 명령어를 모아 보았습니다.  heroku run bash - 실행중인 app에 bash 실행한다  heroku ps - 실행중인 process를 볼수 있습니다. 현재 남아 있는 dyno 시간도 알 수 있다  heroku list - Heroku으로 등록된 앱을 보여준다  heroku ps:stop - 실행중인 app을 멈춘다 6. 참고  IaaS, PaaS, SaaS\n     https://blogs.msdn.microsoft.com/eva/?p=1383  Heroku Docs\n     https://devcenter.heroku.com/categories/reference",
    "category": "cloud",
    "tags": [
      "heroku",
      "node",
      "하루쿠",
      "클라우드",
      "노드",
      "몽고",
      "배포",
      "가상화",
      "mongo",
      "mongodb",
      "vm",
      "virtualization",
      "cloud"
    ],
    "date": "2018-08-21T00:00:00.000Z"
  },
  {
    "id": "cloud/jaeger에-대한-소개",
    "slug": "cloud/jaeger에-대한-소개",
    "title": "Jaeger에 대한 소개",
    "excerpt": "",
    "content": "> 본 내용은 저희 Platform Engineering 팀내 CNCF 스터디를 위해 준비한 자료입니다. 저희가 하는 로봇 플랫폼 개발에 관심이 있는 분들은 아래 링크를 참고해주시고 도전적이고 열정적으로 같이 일하실 분은 많이 지원해주세요.\n>\n> - 네이버는 왜 제2사옥 1784를 지었을 까요?  https://www.youtube.com/watch?v=WG7JHLfClEo\n> - 네이버 랩스 - https://www.naverlabs.com/ 1.What is Jaeger? 1.1 Distributed Tracing? 마이크로 서비스와 같이 여러 컴포넌트로 분리된 분산 환경에서 로그로만 문제점을 파악하기는 쉽지 않다. 특히 마이크로 서비스의 대부분의 문제점은 여러 개의 다른 서비스 간의 통신 이슈(ex. wrong request, latency)인 경우가 많고 이런 환경에서 문제의 근본 원인을 빠르게 찾기는 쉽지 않다. > Distributed Tracing (분석 추적)?\n>\n> - ‘call-stacks’ for distributed services.\n> - 분산 추적은 분산 시스템을 통해 흐르는 서비스 요청을 추적하고 관찰하는 것이다 Distributed Tracing의 기본 아이디어 - 실행되는 컨포넌트마다 실행 시간과 추가 정보 수립\n- 수집한 정보를 DB에 저장\n- DB에 저장된 정보를 가지고 컨포넌트간의 연관관계를 재조합해서 Visualization 도구로 표시함 1.2 Jaeger? Jaeger는 2015년 Uber가 만든 오픈 소스 Distributed Tracing System이다. Jaeger는 처음부터 OpenTracing 표준을 지원하도록 설계되었다. (표준화는 업체 중립적인 Tracing 데이터 모델링) 1.2.1 Tracing Specification - OpenTracing\n    - CNCF project로 현재 deprecated 됨\n    - OpenTracing Observability 백엔드 서버에 telemetry data (metrics, log, traces)를 전송하기 위해 vendor-netural 표준화된 API를 제공한다\n    - 개발자는 OpenTracing API 표준화에 맞게 직접 라이브러리를 구현해야 한다\n- OpenCensus\n    - Google의 오픈 소스 커뮤니티 프로젝트\n    - OpenCensus는 개발자가 자기 어플리케이션을 계측해서 백엔드로 telemetry data를 전송할 수 있도록 언어별 라이브러리 세트를 제공한다\n- OpenTelemetry (OTel)\n    - OpenTracing + OpenCensus 프로젝트가 하나로 merge됨\n    - 2019년에 CNCF Incubation 프로젝트로 채택됨\n    - 프로젝트 성숙도는 아직 Incubating level에 있음\n    - Trace, metric, log 와 같은 원격 측정 데이터를 기기, 생성, 수집 및 내보내기 위한 공급 업체-중립 오픈 소스 관찰 프레임워크이다 Reference - https://opencensus.io/\n- https://opentracing.io/\n- https://opentelemetry.io/docs/concepts/what-is-opentelemetry/ 1.2.2 History - Dapper (Google) : Foundation of all tracers\n    - Tracing 관련 주제는 1990년도 부터 나오기 시작\n    - Google의 논문 Dapper, a Large-Scale Distributed Systems Tracing Infrastructure, 2010 발표이후 주류로 자리 매김하게 됨\n- Zipkin and OpenZipkin (Twitter)\n    - 최초의 Open Source Tracing system\n    - 2012년 Twitter에서 공개\n- Jaeger (Uber)\n    - 2015년 Uber에 의해서 만들어졌고 2017년에 오픈소스로 공개함\n    - 2017년 9월에 CNCF Incubation 프로젝트로 채택됨\n    - 2019년에 Jaeger는 Graduated 프로젝트로 승인됨\n- StackDriver Trace -> Cloud Trace (Google)\n- X-Ray (AWS) 1.2.3 Feature - High Scalability 고려해서 설계됨\n    - collector의 auto-scaling을 지원함 - OpenTracing과 OpenTelemetry을 지원함\n    - 처음부터 OpenTracing 표준을 지원하도록 설계됨\n    - v1.35 이후, Jaeger 백엔드는 OpenTelemetry SDK로부터 trace data를 수신할 수 있음\n- Modern Web UI (React 개발)\n- Cloud Native Deployment\n    - Jaeger 백엔드 도커 이미지로 배포 가능하도록 지원\n- Zipkin과의 역호환성도 지원\n- Topology Graphs\n    - Jaeger UI에서 2가지 타입의 서비스 그래프가 있음\n        - System Archiecture, Deep Dependency Graph\n- Sampling\n    - Adaptive sampling 지원 - 다중 스토리지 백엔드 지원\n    - Memory (기본), Cassandra, Elasticsearch\n- Post-collection data processing pipeline (coming soon)\n- Service Performance Monitoring (SPM) 1.2.4 Tracing 용어와 친해지기 Span\n- 분산 추척에서는 가장 기본이 되는 블록 단위로 분산 시스템에서 실행되는 작업 단위를 나타낸다\n    - ex. HTTP request, call to DB\n- Span은 다음과 같은 정보가 있다\n    - Span Name (operation Name)\n    - Start/Finish Timestamp\n    - Span Tags, Logs (key:value)\n    - Span Context : 서비스에서 다른 서비스로 전달이 될 때 Trace에서 각 Span를 구별할 수 있는 추척 정보 (ex. Span id, Trace id) Trace\n- Trace는 시스템 전반에서 데이터/실행 경로를 나타낸다\n- 1개 이상의 Span으로 이루어져 있고 여러 개의 Span이 모여서 하나의 Trace를 완성하게 된다 Instrumentation\n- Application(ex. DB)에 따라 여러 library를 오픈소스로 제공\n- Instrumentation library를 통해서 Span으로 생성함 2. Jaeger Tracing Architecture Jaeger는 추적 데이터를 수집, 저장, 표시해주기 위해 여러 구성 요소로 이루어져 있다. - Jaeger client / OpenTelemetry Distro (SDK)\n    - Jaeger Client는 분산 추적을 위한 OpenTracing API의 언어별 구현체\n    - 지금은 OpenTelemetry SDK를 사용\n- Jaeger Agent\n    - 사용자 데이터그램 프로토콜 (UDP)을 통해 전송된 스팬을 수신하는 네트워크 데몬으로, 계측된 애플리케이션과 동일한 호스트에 배치됨\n- Jaeger Collector\n    - 프로세싱을 위해 Span을 수신하여 대기열에 배치\n- Storage Backends\n    - Trace를 저장할 수 있는 데이터 저장소\n- Jaeger Query\n    - Query 서비스는 저장소에서 데이터를 가져와 UI에 필요한 API를 제공한다\n- Jaeger UI 2.1 Jaeger Architecture 2.1.1 Jaeger Architecture w/ Kafka - Intermediate Buffer - Ingester\n    - Collector와 DB 간의 Intermediate buffer로 Kafka를 사용할 수 있다\n    - Ingester는 Kafka에서 데이터를 읽고 다른 스토리지에 쓰는 역할을 한다 2.1.2 Instrumentation Span을 생성하는 방법은 2가지가 있다 - Auto Instrumentation\n    - 이미 OpenTelemetry 커뮤니티에서 여러 어플리케이션(ex. Redis, MongoD)를 위한 library를 만들어서 registry 사이트에서 제공하고 있음\n    - https://opentelemetry.io/registry/\n- Manual Instrumentation\n    - 오픈소스로 제공되지 않는 경우에는 어플리케이션에 직접 수동으로 Span을 생성해서 개발을 해야 함 2.1.3 Sampling 모든 tracing 정보를 raw하게 다 저장하지 않고 백엔드에 저장되는 trace 수를 줄이기 위해 sampling을 사용한다. - Head-based sampling\n    - jaeger-client 맨 앞단에서 sampling rule이 결정되는 방식\n- Tail-based sampling\n    - collector 단에서 sampling을 하는 거라 tail-based라고 칭함\n    - Adaptive sampling (v1.27이후) 도 지원해서 시스템으로 들어오는 트랙픽과 trace의 수을 가지고 sampling이 자동으로 조절이 가능함 3.Running Jaeger Docker on Local Machine 3.1 Hot R.O.D - Rides on Demand Sample 실행하기 HotROD는 Jaeger github에서 제공하는 \"ride on demand\" 데모 어플리케이션이고 OpenTracing API를 사용한 버전이다. Standalone으로 실행되고 여러 마이크로 서비스가 별도 port로 실행하여 간단한 MSA 형식으로 동작하게 되어 있다. 이 예제에서는 별도 Instrumentation을 사용하지 않고 직접 Span을 생성하는 방식으로 되어 있다. Jaeger 실행하기 빠른 실행을 위해 Jaeger의 모든 컨포넌트가 포함되어 있는 올인원 도커 이미지로 실행한다. > 운영 환경에서는 올인원 도커 이미지로 실행하고 만약 컨",
    "category": "cloud",
    "tags": [
      "jaeger",
      "telemetry",
      "trace",
      "monitor",
      "msa",
      "분산추적"
    ],
    "date": "2022-07-22T00:00:00.000Z"
  },
  {
    "id": "cloud/jaeger에-대한-소개-en",
    "slug": "cloud/jaeger에-대한-소개-en",
    "title": "Introducing Jaeger",
    "excerpt": "",
    "content": "1.What is Jaeger? 1.1 Distributed Tracing? In a distributed environment such as Microservices, it is not easy to identify the problems right away just going through the logs. In particular, most problems in microservices are often communication issues between multiple services (e.g., incorrect requests, latency), and it's not easy to quickly find the root cause of the problem in this environment. > What is the Distributed Tracing?\n>\n> - ‘call-stacks’ for distributed services.\n> - Distributed tracing is the process of tracing and observing service requests that flow through a distributed system. Basic Idea for Implementing Distributed Tracing - Establish execution time and additional information for each executed component\n- Store the collected information in DB\n- Using the information stored in the DB, recombine the relationships between components and display them as a visualization tool. 1.2 Jaeger? Jaeger is an open source Distributed Tracing System created by Uber in 2015. Jaeger was designed from the ground up to support the OpenTracing standard, which is a vendor-neutral tracing data modeling standard. 1.2.1 Tracing Specification - OpenTracing\n    - Currently deprecated as a CNCF project\n    - Provides a vendor-neutral standardized API for developers to send telemetry data (metrics, logs, traces) to observability backend servers\n    - Developers must implement their own libraries to comply with the OpenTracing API standard\n- OpenCensus\n    - Google's open-source community project\n    - OpenCensus provides a set of language-specific libraries for developers to instrument their applications and send telemetry data to the backend\n- OpenTelemetry (OTel)\n    - It is a merged two projects, OpenTracing and OpenCensus\n    - Adopted as a CNCF Incubation project in 2019\n    - Project maturity is still at the Incubating level\n    - A vendor-neutral open-source observability framework for instrumenting, generating, collecting, and exporting remote measurement data, such as trace, metric, and log data Reference - https://opencensus.io/\n- https://opentracing.io/\n- https://opentelemetry.io/docs/concepts/what-is-opentelemetry/ 1.2.2 History - Dapper (Google): Foundation of all tracers - Topics related to tracing have been emerging since the 1990s.\n    - Google's paper, Dapper, a Large-Scale Distributed Systems Tracing Infrastructure, published in 2010, has become the mainstream foundation of tracing. - Zipkin and OpenZipkin (Twitter)     - The first open-source tracing system\n    - Released by Twitter in 2012 - Jaeger (Uber)     - Developed by Uber in 2015 and release in 2017 as an open-source project\n    - Adopted as a CNCF Incubation project in September 2017\n    - Jaeger was approved as a Graduated project in 2019 - StackDriver Trace -> Cloud Trace (Google) - X-Ray (AWS) 1.2.3 Feature - Jaeger backend is designed with High Scalability in mind - collectors can be auto-scaled - Support OpenTracing and OpenTelemetry\n    - Designed to support OpenTracing standards from the beginning\n    - Since v1.35, Jaeger backend can receive trace data from OpenTelemetry SDK\n- Modern Web UI is developed using React\n- Cloud Native Deployment\n    - Jaeger backend is distributed as a docker image\n- Support backwards compatibility with Zipkin\n- Support two types of services graphs\n    - System Architecture, Deep Dependency Graph\n- Provide adaptive sampling\n- Support Multiple Storage Backends\n    - ex. Memory (default), Cassandra, Elasticsearch\n- Service Performance Monitoring (SPM) 1.2.4 Tracing Terms Span - In distributed tracing, span represents the unit of work performed in a distributed system, typically corresponding to a single operation such as an HTTP request or call to a database.\n- A span has the following information:\n    - Span Name (Operation Name)\n    - Start/Finish Timestamp\n    - Span Tags, Logs (key:value)\n    - Span Context: Tracing information that allows each span to be distinguished in a trace as it is propagated from one service to another (e.g. span id, Trace id) Trace - A trace represents the data and execution path across a system\n- It is composed of one or more spans, with multiple spans coming together to form a complete trace Instrumentation - Several libraries are provided as open source for different applications (e.g. databases)\n- Instrumentation libraries are used to create spans 2. Jaeger Tracing Architecture Jaeger consists of multiple components to collect, store, and display tracing data. - Jaeger client / OpenTelemetry Distro (SDK)\n    - Jaeger client is a language-specific implementation of the OpenTracing API for distributed tracing\n    - Currently using the OpenTelemetry SDK\n- Jaeger Agent\n    - A network daemon that receives spans sent via the UDP and is deployed on the same host as the instrumented application\n- Jaeger Collector\n    - Receives spans for processing and places them in a queue\n- Storage Backends\n    - Data storage for traces\n- Jaeger Query\n    - Query service that retrieves data f",
    "category": "cloud",
    "tags": [
      "jaeger",
      "trace",
      "tracing",
      "pinpoint",
      "telemetry"
    ],
    "date": "2023-03-07T00:00:00.000Z"
  },
  {
    "id": "cloud/kafka-cli-명령어-모음",
    "slug": "cloud/kafka-cli-명령어-모음",
    "title": "Kafka CLI 명령어 모음",
    "excerpt": "",
    "content": "Kafka 사용시 Ahkq UI를 대부분 사용하고 있지만, Trouble-shooting이나 스크립트 작성을 하는 경우에는 Kafka CLI를 사용하는 경우도 종종있다. 자주 사용하는 Kafka CLI 명령어를 정리합니다. 로컬환경에서 Kafka를 실행하는 방법은 그전 포스팅을 참고해주세요. 1.Download Kafka 최신 Kafka binary 파일은 아래 링크에서 다운로드한다. - https://kafka.apache.org/downloads 2.Kafka CLI Kafka 기본 포트번호는 9092로 시작하지만, 로컬환경에서 Kafka 실행하기에서 설정한 포트번호로 실행한다. > Kafka v2.2이하에서는 Zookeeper URL과 port 번호 (ex. )를 사용했지만, Kafka v2.2+ 부터는  옵션을 사용을 추천하낟. v3부터는 Zoopkeeper 옵션을 제거될 예정이다. Kafka CLI를 자주사용하는 경우라면  환경변수에 추가하는 걸 추천한다. 매번 Kafka binary 폴더로 이동해서 명령어를 입력하지 않아도 된다. 2.1 Topics 2.1.2 Topic 목록 출력 2.1.1 Topic 생성 2.1.3 Topic 정보 보기 2.1.4 Topic 삭제하기 2.2  Producer 아래 가 표시되면 topic에 데이터를 보낼 수 있다. ( 키를 누르면 전송이 된다) producer console 창에서 나가려면 를 입력하여 종료시킨다. 2.2.1 에서 키와 같이 메시지를 생성하는 방법은? 기본적으로 Kafka topic에 메시지를 보내면  키가 있는 메시지가 생성된다. 키와 함께 메시지를 보내려면, 와  속성 값을 사용해야 한다. 예제에서는 를 separator로 사용했다. 2.3 Consumer  명령어 사용시 알아야 하는 내용들이다. -  옵션을 지정하지 않는 한, 가장 최신 메시지만 출력된다\n- Topic이 생성되지 않았을 경우에는 기본적으로 topic을 자동으로 생성한다\n- 쉼표로 여러 topic을 지정하면 한 번에 여러 topic을 consume 할 수 있다\n- consumer group을 지정하지 않는 경우 는 임의 consumer group을 생성한다\n- 메서지의 순서는 보장이 안될 수도 있다\n    - 메시지의 순서는 topic 레벨이 아니라 partition 레벨에서만 순서를 보장한다  명령어 옵션은 다음과 같다. - \n    - 처음부터 메시지를 받을 수 있다\n- \n    - Consumer group을 지정하지 않으면 기본적으로 임의의 consumer group ID가 자동으로 생성이 된다\n- \n    - 특전 partition에서만 consumer하려면 이 옵션을 사용한다 2.3.2 key, value 값을 출력하려면? 2.4 Consumer Group Consumer group 기능에 대해서 알아보기 위해서 topic은 최소 2개 이상의 partition 값으로 생성한다. Consumer group 시 알아야 하는 사항은 다음과 같다. - Kafka topic의 partition 수는 group에 더 많은 consumer를 가질 수 없다. (of consumer <= of partition)\n-  옵션 사용해서 consumer group에서 데이터를 consume하고 나중에 다시 같은 consumer group으로  옵션을 사용해보면 무시되는 것을 볼 수 있다. 이런 경우, consumer group을 재설정해야 한다\n-  옵션을 지정하지 않으면 consumer group은 와 같은 임의의 consumer group이 생성된다\n- 하나의 consumer가 모든 메지지를 받는다면, 아마 topic의 partition 수가 1로 만들어졌을 것이다. Partition 3으로 topic을 생성한다. 터미널 창을 2개 띄워서  옵션으로 consumer group을 각각 시작한다. Topic에 메시지를 보내보면 번갈라 가면서 consume 을 하는 것을 볼 수 있다. 2.5. Consumer Group Management 여기서는 Kafka consumer group을 어떻게 재설정 할 수 있는지 다룬다. - 활성화된 consumer 가 있는 경우에는 consumer group을 재설정 할 수 없다\n- 재설정은 consumer group의 데이터를 재처리하는데 사용한다 (ex. 버그 수정이 있는 경우)\n-  옵션으로 설정한다\n- Offset reset 전략 옵션\n    - , , , , , ,   명령어 옵션은 다음과 같다. - \n    - 실행할 결과만 보여주고 실제로 명령은 수행하지 않는다\n- \n    - 모든 group에 reset offset을 적용할 수 있기 때문에 주의해서 사용해야 한다\n- \n    - 모든 topic에 reset offset을 적용할 수 있기 때문에 주의해서 사용해야 한다\n- \n    - duration으로 offset을 reset한다 2.5.1  옵션으로 offset 재설정하기 먼저 활성화된 consumer가 없는지 확인한다. Topic 전체를 다시 읽으려면, offset을 최초의 위치(earliest)로 변경한다. 모든 partition에 새로운 offset이 0으로 재설정되었다. 해당 consumer를 다시 시작하면 각 partition의 시작 offset부터 읽어온다. 2.5.2  옵션으로 offset 재설정하기 Offset을 2만큼 이동하는 방법도 있다. Consumer를 재시작하면 topic의 각 partition에서 마지막 2개의 메시지만 반환하는 것을 볼 수 있다. 3.FAQ 3.1 Topic의 Partition 수를 늘리는 방법 현재 partition 수를 확인한다  Topic의 partition 수를 3으로 늘린다 잘 반영이 되었는지 확인하다 3.2 Kafka에서 Consumer Group을 삭제하는 방법 Consumer group을 삭제하여 완전히 처음부터 데이터를 읽어 올 수 있다. 3.3 Consumer Group에서 모든 Consumer를 조회하는 방법 Consumer group에서 모든 consumer를 조회하면, consumer가 네트워크 상으로 어디에 위치해 있고 얼마나 topic을 consume을 하고 있는지 쉽게 알 수 있다. 3.4 특정 offset과 partition에서 메시지를 조회하는 방법 https://developer.confluent.io/tutorials/kafka-console-consumer-read-specific-offsets-partitions/confluent.html 3.5 메시지의 Timestamp도 출력하는 방법 참고 - https://github.com/confluentinc/schema-registry/issues/947 4. 참고 - https://www.conduktor.io/kafka/kafka-cli-tutorial\n- https://kafka.apache.org/documentation/#basicops\n- https://betterprogramming.pub/kafka-cli-commands-1a135a4ae1bd\n- https://medium.com/@TimvanBaarsen/apache-kafka-cli-commands-cheat-sheet-a6f06eac01b\n- https://akageun.github.io/2020/05/07/kafka-cli.html\n- https://hevodata.com/learn/kafka-cli-commands/\n- https://docs.confluent.io/platform/current/tutorials/examples/clients/docs/kafka-commands.html",
    "category": "cloud",
    "tags": [
      "kafka",
      "script",
      "cli",
      "ahkq",
      "명령어",
      "카프카",
      "브로커",
      "메시지",
      "아파치"
    ],
    "date": "2022-08-14T00:00:00.000Z"
  },
  {
    "id": "cloud/kafka-cli-명령어-모음-en",
    "slug": "cloud/kafka-cli-명령어-모음-en",
    "title": "Kafka CLI Collection",
    "excerpt": "",
    "content": "When using Kafka, it is much nicer to have some kind of UI interface and I believe many people use Ahkq UI including myself. But sometimes using Kafka CLI is necessary for troubleshooting and it can be used for scripting. This article summarizes frequently used Kafka CLI commands. To learn how to run Kafka on your local environment, see our previous article. 1.Download Kafka The Kafka binary file has CLI so let's download the latest binary file from the link below. - https://kafka.apache.org/downloads 2.Kafka CLI The default port number for Kafka is 9092. If you running the Kafka in different port,  make sure to use the number instead. > In Kafka v2.2 and earlier version uses the Zookeeper URL and port number (e.g. ), but since  the Kafka v2.2 higher version,  the option should be used. After v3, the Zoopkeeper option will be removed. If you use the Kafka CLI frequently, I recommend adding it to your  environment variable. This way you won't have to navigate to the Kafka binary folder and type the command every time. 2.1 Topics 2.1.2 A List of Topics 2.1.1 Creating a Topic 2.1.3 View Topic Information 2.1.4 Deleting a Topic 2.2  Producer When the prompt is displayed, you can send data to the topic (press  to send). To exit the producer console window, enter . 2.2.1 How to send a message with a key? By default, sending a message to a Kafka topic generates a message with a  key. To send a message with a key, you need to use the values of the  and  properties. In the example,  is used as the separator. 2.3 Consumer Here are the things to know when using the  command: - Unless the  option is specified, only the latest message is shown.\n- If the topic does not exist, it is automatically created by default.\n- Multiple topics can be consumed at once by specifying them separated by commas.\n- If the consumer group is not specified,  creates an arbitrary consumer group.\n- The order of messages is not guaranteed.\n    - The message order is only guaranteed at the partition level, not at the topic level. The options for the  command are as follows: - \n    - Start consuming messages from the beginning.\n- \n    - If no consumer group is specified, an arbitrary consumer group ID is automatically generated.\n- \n    - Use this option to consume only from a specific partition. 2.3.2 How to print out the consumed message with key? 2.4 Consumer Group To learn about the Consumer group feature, a topic is created with at least two partition values. Here's what you need to know about consumer groups: - The number of consumers in a group cannot exceed the number of partitions in a Kafka topic. (of consumer <= of partition)\n- If you consume data from a consumer group using the  option and later try to consume from the beginning using the same consumer group with the  option, the messages are ignored. In this case, you need to reset the consumer group.\n- If you do not specify the  option, a random consumer group such as  will be created.\n- If one consumer receives all the messages, the topic was probably created with a partition count of 1. To play around with the consumer group, let's create a topic with partition 3. Open two terminal windows and start the consumer group with the  option in each. If you send a message to the topic, you'll see that it takes turns to consume. 2.5. Consumer Group Management This section covers how you can reset a Kafka consumer group. - You cannot reset a consumer group if there are active consumers\n- Resetting a consumer group is used to reprocess data in the group (e.g., to fix bugs).\n- Use the  option to reset a consumer group.\n- Offset reset strategy options include:\n    - , , , , , ,  The following are options for the  command: - : Only shows what the result of the command would be without actually executing it.\n- : Be careful using this option, as it applies the offset reset to all consumer groups.\n- : Be careful using this option, as it applies the offset reset to all topics.\n- : Resets the offset by duration. 2.5.1 Reset offset with  option Make sure to check to see if there is any active consumers. To read the entire topic again, change the offset to the initial position (earliest). The new offset is reset to 0 for all partitions. When you restart the consumer, it will read from the start offset of each partition. 2.5.2 Reset the offset with  You can also move the offset by 2. When you restart the consumer, you will see that only the last 2 messages are returned from each partition of the topic. 3.FAQ 3.1 How to increase the number of partition for a topic? First, you can check the current number of partition by the following command. Let's increase the number of partitions for the  topic by 3. Let's verify the updated number of partition. 3.2 How to delete a consumer group? You can read the entire message from the beginning if you delete the consumer group. 3.3 How to list all consumers in the consumer group? By querying all consumers in the consumer group, you can easily see where the consumer",
    "category": "cloud",
    "tags": [
      "kafka",
      "mq",
      "script",
      "cli",
      "ahkq",
      "broker",
      "mq",
      "en"
    ],
    "date": "2023-03-06T00:00:00.000Z"
  },
  {
    "id": "cloud/kafka-connect에-대한-소개",
    "slug": "cloud/kafka-connect에-대한-소개",
    "title": "Kafka Connect에 대한 소개",
    "excerpt": "",
    "content": "1. Kafka Connect 소개 Kafka Connect를 사용하려고 고려하고 있다면 Kafka에 대해서는 이미 잘 알고 찾아봤을 거라고 생각해서 Kafka는 간단하게 언급만 하고 바로 Kafka Connect에 대해서 소개한다. > 아파치 카프카(Apache Kafka)는 아파치 소프트웨어 재단이 스칼라로 개발한 오픈 소스 메시지 브로커 프로젝트이다. ...\n> 요컨대 분산 트랜잭션 로그로 구성된, 상당히 확장 가능한 pub/sub 메시지 큐로 정의할 수 있으며, 스트리밍 데이터를 처리하기 위한 기업 인프라를 위한 고부가 가치 기능이다.\n>\n> From Wikipedia Kafka는 메시지 브로커 프로젝트 중에 하나이고 다른 프로젝트와 같이 pub-sub 모델을 지원하는 분산 메시지 큐 시스템이다. 처음 LinkedIn에서 개발되어 2011년 초에 오픈소스화 되어 공개되고 2014년 말에는 Kafka를 개발하던 개발자들이 Confluent라는 새로운 회사를 창립하여 Kafka 개발에 집중하고 있다. Kafka Connect는 Kafka 생태계에서 어떤 역할을 하고 있는지 알아보자. 1.1 Kafka Connect Kafka Connect는 Kafka를 사용하여 다른 시스템과 데이터를 주고 받기 위한 오픈소스 프레임워크이다. Kafka Connect에는 기존 시스템에 연결하여 Kafka와 데이터를 주고 받는 데 도움이 되는 다양한 내장 Connector(ex. mongo)를 제공해주고 있다. Kafka Connect의 특징과 장점은 다음과 같다. - 반복적인 파이프라인 생성 작업시 매번 프로듀서 컨슈머 어플리케이션을 개발하고 배포, 운영 하는것은 비효율적이다\n- Connector를 이용하면 특정한 작업 형태를 템플릿으로 만들어 놓은 Connector를 실행함으로써 반복작업을 줄일 수 있다\n    - Business 로직에만 집중할 수 있고, 내부 시스템 코드도 더욱 단순화되는 장점이 있다\n- Connector는 2가지 타입이 존재한다\n    - Source Connector는 외부 시스템에서 Kafka로 데이터를 넣어주는 Connector이다 (External System -> Kafka)     - Sink Connector는 Kafka에서 데이터를 꺼내 외부 시스템에 데이터를 넣어주는 Connector이다. (Kafka -> External System)\n- 다양한 Connector들이 존재 (ex. 오픈소스, 유료)하고 개발자들도 직접 원하는 Connector를 개발할 수 있다\n    - MongoDB, JDBC, Redis, HTTP, S3, Elasticsearch, AWS Lambda, etc\n    - Connector 목록\n    - Confluent Hub - Connector App Store 1.2 Kafka Connect Usecase 다른 시스템에서 Kafka로 Kafka에서 다른 시스템으로 데이터를 스트리밍할 방법은 여러 가지가 있겠지만, 직접 개발하기보다는 Kafka Connect로 쉽게 해결될 수 있는지 첫 번째로 고려해보면 좋을 것이다. 몇 가지 사례를 통해서 어떻게 다양하게 사용될 수 있는지 알아보자. 1.2.1 멀티 타겟 시스템에 스트리밍하기개 Kafka Connect를 사용하면 이미 여러 타겟 시스템 대상으로 connector가 제공되어 있어서 Kafka에 저장된 데이터를 쉽게 스트리밍이 가능하다. 비즈니스 요구에 맞게 새로운 타겟 시스템이 필요할 수 있고 Kafka Connect로 인해서 빠르게 개발 단계로 이어질 수가 있다. - kafka -> (kafka connect) -> multiple targets (s3, hdfs) 1.2.2 다양한 외부 시스템에서 다른 곳으로 데이터 전달이 필요한 경우 한 컨포넌트에서 다른 컨포넌트로 전달할 수 있는 방법은 여러 가지가 있다. - A component -> db (ex. mysql) -> (mysql sink connect) -> kafka -> B component (consume)     - 어드민 성격의 component에서 설정 값을 mysql에 저장한다.\n    - 이 값을 다른 component에서 사용할 필요가 있는 경우에는 mysql source connector를 등록해서 사용하면 쉽게 다른 component로 전달할 수 있다 - A component (http API 노출) -> (http source connect) -> kafka -> (mongo sink connector) -> db (ex.mongo) -> B component     - B component는 mongodb로 read/write 할 수 있는 application이지만, A component의 역할과 domain에 따라서 직접 db에 접근하는 건 부절적한 경우가 있어 kafka를 이용해서 event 기반으로 개발한다     - A component에서 직접 kafka로 write 할 수도 있지만, http API로 노출해서 http source connector를 이용해서 kafka로 전달하고 mongo sink connector를 이용해서 db에 넣으면 각 component에서 추가 개발 없이도 B component로 데이터를 전달할 수 있다 1.2.3 새로운 어플리케이션으로 마이그레이션 작업 - db(ex. mysql) -> (mysql sink connect) -> kafka -> new application (consume) 새로운 애플리케이션 개발할 때 기존의 애플리케이션에는 영향을 주지 않기 위해 Kafka Connect를 사용하면 더 쉽게 마이그레이션이 가능할 수 있다. Mysql이나 MongoDB는 Change Data Capture (CDC) 기능을 지원하고 있어서 해당 sink connector를 사용하면 스키마 변경, INSERT, UPDATE, DELETE 모두에 대한 변경은 포착해서 Kafka로 데이터를 스트리밍할 수 있다. 이렇게 되면 기존 애플리케이션에는 전혀 수정하지 않고 새로운 애플리케이션을 개발할 수 있다. 2.내부 구성요소 및 동작 원리 Kafka Connect는 크게 5가지 요소로 되어 있다. - Worker\n    - Connector와 task를 실행하는 프로세스이다\n    - Worker는 REST API 요청에 대한 처리를 담당한다\n        - Connector 등록, 설정, 시작, 종료등의 처리를 해준다\n    - 2가지 모드를 지원한다\n        - standalone : 하나의 process가 connector와 task 실행을 시킨다\n        - distributed\n            - 분사 모드는 kafka connect의 확장성과 자동 결함 허용 기능을 제공한다\n            - 여러 worker 프로세스로 실행시킬 수 있다\n- Connector\n    - Connector는 파이프라인에 대한 추상 객체이고 Task들을 관리하는 역할을 한다\n        - 실행할 작업 수를 결정하고 Task 간의 작업을 나누는 작업\n        - Worker로부터 Task를 위한 설정 값을 가져오고 Task에게 전달하는 작업\n    - 실제 Worker가 Task를 구동시킨다\n- Task\n    - Kafka로부터 데이터를 가져오고나 넣는 작업을 하고 실제 파이프라인 동작 요소이다\n    - Source Task는 source system으로 부터 데이터를 poll하고 worker는 가져온 데이터를 Kafka topic으로 보낸다\n    - Sink Task는 Kafka로부터 Worker를 통해 record를 가져오고 sink system에 record를 쓴다\n    - Task Rebalancing 기능도 제공한다\n- Converter\n    - Worker가 데이터를 수신하면 converter를 사용하여 데이터를 적절한 형식으로 변환한다\n- Transform\n    - Connector를 통해 흘러가는 각 메시지에 대해 변환하는 역할을 한다 2.1 Kafka가 데이터를 스트리밍하는 과정 다음 Sink Conector는 Kafka에서 외부 시스템으로 데이터를 스트리밍할 때의 흐름을 보여준다. Source Connector는 반대로 외부 시스템에서 Kafka로 스트리밍하는 차이가 있지만, 기본 데이터를 스트리밍하는 과정이 비슷하다. 1. Plugin은 각 worker에 배포되는 connector와 task의 구현 아티팩트를 제공한다\n2. Worker는 connector 인스턴스를 시작시킨다\n3. Connector는 데이터 처리를 위해 task를 생성한다\n4. Task는 Kafka를 polling 하기 위해 병렬로 실행되고 record를 반환한다\n5. Converter는 외부 시스템에 적합한 형식으로 레코드를 변환한다\n6. Transform은 record를 정의된 변환 설정에 따라서 filtering, renaming 등의 변환 작업을 한다 이미 위에서 언급된 내용도 있지만, 각 구성 요소와 요소 간의 관계, 역할을 쉽게 이해하는 데 도움이 되는 다이어그램이다. - 하나 이상의 Worker는 서버에서 실행된다\n- Worker는 하나 이상의 Connector Plugin을 가지고 있다\n    - 각 plugin은 connector와 task를 가지고 있다\n- Worker는 topic과 task 간의 데이터를 이동시킨다\n- Worker는 connector와 task를 시작시킨다 2.1 Task Rebalancing Task rebalancing은 새로운 worker가 추가되거나 worker가 강제 종료된 경우에 worker 간의 작업을 다시 나누기 위해 task 재조정이 발생한다. Task rebalancing이 일어나는 경우는 다음과 같다. 1. 클러스터에 새로운 connector가 등록하는 경우\n    - 전체 connector와 task를 각 worker에서 같은 양의 task을 가지도록 재조정을 한다\n2. Tasks의 rebalancing은 task의 수 설정을 변경하거나 connector 설정 값을 변경하는 경우\n3. 하나의 worker 가 failure 되는 경우\n    - Fail task는 활성화된 worker에 다시 할",
    "category": "cloud",
    "tags": [
      "kafka",
      "connect",
      "task",
      "worker",
      "source",
      "sink",
      "카프카",
      "브로커",
      "커넥트",
      "커넥터",
      "converter",
      "transform",
      "consumer",
      "producer"
    ],
    "date": "2022-08-27T00:00:00.000Z"
  },
  {
    "id": "cloud/kcat-사용방법",
    "slug": "cloud/kcat-사용방법",
    "title": "kcat 사용방법",
    "excerpt": "",
    "content": "은 아파치 카프카를 쉽게 테스트하고 디버깅하는데 유용하게 사용할 수 있는 도구이다.  명령어를 통해서 메시지를 보내고 받거나 메타데이터 목록을 확인할 수 있다. 기본적인 사용방법에 대해서 알아보자. 카프카 설치는 로컬환경에서 Kafka 실행하기를 참고해주세요. kcat 설치 여러 방법으로 설치 가능하지만, 본 포스팅에서는 맥기준으로 설명합니다. 로 을 설치한다. kcat 기본 사용방법 기본 Synatx  의 기본 명령어 포맷은 아래와 같다. - \n    - Consume, Produce, Metadata List, Query 모드를 지정한다\n- \n    - 토픽을 지정한다\n- \n    -  브로커 목록을 지정한다 메시지 보내기 (-P) 메시지는 Produce () 모드를 지정해서 메시지를 보낼 수 있다. 필수로 카프카 브로커 ()와 토픽 옵션 ()을 지정해줘야 한다. 을 사용하여 특정 토픽에 쉽게 메시지를 보낼 수 있다.  명령으로 실행하고 원하는 데이터를 입력한 다음 를 눌러 완료한다. 메시지 값 생성하기 잘 보내줬는지 확인하려면  consume 모드로 확인한다. 키와 값을 가진 메시지를 생성하기 메시지를 키와 함께 생성하고 싶은 경우에는 키 구분자 (key delimiter)  옵션으로 원하는 구분자를 같이 지정해서 사용한다. 파일로 메시지 생성하기 메시지를 파일에서도 읽어 드려 메시지를 생성할 수 있다. 파티션 (Partition)은 각 토픽 당 데이터를 분산 처리하는 단위로 카프카에서는 토픽을 여러 파티션에 나눠서 저장하고 카프카 옵션에서 지정한 replica의 수만큼 파티션이 각 브로커에 복제가 된다. 토픽을 여러 파티션으로 설정해두었다면 아래와 같이 파티션 1에 메시지를 보내고 받을 수 있다. 토픽의 파티션의 수 늘리기 토픽의 파티션의 수를 늘려주려면 아래와 같이 카프카에서 제공하는 스트립트를 통해서 파티션을 추가할 수 있다. > 카프카에서는 파티션을 한번 늘리면 줄일 수 있는 방법은 없기 때문에 Real 환경에서는 늘려주기 전에 꼭 필요한 상황인지 판단할 필요가 있다. 메시지 받기 (-D) 토픽에서 모든 메시지 받기 은 기본적으로 추가 옵선 지정없이 토픽에서 처음부터 모든 메시지를 가져온다. N 개만 메시지 받기 모든 메시지를 가져오는 대신 몇개만 메시지를 가져오려면  옵션으로 개수를 지정하면 된다. 특정 오프셋에서 메시지 가져오기  옵션으로 특정 오프셋이후부터 데이터를 가져온다. > 오프셋을 절대 값으로 지정할 수도 있지만, 처음과 끝은 이나   로 지정할 수 있다.\n>\n>  오프셋 값을 음수로 지정하면 끝에서부터 메시지를 가져온다. 출력 포멧 변경하기 기본적으로 은 메시지 값(카프카 레코드의 값)만 출력한다. 출력의 포멧을 변경하려면  옵션으로 아래와 같이 여러 값을 사용해서 정의할 수 있다. Key와 Value, 그리고 Partition, Offset의 값을 읽게 편하게 출력할 수 있다. 메타데이터 조회하기 (-L) 브로커나 현재 토픽에 대한 정보를 확인하려면  옵션으로 확인할 수 있다. 브로커는 총 3대로 구성되어 있고 토픽당 몇개의 파티션으로 구성되어 있는지도 확인할 수 있다. 참고 - https://docs.confluent.io/platform/current/app-development/kafkacat-usage.html\n- https://github.com/edenhill/kafkacat\n- https://dev.to/demaric/learn-how-to-use-kafkacat-the-most-versatile-kafka-cli-client-1kb4\n- https://codingharbour.com/kafkacat-cheatsheet/\n- https://docs.confluent.io/platform/current/tutorials/examples/clients/docs/kafkacat.html#client-examples-kafkacat\n- https://wonyong-jang.github.io/kafka/2021/02/10/Kafka-message-order.html\n- https://redhat-developer-demos.github.io/kafka-tutorial/kafka-tutorial/1.0.x/03-consumers-producers.html",
    "category": "cloud",
    "tags": [
      "kcat",
      "kafka",
      "kafkacat",
      "카프카",
      "브로커",
      "메시지",
      "아파치"
    ],
    "date": "2021-07-20T00:00:00.000Z"
  },
  {
    "id": "cloud/ksqldb-소개",
    "slug": "cloud/ksqldb-소개",
    "title": "ksqlDB 소개",
    "excerpt": "",
    "content": "What ksqlDB (formerly Kafka SQL, KSQL)는 Kafka를 위한 스트리밍 SQL 엔진이다. SQL 인터페이스를 제공하고 있어 익숙한 SQL 구문으로 개발자들이 쉽게 Kafka에서 스트리밍 처리를 할 수 있게 도와준다. ksqlDB에서 제공하는 기능은 다음과 같다. 1.1 Feature - 친숙하고 가벼운 SQL 구문을 통해 관계형 데이터베이스에 유사하게 접근하는 방식과 비슷하게 실시간 스트리밍 처리를 가능\n- ksqlDB는 fault-tolerant, scale이 가능하도록 설계\n- ksqlDB 내에서 Kafka Connect를 관리기능 제공\n- 데이터 필터링, 변환, 집계, 조인, 윈도우 및 세션화를 포함하여 광범위한 스트리밍 작업을 위해 여러 함수 지원\n    - ex. , , , , \n- KSQL 사용자 정의 함수도 구현 가능하도록 지원\n    - 람다 함수도 지원 참고 - https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-reference/functions/\n- https://docs.confluent.io/5.4.0/ksql/docs/developer-guide/udf.html\n- https://docs.ksqldb.io/en/latest/how-to-guides/use-lambda-functions/ 1.2 ksqlDB Architecture ksqlDB client - ksqlDB CLI\n    - MySQL이나 PostgreSQL과 같은 console을 command interface (CLI)를 제공한다\n- ksqlDB UI\n    - Control Center (유료 버전)는 Kafka 클러스터, 브로커, 토픽, Connector, ksqlDB 등을 포함한 주요 구성 요소를 한 곳에서 관리하고 모니터링할 수 있는 GUI 이다 REST Interface - ksqlDB client가 ksqlDB Engine 에 접근하게 도와준다 ksqlDB Engine - KSQL 구문과 쿼리를 실행한다\n- 사용자는 KSQL 구문으로 어플리케이션 로직을 정의하고 엔진은 KSQL 구문을 파싱, 빌드해서 KSQL 서버에서 실행시킨다\n- 각 KSQL서버들은 KSQL 엔진을 인스턴스로 실행시킨다\n- 엔진에서는 RocksDB를 내부 상태 저장소로 사용된다\n    - ksqlDB는 Materialized View를 로컬로 디스크에 저장하는데 RocksDB를 사용한다\n    - RocksDB는 빠른 embedded key-value 저장소이고 library로 제공된다 > \"RocksDB는 Facebook에서 시작된 오픈소스 데이터베이스 개발 프로젝트로, 서버 워크로드와 같은 대용량 데이터 처리에 적합하고 빠른 저장장치, 특히 플래시 저장장치에서 높은 성능을 내도록 최적화되어 있다\" 참고 - https://docs.confluent.io/5.4.3/ksql/docs/index.html\n- https://docs.ksqldb.io/en/latest/operate-and-deploy/how-it-works/\n- https://www.confluent.io/blog/ksql-streaming-sql-for-apache-kafka/\n- https://docs.ksqldb.io/en/latest/tutorials/event-driven-microservice/\n- https://github.com/confluentinc/ksql\n- https://docs.ksqldb.io/en/latest/operate-and-deploy/how-it-works/\n- https://www.confluent.io/blog/ksqldb-architecture-and-advanced-features/\n- https://www.confluent.io/blog/ksqldb-pull-queries-high-availability/?ga=2.35560801.1998071110.1666397521-1519298907.1666271761\n- https://www.confluent.io/blog/how-to-tune-rocksdb-kafka-streams-state-stores-performance/\n- https://www.datanami.com/2019/11/20/confluent-reveals-ksqldb-a-streaming-database-built-on-kafka/\n- https://meeeejin.gitbooks.io/rocksdb-wiki-kr/content/overview.html\n- https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams+Internal+Data+Management\n- https://stackoverflow.com/questions/58621917/ksql-query-and-tables-storage Why ksqlDB를 왜 사용하면 좋은지 알아보아요. 1.Kafka 스트림 처리에 대한 3가지 방법 2. ksqlDB vs Kafka Streams - ksqlDB\n    - Kafka Streams library 기반으로 개발되었다\n    - KSQL CLI interactive 하게 바로 스트리밍 처리를 확인해 볼 수 있다\n    - 익숙한 SQL 구문을 사용해서 빠르게 스트리밍 처리를 해 볼 수 있다\n- Kafka Streams\n    - Kafka 기반의 스트리밍 처리를 할 수 있게 도와주는 라이브러리이다\n    - 더 복잡한 스트리밍 처리가 필요한 경우 Kafka Streams을 사용하는 게 더 좋을 수 있다\n    - ksqlDB와 비교했을 때 학습과 라이브러리에 대한 이해 및 경험치가 더 필요하다 참고 - https://engineering.linecorp.com/ko/blog/applying-kafka-streams-for-internal-message-delivery-pipeline/\n- https://yooloo.tistory.com/m/115\n- https://developer.confluent.io/tutorials/transform-a-stream-of-events/kafka.html#create-the-code-that-does-the-transformation\n- https://laredoute.io/blog/why-how-and-when-to-use-ksql/\n- https://www.slideshare.net/ConfluentInc/ksqldb-253336471 Who ksqlDB는 Confluent 회사에 의해서 2017년부터 개발되었다. History Kafka - 2010년 LinkedIn에서 내부 회사에서 발생하고 있는 이슈들을 해결하기 위해 만들어짐\n- 2011년 Apache Kafka 오픈소스로 세상에 처음 공개\n- 2014년 Confluent 회사 설립\n    - Kafka 공동 창시자가 LinkedIn을 나와서 새로운 회사를 설립 Kafka Connect - 2015년 Kafka 0.9.0.0 relealse 버전에 포함 Kafka Stream - 2016년 Kafka 0.10.0.0 release 버전에 포함 ksqlDB\n- 2017년 KSQL Developer Preview로 공개\n- 2019년 KSQL (Kafka SQL) -> ksqlDB 재브랜딩을 위해 새로운 이름올 변경 참고 - https://docs.ksqldb.io/en/latest/operate-and-deploy/changelog/?ga=2.211885267.679177078.1665747634-256754440.1665501546&gac=1.60161119.1665905457.Cj0KCQjw166aBhDEARIsAMEyZh6DZ-g9fEdHzNf4ywXk1Oj2Q93PLdfgAephLu9UaUpztGKaOoFYaAsqyEALwwcB\n- https://docs.confluent.io/platform/current/installation/versions-interoperability.html#ksqldb\n- https://dbdb.io/db/ksqldb\n- https://www.buesing.dev/post/kafka-versions/\n- https://www.linkedin.com/pulse/kafkas-origin-story-linkedin-tanvir-ahmed/ Where 다음과 같이 여러 회사에서 공식적으로 ksqlDB를 사용하고 있다. 국내에서는 LINE에서 ksqlDB를 이용해서 AB Test Report 시스템을 개선했다고 한다. - Naver LINE\n    - AB Test Report\n    - 기존 시스템 구조에서는 Redis에 저장된 event log를 가져와 join window를 구현함\n    - ksqlDB를 사용해서 아키텍쳐가 단순화됨 (join two streams without redis)\n- ticketmaster - 티켓 판매 회사\n- Nuuly - 옷 렌탈 및 재판재 서비스\n- ACERTUS - 자동차 픽업/배달 서비스\n- optimove - CRM 마케팅 소프트웨어 (w/ AI)를 서비스로 개발 및 판매하는 비상장 회사\n- Bosch: 자동차 및 산업 기술, 소비재 및 빌딩 기술 분야의 선도적 기업\n- Voicebridge: voice-based systems for rural populations in developing countries that lack internet access 참고 - https://stackshare.io/ksql\n- https://ksqldb.io/\n- https://www.confluent.io/ko-kr/product/ksqldb/ How 1. Installation on local-machine 로컬환경에서 쉽게 여러 Kafka 구성요소를 구동하기 위해 로 실행한다. 먼저  파일을 다운로드한다. Confluent Platform stack을 시작시킨다. KSQL Interactive CLI 시작하기 2. KSQL Usage 예제를 통해서 조금 더 ksqlDB에 대해서 알아보자. 2.1 Collections : Stream vs Table 2.1.1 Stream - 영속적으로 무제한의 스",
    "category": "cloud",
    "tags": [
      "kafka",
      "ksql",
      "ksqldb",
      "sql",
      "kstream",
      "stream"
    ],
    "date": "2022-10-28T00:00:00.000Z"
  },
  {
    "id": "cloud/ksqldb-소개-en",
    "slug": "cloud/ksqldb-소개-en",
    "title": "Introducing ksqlDB",
    "excerpt": "",
    "content": "What  (formerly Kafka SQL, KSQL) is a streaming SQL engine for Kafka. It provides an SQL interface that allows developers to easily perform streaming processing in Kafka using familiar SQL syntax. The feature provided by ksqlDB includes: 1.1 Feature - Real-time streaming processing made possible through a familiar and lightweight SQL syntax, similar to accessing relational databases\n- ksqlDB is designed to be fault-tolerant and scalable\n- ksqlDB provides management functionality for Kafka Connect within the ksqlDB environment\n- Support various functions for wide-ranging streaming tasks, including data filtering, transformation, aggregation, joining, windowing, and sessionization\n    - e.g. , , , , \n- Support user-defined functions for KSQL\n    - Also support lambda functions Reference - https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-reference/functions/\n- https://docs.confluent.io/5.4.0/ksql/docs/developer-guide/udf.html\n- https://docs.ksqldb.io/en/latest/how-to-guides/use-lambda-functions/ 1.2 ksqlDB Architecture ksqlDB client - ksqlDB CLI\n    - Provide a command interface (CLI) similar to the console provided by MySQL or PostgreSQL.\n- ksqlDB UI\n    - Control Center (paid version) is a GUI that allows you to manage and monitor key components including Kafka cluster, broker, topic, connector, and ksqlDB in one place. REST Interface - ksqlDB client uses REST API to access ksqlDB engine ksqlDB Engine - Execute KSQL statements and queries\n- User defines application logic using KSQL statements, and the engine parses and builds the statements and executes them on the KSQL server\n- Each KSQL server runs an instance of the KSQL engine\n- The engine uses RocksDB as its internal state store\n    - ksqlDB uses RocksDB to store Materialized Views locally on disk\n    - RocksDB is a fast embedded key-value store provided as a library > \"RocksDB is an open-source database development project started by Facebook, optimized for large-scale data processing workloads such as server workloads, and optimized for high performance on fast storage, especially flash storage.\" Reference - https://docs.confluent.io/5.4.3/ksql/docs/index.html\n- https://docs.ksqldb.io/en/latest/operate-and-deploy/how-it-works/\n- https://www.confluent.io/blog/ksql-streaming-sql-for-apache-kafka/\n- https://docs.ksqldb.io/en/latest/tutorials/event-driven-microservice/\n- https://github.com/confluentinc/ksql\n- https://docs.ksqldb.io/en/latest/operate-and-deploy/how-it-works/\n- https://www.confluent.io/blog/ksqldb-architecture-and-advanced-features/\n- https://www.confluent.io/blog/ksqldb-pull-queries-high-availability/?ga=2.35560801.1998071110.1666397521-1519298907.1666271761\n- https://www.confluent.io/blog/how-to-tune-rocksdb-kafka-streams-state-stores-performance/\n- https://www.datanami.com/2019/11/20/confluent-reveals-ksqldb-a-streaming-database-built-on-kafka/\n- https://meeeejin.gitbooks.io/rocksdb-wiki-kr/content/overview.html\n- https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams+Internal+Data+Management\n- https://stackoverflow.com/questions/58621917/ksql-query-and-tables-storage Why Let's find out why we should use ksqlDB. 1.Three Approaches for Processing Kafka Stream 2. ksqlDB vs Kafka Streams - ksqlDB\n    - Developed based on the Kafka Streams library\n    - Enable interactive streaming processing directly through the KSQL CLI\n    - Allow developers to use familiar SQL syntax to perform streaming processing quickly\n- Kafka Streams\n    - A library that enables streaming processing based on Kafka\n    - May be better suited for more complex streaming processing needs\n    - Require a higher level of understanding and experience with learning compared to ksqlDB Reference - https://engineering.linecorp.com/ko/blog/applying-kafka-streams-for-internal-message-delivery-pipeline/\n- https://yooloo.tistory.com/m/115\n- https://developer.confluent.io/tutorials/transform-a-stream-of-events/kafka.html#create-the-code-that-does-the-transformation\n- https://laredoute.io/blog/why-how-and-when-to-use-ksql/\n- https://www.slideshare.net/ConfluentInc/ksqldb-253336471 Who ksqlDB has been developed by Confluent since 2017. History Kafka - Kafka was developed around 2010 at LinkedIn\n- Open-sourced as Apache Kafka in 2011\n- Confluent was founded in 2014 by one of the Kafka co-founders who left LinkedIn Kafka Connect - Included in Kafka 0.9.0.0 release in 2015 Kafka Stream - Included in Kafka 0.10.0.0 release in 2016 ksqlDB - Release as KSQl developer preview in 2017\n- Renamed to ksqlDB in 2019 from KSQL (Kafka SQL) for rebranding Reference - https://docs.ksqldb.io/en/latest/operate-and-deploy/changelog/?ga=2.211885267.679177078.1665747634-256754440.1665501546&gac=1.60161119.1665905457.Cj0KCQjw166aBhDEARIsAMEyZh6DZ-g9fEdHzNf4ywXk1Oj2Q93PLdfgAephLu9UaUpztGKaOoFYaAsqyEALwwcB\n- https://docs.confluent.io/platform/current/installation/versions-interoperability.html#ksqldb\n- https://dbdb.io/db/ksqldb\n- https://www.buesing.dev/post/kafka-versions/\n- https://www.lin",
    "category": "cloud",
    "tags": [
      "kafka",
      "ksql",
      "ksqldb",
      "sql",
      "kstream",
      "stream"
    ],
    "date": "2023-04-02T00:00:00.000Z"
  },
  {
    "id": "cloud/kubernetes-환경에서-secret-안전하게-관리하기",
    "slug": "cloud/kubernetes-환경에서-secret-안전하게-관리하기",
    "title": "Kubernetes 환경에서 Secret 안전하게 관리하기",
    "excerpt": "",
    "content": "1. 개요 개발한 애플리케이션을 쿠버네티스에 배포하기 위해 helm charts로 애플리케이션에 필요한 설정을 저장한다. Git 저장소에 id와 password와 같은 민감한 정보를 저장하므로 Git 접근 권한이 있는 사용자에게 그대로 노출이 되는 보안 이슈가 있다. 이런 해결하기 위해 Sealed Secrets에 대해서 알아보자. 1.1 동작 원리 Sealed Secrets는 다음과 같은 방식으로 동작한다. - 사용자는 평문으로 된 비밀 데이터를 생성한다\n-  CLI 도구를 사용해 이 데이터를 암호화된 Sealed Secret 리소스로 변환한다\n- SealedSecret 리소스를 쿠버네티스 클러스터에 적용하면, Sealed Secrets 컨트롤러가 이를 복호화하여 일반 Secret 리소스로 변환해준다\n- 복호화 과정은 클러스터 내에서만 이루어지며, 외부에서는 복호화할 수 없다 2. Sealed Secrets 설치하기 > 로컬환경에서 쿠버네티스를 실행할 거라서 minikube를 사용한다 Sealed Secrets를 설치하는 과정은 kubeseal CLI 도구와 Sealed Secrets 컨트롤러를 설치하는 과정으로 나뉜다. 2.1 kubeseal 설치하기 kubeseal CLI는 사용자가 비밀 데이터를 암호화하는 데 사용된다. 설치 방법은 다음과 같다 > 설치는 맥 기준으로 설명한다 2.2 Sealed Secrets Controller 설치하기 Sealed Secrets 컨트롤러는 Kubernetes 클러스터 내에서 SealedSecret 리소스를 복호화하는 역할을 한다. 설치는 여러 방식 (ex. helm, kubectl)이 있지만, 여기서는 helm으로 설치한다. Helm을 사용하여 Sealed Secrets 컨트롤러를 설치한다. 아래 명령어로 설치가 잘 되었는지 확인한다. 3. Secret 생성하고 암호화하기 Sealed Secrets를 사용하여 비밀 데이터를 생성하고 암호화하는 방법을 알아보자. 3.1 샘플 Secret YAML 생성하기 먼저 평문 비밀 데이터를 포함한 YAML 파일을 생성한다. 위 명령어는 아래 을 생성해주고 의  값은 로 암호화된 값이다. 을 수동을 작성할 때 암호로 표시해야 하는 값은 base64로 엔코딩이 되어야 sealed secret를 생성할 수 있다. base64로 엔코딩할 때 꼭  옵션 추가해서 실행을 해야 한다. 없이 실행하면 newline이 추가가 된다는 것을 기억하자. >  옵션으로 실행해야 newline 없이 추가된다\n>\n>  3.2 Secrets 암호화하기 kubeseal CLI 도구를 사용하여 Secret을 암호화된 SealedSecret으로 변환한다. 3.3  쿠버네티스에 Sealed Secrets 적용하기 암호화된 SealedSecret을 Kubernetes 클러스터에 적용한다. 3.4 적용 잘 되었는지 확인해보기  명령어로 이 잘 생성이 되었는지 확인할 수 있다. > 로도 쉽게 확인할 수 있다 > OpenLens로 확인하면 실제 decrypt 된 값도 확인할 수 있다. 4. 참고 - How to create an actually safe secrets for GitOps\n- Managing secrets deployment in Kubernetes using Sealed Secrets\n- How to Encrypt Kubernetes Secrets Using Sealed Secrets in DOKS",
    "category": "cloud",
    "tags": [
      "sealed secrets",
      "sealed secrets controller",
      "secret",
      "kubeseal",
      "k8s",
      "kubernetes",
      "vault",
      "쿠버네티스",
      "helm",
      "charts",
      "보안",
      "git",
      "vault"
    ],
    "date": "2024-09-29T00:00:00.000Z"
  },
  {
    "id": "cloud/ssl-인증서-ngnix-서버에-설치하기",
    "slug": "cloud/ssl-인증서-ngnix-서버에-설치하기",
    "title": "SSL 인증서 Ngnix 서버에 설치하기 (무료 Lets Encrypt 인증서 발급)",
    "excerpt": "",
    "content": "1. 들어가며 웹사이트를 HTTPS로 설정하는 방법에 대해서 알아보자. HTTP -> HTTPS로 적용하려면 아래 절차가 필요하다. - SSL 인증서 발급 받기\n    - letsencrypt에서 SSL 인증서를 무료로 받을 수 있다\n- 서버에 SSL 인증서 설치 및 웹 서버 설정하기 1.1 개발환경 - 서버 : Amazon Linux\n- 웹 서버 : Nginx 서버\n- 적용 사이트 : http://quote.advenoh.pe.kr #2.  도구 설치 및 환경 설정 2.1 Certbot로 인증서 설치하기 Let's Encrypt에서는 certbot 명령어를 제공하여 Let's Encrypt 인증서를 자동으로 발급받거나 개신 할 수 있다. 먼저 certbot 명령어를 설치한다. standalone 방식으로 인증서를 생성해보자. standalone 방식은 certbot이 간이 웹 서버를 돌려 도메인 인증 요청을 처리하는 방식이다. 간이 서버가 80, 443번 포트를 사용하기 때문에 ngnix 서버가 같은 포트를 사용하면 인증서 발급이 안된다. certbot 실행하기 전에 ngnix 서비스를 종료시켜두자. 이상없이 생성되었으면 아래 화면과 같이 메시지를 볼 수 있다. 2.2 Ngnix 서버 설정 변경하기 이제 ngnix 웹 서버에 HTTPS 설정을 추가해보자. 80 관련 server 설정 아래 추가로 아래를 넣어주자. Ngnix 서버를 재시작하고 https 주소로 접속해보자. 2.3 Troubleshooting 및 기타 설정 2.3.1 Let's Encrypt 인증서 자동으로 갱신하기 Let's Encrypt 인증서는 3개월마다 개신을 해줘야 한다. cron 설정으로 자동으로 개신할 수 있도록 설정해두자. 2.3.2 Problem binding to port 80 오류 메시지 ngnix 서버를 종료시키고 다시 certbot을 실행하면 된다. 2.3.3 http -> https redirect 시키기 http 접속시 https로 redirect 하도록 ngnix 설정을 변경하자. 3. 마무리 letsencrypt에서 SSL 인증서를 무료로 제공하고 쉽게 설치할 수 있는 certbot도 제공한다. certbot 명령어로 거의 5분 안에 https를 설정할 수 있었다. 전체 ngnix은 gist를 참고해주세요. 4. 참고  SSL 인증서 설치\n     https://levelup.gitconnected.com/how-to-install-ssl-certificate-for-nginx-server-in-amazon-linux-2986f51371fb\n     https://medium.com/@devAsterisk/certbot%EC%9D%84-%ED%86%B5%ED%95%9C-%EB%AC%B4%EB%A3%8C%EC%9D%B8%EC%A6%9D%EC%84%9C-%EC%83%9D%EC%84%B1-707f86b642d1\n     https://elfinlas.github.io/2018/03/19/certbot-ssl/\n Cron 표현식\n     https://crontab.guru/#0109/3",
    "category": "cloud",
    "tags": [
      "letsencrypt",
      "http2",
      "ssl",
      "certificate",
      "ngnix",
      "인증서"
    ],
    "date": "2020-10-01T00:00:00.000Z"
  },
  {
    "id": "cloud/도커-이미지-다른-도커-registry로-복사하기-skopeo",
    "slug": "cloud/도커-이미지-다른-도커-registry로-복사하기-skopeo",
    "title": "도커 이미지 다른 도커 registry로 복사하기 - Skopeo",
    "excerpt": "",
    "content": "1. 개요 최근 들어 우리 회사는 다양한 쿠버네티스 클러스터 환경에 제품을 배포해야 하는 상황이 많아졌다. 동일한 도커 이미지를 여러 쿠버네티스의 도커 레지스트리에 복사해야 했다. 기본 도커 명령어를 사용하여 이미지를 로컬에 다운로드한 후, 다른 도커 레지스트리로 푸시하는 방식으로 작업을 진행했다. 여러 도커 명령어를 실행해야 하지만, shell 스크립트로 작성하면 다소 번거로움은 줄일 수 있어서 그냥 사용했었다. 근데, 직장 동료의 추천으로 Skopeo 라는 CLI를 알게 되었고 기존 도커 명령어로 실행하는 것보다 더 쉽게 도커 이미지를 다른 레지스트리로 복사할 수 있어서 Skopeo는 뭔가 어떻게 사용하는지 공유 차원에서 정리를 해둔다. 2. Skopeo란 Skopeo는 컨테이너 이미지를 다루기 위한 명령줄 도구로, 이미지의 복사, 검사, 서명, 삭제 등 다양한 작업을 지원한다. Skopeo는 Docker 엔진 없이도 이미지 레지스트리 간의 작업을 수행할 수 있어, 클라우드 네이티브 환경에서 특히 유용하다. > M1에서는 harbor registry 서버 설치가 잘 안되어서 Harbor 사이트에서 제공하는 데모 서버를 이용한다. New Project를 생성해서 사용하면 된다. \n> 참고: Test Harbor with the Demo Server 2.1 Skopeo 설치 맥에서는 Homebrew 명령어를 통해서 아래와 같이 설치한다. 2.2 Skopeo 사용방법 2.2.1 이미지 검사하기 이미지를 로컬에 다운로드하지 않고도 원격 레지스트리의 이미지를 검사할 수 있다. 이미지의 메타데이터를 아래와 같이 확인할 수 있다. M1 맥북에서 실행하면 위와 같이 오류가 발생하는데, 아래 옵션으로 OS와 Arch를 지정해서 실행해야 한다. 2.2.2 다른 도커 레지스트리로 이미지 복사하기 Skopeo의 강력한 기능 중 하나는 이미지를 한 레지스트리에서 다른 레지스트리로 복사하는 것이다. 예를 들어, Docker Hub에서 내부 프라이빗 레지스트리로 이미지를 복사하려면 다음 명령어를 사용하면 된다. 2.2.3 이미지 삭제하기 레지스트리에서 더 이상 필요하지 않은 이미지를 아래 명령어로 삭제할 수 있다. 2.2.4 태그 목록 조회 도커 레지스트리에서 제공하는 이미지 목록을 확인하려면  옵션을 사용하면 된다.  2.2.5 로그인 인증  명령어는 특정 레지스트리에 로그인하고 인증 토큰을 에 저장을 해서 자격 증명을 반복해서 입력할 필요가 없다. authfile 기본 파일 위치는 에 저장이 된다. > 레지스트리에 로그인하기 >  명령어 옵션 사용하기 3. 참고 본 포스팅에서 설명한 예제 파일은 여기에서 찾아볼 수 있다. - skopeo github",
    "category": "cloud",
    "tags": [
      "skopeo",
      "쿠버네티스",
      "Kubernetes",
      "docker",
      "도커",
      "도커 이미지",
      "도커 레지스트리",
      "레지스트리",
      "registry",
      "docker registry",
      "mirror docker registry",
      "도커 이미지 복사"
    ],
    "date": "2024-08-26T00:00:00.000Z"
  },
  {
    "id": "cloud/라즈베리파이에-도커-설치하기",
    "slug": "cloud/라즈베리파이에-도커-설치하기",
    "title": "라즈베리파이에 도커 설치하기",
    "excerpt": "",
    "content": "라즈베리파이에 도커 설치하는 방법에 대해서 알아보겠습니다. 사전 조건 - 라즈베리파이에 Raspbian OS 설치\n    - 참고 : 모니터없이 Raspberry Pi 4 OS 설치\n- SSH 연결 활성화 라즈베리파이에 도커 설치하기 스크립트로 도커 설치 도커에서 제공하는 설치 스크립트를 다운로드해서 바로 실행시킨다. 도커 그룹에 non-root 사용자 추가하기 기본적으로 도커 컨테이너를 실행시키려면 root 권한이 필요하다. sudo로 실행할 수 있지만, root 권한이 없는 사용자도 실행하고 싶은 경우 docker 그룹에 사용자를 추가하면 된다. 로그아웃하고 다시 로그인해야 실행이 가능하다. 지금까지 도커 설치가 잘 되었는지 도커 명령어를 실행해보자. 잘 설치가 되었으면 도커 버전과 추가 정보를 확인할 수 있다. Hello World 컨테이너 실행해서 테스트해보기 도커 설치가 잘 되었는지 테스트하는 가장 좋은 방법은 마지막으로 Hello World 컨테이너를 실행하는 것이다. 아래 명령어로 컨테이너를 실행시킨다. 참고 - https://dev.to/elalemanyo/how-to-install-docker-and-docker-compose-on-raspberry-pi-1mo\n- https://www.boolsee.pe.kr/installation-and-running-of-docker-in-raspberry-pi-buster/\n- https://phoenixnap.com/kb/docker-on-raspberry-pi",
    "category": "cloud",
    "tags": [
      "raspberry",
      "docker",
      "install",
      "도커",
      "라즈베리파이",
      "설치"
    ],
    "date": "2021-07-18T00:00:00.000Z"
  },
  {
    "id": "cloud/로컬환경에서-kafka-실행하기-with-akhq",
    "slug": "cloud/로컬환경에서-kafka-실행하기-with-akhq",
    "title": "로컬환경에서 Kafka 실행하기 (with AKHQ)",
    "excerpt": "",
    "content": "로컬환경에서 Kafka를 실행하는 방법에 대해서 알아보자. 개인 맥북이 M1이라서 M1 기준으로 실행방법에 대해서 기술합니다. 1.실행 조건  명령어를 이용해서 Kafka를 실행하기 위해서 아래 조건이 만족되어야 실행 가능하다. 아래 조건은 기본이라서 본 포스팅에서 추가로 설명하지 않습니다. - Docker 실행중이어야 한다\n- 가 설치되어 있어야 한다 2.Kafka 실행하기 사용자가 쉽게 Kafka를 실행할 수 있도록 여러 오픈소스 프로젝트에서 나  설정 파일을 제공하고 있다. 공개된 프로젝트의 파일을 이용해서 Kafka를 실행해보자 2.1 Kafka 컨포넌트만 실행하기 wurstmeister에서 제공하는  파일을 이용해서 도커를 실행한다.  명령어로  포트가 접속이 되는지 확인한다. 가 출력되는 것을 볼 수 있어서 Kafka 서버가 잘 구동되었음을 확인할 수 있다. 2.2 Kafka + Akhq (UI)도 같이 실행하기 Kafka 서버만 구동하면 topic을 생성하고 조회하려면, kafka binary를 다운로드 받아서 kafka 명령어로 실행해야 하고 여러 옵션을 잘 알고 있어야 한다. 보다 손쉽게 Kafka를 이용하고 관리할 수 있도록 여러 Kafka Manager UI를 제공하는데, 그중에 대표적으로 많이 사용하는 AKHQ도 같이 구동해보자. AKHQ에서 이미  설정 파일을 제공하고 있어서 이 파일을 사용하면 된다. 기본 설정으로 실행도 잘 되지만, 아래 설정도 같이 해주자. - M1에서도 실행 가능하도록  옵션 추가\n- 로컬환경에서도 Kafka 서버 접속되도록  설정 추가 AKHQ에서 제공하는 를 실행하면 아래 컴포넌트도 같이 실행된다. 사용하지 않는 컴포넌트를 comment out 시켜도 무방하다. 개발 시 여러 컴포넌트를 사용할 수도 있어서 일단 그대로 둔다. 이제 Kafka + AKHQ 구동시켜보자. - Kafka\n- AKHQ\n- Kafka Connect\n- Schema Registry Kafka 서버도 로컬환경에서 접속 가능한지  명령어로 확인하다. AKHQ UI로 접속하려면, http://localhost:8080로 접속하면 된다. 참고 - https://akhq.io/docs/#installation - https://github.com/wurstmeister/kafka-docker - https://towardsdatascience.com/overview-of-ui-tools-for-monitoring-and-management-of-apache-kafka-clusters-8c383f897e80",
    "category": "cloud",
    "tags": [
      "kafka",
      "docker",
      "local",
      "akhq",
      "로컬환경",
      "카프카"
    ],
    "date": "2022-08-07T00:00:00.000Z"
  },
  {
    "id": "cloud/맥북에서-개인용-docker-registry-구축하고-이미지-관리하기",
    "slug": "cloud/맥북에서-개인용-docker-registry-구축하고-이미지-관리하기",
    "title": "맥북에서 개인용 Docker Registry 구축하고 이미지 관리하기",
    "excerpt": "",
    "content": "1. 개요 개발한 애플리케이션을 배포하거나 테스트 환경에서 활용하기 위해  이미지를 직접 관리하고자 할 때, 개인용 를 운영하는 것이 유용하다. 특히, 외부 레지스트리에 의존하지 않고 로컬 네트워크나 개발용으로 자체적인 이미지 저장소를 갖추면 배포 속도와 보안 측면에서도 장점이 있다. 이번 글에서는 맥북 로컬 환경에서  서버를 구축하고, 개발한 앱 이미지를 업로드 및 실행하는 방법을 단계별로 소개한다. 2. Docker Registry 서버 구축하기 2.1 로  서버 실행하기 는  이미지를 제공하고 있어 별도로 복잡한 설치 없이 컨테이너 하나로  서버를 실행할 수 있다. 다음 명령어를 통해 기본 사용자 인증과 로컬 저장소 폴더 설정을 적용한 를 실행한다. 1. 사용자 로그인 인증 파일 생성 입력한 과 는  파일에 저장이 된다. 2. Docker Registry 컨테이너 실행 - : 환경 변수를 설정해서  인증 방식을 사용하고 인증 파일의 경로를 지정한다\n-  : 이미지 데이터가  머신에 저장하도록 설정하여 도커를 재시작해도 기존 데이터로 동작하도록 한다 ------ 2.2 Docker 이미지 업로드 및 실행 이제 개인 에 도커 이미지를 업로드해보고 업로드한 도커를 실행해보자. 암호를 입력해야 하기 때문에  로그인시 아래 명령어와 같이 /password를 입력해서 로그인을 한다. 도커 실행 테스트를 위해  도커 이미지를 개인  서버에 올려본다. 업로드 된 버전으로 실행된 것을 확인할 수 있지만, 로도 확인해볼 수 있다. 3. 마무리 이 글에서는 맥북 로컬 환경에서  서버를 구축하고, 개발한  이미지를 업로드 및 실행하는 전 과정을 다뤄보았다. 공개  서버로 업로드하기 어려운 이미지의 경우에는 이렇게 개인용 를 구축해서 이미지 관리를 쉽게 할 수 있다. > 만약 팀 단위로 이미지 접근 권한을 나누거나, 사용자 인증, 취약점 스캔, UI 기반의 이미지 관리 기능이 필요하다면 Harbor 를 고려해볼 수 있다. 는 에서 관리하는 오픈소스 로, 더 강력하고 정교한 이미지 관리 기능을 제공한다. 4. 참고 - [[Docker\\] Docker Registry(Private Repository, Http)](https://lucas-owner.tistory.com/89)\n- Private Docker Registry 구축 및 보안 강화",
    "category": "cloud",
    "tags": [
      "docker",
      "docker registry",
      "도커 레지스트리",
      "개인용 도커 레지스트리 구축",
      "private registry",
      "도커 이미지 관리",
      "harbor"
    ],
    "date": "2025-05-05T00:00:00.000Z"
  },
  {
    "id": "cloud/맥에서-minikube로-로컬-kubernetes-클러스터-쉽게-구축하기",
    "slug": "cloud/맥에서-minikube로-로컬-kubernetes-클러스터-쉽게-구축하기",
    "title": "맥에서 Minikube로 로컬 Kubernetes 클러스터 쉽게 구축하기",
    "excerpt": "",
    "content": "1. 개요 Minikube란? 는 로컬 환경에서 가볍게  클러스터를 실행할 수 있는 도구이다. 를 실습하거나 개발 환경에서 테스트할 때 유용하다. Mac, Linux, Windows에서 실행할 수 있으며, 가상화 기술을 이용해 클러스터를 구성한다.  Kubernetes 클러스터 실행 방식 비교  클러스터를 실행하는 방법은 여러 가지가 있으며, 각각 장단점이 있다.  | 항목 | Kind | Minikube | Docker Desktop Kubernetes | Rancher Desktop |\n| --- | --- | --- | --- | --- |\n| 실행 방식 | Docker 컨테이너 기반 | 가상화 기반 (Docker, VirtualBox 등 지원) | Docker 내장 K8s 기능 활용 | 여러 K8s 배포판 선택 가능 |\n| 성능 | 가볍고 빠름 | 다양한 환경 지원, 다소 무거움 | Mac/Windows에서 최적화됨 | 다소 무거움 |\n| LoadBalancer 지원 | 기본적으로 미지원 (추가 설정 필요) | 기본적으로 미지원 (추가 설정 필요) | 기본 제공 | 기본 제공 |\n| 사용 용도 | 개발 및 테스트 환경 | 개발 및 로컬 테스트 환경 | 로컬 개발 및 간단한 테스트 | 다양한 K8s 환경 실습 |\n| 설치 난이도 | 간단함 | 비교적 쉬움 | 기본적으로 포함됨 | 다소 설정 필요 | minikube의 경우에는 k8s 구축하는 게 제일 쉽기도 하고 간단한 pod 올리고 테스트할 때 많이 사용되기도 해서 어떻게 사용하는지 알아보자.  --- 2. 맥에서 Minikube로 클러스터 구축하기 2.1 Minikube 설치 Mac에서는 를 사용하여 간단히 를 설치할 수 있다.  설치가 완료되었는지 확인한다.  2.2 Minikube 시작 를 실행하기 전에 Docker Desktop이 실행 중인지 확인해야 한다. 는 기본적으로 를 사용하여  클러스터를 구동한다.   클러스터를 시작한다.  > 개인적으로는 가 명령어가 길어서 shell  설정해서 사용하고 있다.\n>  2.3 애플리케이션 배포하기 간단한 테스트를 위해 Echo Server 애플리케이션을  클러스터에 배포해보자.  tutorial-go repo에 이미 echo-server  이 생성되어 있어서 아래 명령어로 실행한다.  배포가 정상적으로 이루어졌는지 확인한다.  2.4 외부에서 접속해보기 Minikube Service 명령어로 외부로 노출하는 방법 는 기본적으로 를 지원하지 않으므로,  명령어를 이용해 외부에서 접근할 수 있도록 한다.  출력되는 URL을 확인한 후, 해당 주소로  요청을 보내서 잘 동작하는지 확인한다.  Port Forwarding 하는 방법 port forwarding으로 특정 포트를 외부에 노출할 수 있다.  2.5 Minikube Dashboard 는 기본적으로  대시보드를 포함하고 있다. 실행하려면 다음 명령어를 입력한다.  브라우저가 열리면서  클러스터 상태를 시각적으로 확인할 수 있다.  2.6 Minikube 중지 및 삭제 Minikube 중지 Minikube 으로 를 중지시킬 수 있다.  Minikube 삭제  클러스터를 완전히 삭제하려면 다음 명령어를 실행한다.  --- 3. 마무리 이번 포스트에서는 Mac 환경에서 를 사용하여  클러스터를 구성하는 방법을 살펴보았다. 는 로컬환경에서 가장 쉽게  클러스터를 구축하기 테스트하기 좋아서 가장 선호하는 도구중에 하나이다.  4. 참고 - [[Kubernetes] Mac OS에 minikube로 Cluster 설치하기](https://wanbaep.tistory.com/19)\n- MacOS 에서 Minikube 로 Kubernetes 입문하기",
    "category": "cloud",
    "tags": [
      "minikube",
      "kind",
      "k8s",
      "kubernetes",
      "클러스터",
      "mac",
      "로컬환경"
    ],
    "date": "2025-03-29T00:00:00.000Z"
  },
  {
    "id": "cloud/포트포워딩-없이-kubernetes-접근하기-kubevpn으로-네트워크-연결",
    "slug": "cloud/포트포워딩-없이-kubernetes-접근하기-kubevpn으로-네트워크-연결",
    "title": "포트포워딩 없이 Kubernetes 접근하기 - KubeVPN으로 네트워크 연결",
    "excerpt": "",
    "content": "1. 개요 KubeVPN 란? 은  클러스터와 로컬 환경 간의 원활한 네트워크 연결을 제공하는 도구이다. 기존의 port forwarding 방식과는 다음과 같은 차이점이 있다. | 방식                | 설명                                                         |\n| ------------------- | ------------------------------------------------------------ |\n| Port Forwarding | 특정 포트를 로컬로 전달하여 단일 서비스에 접근 가능하지만, 여러 포트나 복잡한 네트워크 설정이 필요할 경우 불편함 |\n| KubeVPN         | 전체 네트워크를 클러스터 내부처럼 확장하여 Pod IP 및 네이티브 DNS를 직접 사용 가능 | KubeVPN의 Technical Architecture 의 아키텍처는 클러스터 내부와 로컬 환경을 VPN 터널을 통해 연결하여 원활한 통신이 가능하도록 구성된다. 주요 구성 요소는 다음과 같다. - Proxy Pod: 클러스터 내부에서 네트워크 터널링 역할 수행\n- VPN Client: 로컬 머신에서 클러스터 네트워크로 접근할 수 있도록 설정\n- Traffic Routing: HTTP 헤더 조건 등을 기반으로 로컬 환경으로 트래픽을 리디렉션 1.1 KubeVPN의 주요 특징 1. Direct Cluster Networking - Pod IP 주소로 직접 통신 가능: 클러스터 내부처럼 Pod 간 네트워크 통신 가능\n- Native Kubernetes DNS Resolution: 클러스터에서 제공하는 DNS를 그대로 활용 가능 2. Route Traffic from Cluster to Local Machine - HTTP 요청 헤더 조건에 따라 트래픽을 로컬 환경으로 리디렉션 가능\n- 원활한 디버깅 및 개발을 위한 로컬 환경과 클러스터 간의 네트워크 연결 지원 3. Multi-Cluster 연결 지원 - 여러 개의  클러스터를 동시에 연결하여 통합 네트워크 환경 구성 가능 ------ 2. KubeVPN 사용하는 방법 >  사용해보기 위해 로컬환경에서 Minikube로 k8s 클러스터를 실행한다. \n> 참고: 맥에서 Minikube로 로컬 Kubernetes 클러스터 쉽게 구축하기 테스트를 위해 샘플 애플리케이션을 배포한다. 2.1 KubeVPN 설치하기 를 사용하여 간편하게 설치할 수 있다. 잘 설치 되었는지 버전을 확인한다. > 💡 Tip: Alias 설정하기  명령어가 길게 느껴진다면 아래와 같이 를 설정하면 편리하다. 2.2 KubeVPN으로 Kubernetes 클러스터 연결하기  클러스터에 연결하려면 다음 명령어를 실행하면 된다. 로 연결 상태를 확인한다. 2.3 네트워크 상태 테스트 Pod의 IP 주소를 확인하고 으로 네트워크 상태를 확인한다. 서비스 IP 주소로 접근이 안되는 경우에는  옵션으로 연결을 해줘야 한다.  클러스터 kube-proxy가  모드를 사용하고 있는 경우 서비스 IP로 접근이 안될 수 있어서 위 옵션으로 연결해서 를 사용하도록 하면 된다. 참고 - Can not access Service IP or Service name, but can access Pod IP? 3. 마무리 여러 pod에 연결하려면 매번 port forwarding을 해줘야해지만,  을 활용하면서 클러스터에 전체 pod에 접속할 수 있어서 원활한 개발 및 디버깅이 가능해졌다 4. 참고 - QuickStart - KubeVPN\n- Using port-forward too often? KubeVPN Can help!\n- kubevpn github",
    "category": "cloud",
    "tags": [
      "kubevpn",
      "k8s",
      "kubernetes",
      "포트포워딩",
      "port forwarding"
    ],
    "date": "2025-04-03T00:00:00.000Z"
  },
  {
    "id": "cloud/헬름으로-kafka-설치하기",
    "slug": "cloud/헬름으로-kafka-설치하기",
    "title": "헬름으로 Kafka 설치하기",
    "excerpt": "",
    "content": "1.들어가며 로컬환경에서 헬름으로 Kafka를 설치하는 방법에 대해서 알아보겠습니다. 2.Kafka 설치 2.1 Helm repo 추가 및 helm으로 설치 Helm Repository에 Bitnami가 없은 경우 아래 명령어로 repository를 추가한다.  명령어로 kafka를 설치하면 간단하게 설치가 끝난다. > 추가 옵션 없이 설치하면 기본적으로 1개의 broker만 생성이 된다. Broker의 개수를 늘리려면  옵션에 개수를 입력해서 여러 브로커로 Kakfa를 설치한다. 설치 이후 로 설치된 kafka를 확인해보면 잘 되는 것을 볼 수 있다. 3.Kafka 테스트해보기 Kafka 설치 이후 정상적으로 동작하는지는 간단하게 브로커 서버에 메시지를 보내고 브로커로부터 메시지를 잘 받는지 확인해서 테스트해보자. 3.1 Producer - 브로커에 메시지 보내기 헬름으로 Kafka를 설치하면 설치 이후 kafka 브로커에 접속하고 테스트할 방법도 친절하게 터미널에서 알려준다. 가이드받은 내용으로 동일하게 테스트를 한다.  스크립트를 실행하면  토픽에 메시지를 계속 쓸 수가 있다. 메시지는 broker에 계속 쌓이게 된다. 3.2 Consumer - 브로커로부터 메시지 받기 브로커로부터 메시지를 받기 위해서 별도 터머널에서 kafka client pod에 접속한다. 지금까지 kafka에 저장된 메시지를 모두 받으려면  옵션 사용해서 확인한다. 4. 정리 본 포스팅에서는 헬름 차트로 쉽게 Kafka를 설치해보고 kafka client pod에 포함된 여러 script를 사용해서 메시지를 보내고 받는 테스트까지 해보았습니다. 다음 시간에  utility 명령어로도 동일하게 아래와 같이 테스트가 가능합니다. 에 대한 사용 방법은 다음 포스팅에서 기다려주세요. > bitnami/kafka 이미지는 non-root 도커 이미지라서 root 권한으로 다른 패키지를 설치를 할 수 없는 듯하다. (혹시 방법을 아시는 분은 댓글 부탁드립니다)\n>\n> 참고로 저는 다른 도커 이미지를 사용해서 kafkacat를 설치했습니다.\n>\n> $ kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il\n>\n> $ apt-get update && apt-get install kafkacat 테스트해보았던 화면으로 마무리하겠습니다. 오늘도 수고하셨습니다 :) 5. 참고 - https://artifacthub.io/packages/helm/bitnami/kafka",
    "category": "cloud",
    "tags": [
      "kafka",
      "kubernetes",
      "docker",
      "helm",
      "bitnami",
      "카프카",
      "쿠버네티스",
      "헬름",
      "차트"
    ],
    "date": "2021-07-18T00:00:00.000Z"
  },
  {
    "id": "database/jpa-n1-문제-해결방법",
    "slug": "database/jpa-n1-문제-해결방법",
    "title": "JPA N+1 문제 및 해결방법",
    "excerpt": "",
    "content": "1. 들어가며 JPA로 작업하다 보면 N+1 문제에 맞닥뜨리게 되는데요. N+1은 언제 발생할 수 있는 이슈이고 이를 해결하기 위해서 어떤 방법들이 있는지 알아보겠습니다. 2. 개발 환경 포스팅에서 언급한 코드는 github에 올라가 있습니다.  OS : Mac OS\n IDE: Intellij\n Java : JDK 1.8\n Source code : github\n Software management tool : Maven 3. N+1 문제 및 해결 방법 JPA에서 N+1 발생 시 성능에 큰 영향을 줄 수 있기 때문에 JPA로 개발하고 있다면 꼭 알아두어야 하겠습니다. N+1은 언제 발생할 수 있는 같이 알아보겠습니다. 와  엔티티는 다음과 같습니다. 3.1 N+1 문제 발생 케이스 3.1.1 즉시 로딩 (fetchType.EAGER) 변경후 findAll()로 조회하는 경우 와  엔티티 간에 다대일 양방향 연관 관계입니다. @OneToMany 언노테이션의 fetch의 기본값은 지연 로딩이지만, 즉시 로딩으로 변경하면 N+1 문제가 발생할 수 있습니다.  메서드로 Post 전체를 조회해보겠습니다. 4개의 와 각 에 2개의 를 생성하고 나서 findAll() 메서드로 조회합니다. 실제 실행되는 쿼리를 살펴보면 먼저  select 쿼리를 실행합니다. 그리고 해당 에 대해서 를 조회하기 위해서 의 수만큼 4번의 쿼리가 추가로 발생합니다. 데이터의 수만큼 조회하는 것을 N+1 문제라고 합니다. 데이터가 많을수록 쿼리 해야 하는 수가 많아져서 성능에도 큰 영향을 주게 됩니다. 가장 빠르게 해결하는 방법은 지연 로딩으로 변경하는 것입니다. 변경 이후  메서드로 호출하면 지연 로딩이기 때문에  select 쿼리만 실행됩니다. 지연 로딩은 실제 의 값을 조회하는 경우에만 해당 select 쿼리가 발생합니다. 이미 짐작하셨겠지만, loop으로 조회하면 즉시 로딩하는 것과 같은 결과가 발생합니다. 3.1.2 지연 로딩(LAZY) 변경 + Loop으로 조회하는 경우 @OneToMany에서 fetch를 지연 로딩으로 변경한 이후에 loop으로 조회해보겠습니다. [3.1.1]()에서와 같이 동일하게 N+1 이슈가 발생합니다. 3.1.3 N+1이 발생하는 원인 에 정의한 인터페이스 메서드를 실행하면 JPA는 메서드 이름을 분석해서 JPQL를 생성하여 실행하게 됩니다.  JPQL은 SQL을 추상화한 객체지향 쿼리 언어로서 특정 SQL에 종속되지 않고 엔티티 객체와 필드 이름을 가지고 쿼리를 합니다. 그면 지연 로딩 + loop으로 조회 시 왜 N+1 쿼리가 생성이 되어 실행되는지 알아보겠습니다. (1) 지연로딩으로 findAll() 실행시  객체 관련된 정보를 조회합니다. (2) 여기서 Comment 정보를 조회하면, Post에 대한 조회는 이미 끝난 상태라서 JOIN으로 쿼리가 생성이 안 됩니다. 단지 Post에 대한 정보 ID로 조회할 수밖에 없어서 where comment.postId=? 형식으로 JPQL 쿼리를 생성합니다. 이로 인해 매번 조회 쿼리가 생성이 되어 N 번 실행하는 이슈가 발생합니다. 3.2 해결 방안 N+1을 어떻게 해결할 수 있는지에 대해서 알아보겠습니다. 3.2.1 JPQL 페치 조인 사용 - 추천 JPQL에 fetch join 키워드를 사용해서 join 대상을 함께 조회할 수 있습니다.  조회 시 도 같이 join 해서 조회해옵니다. 지연 로딩 설정 이후에 loop을 사용하면 그전 예제에서는 N+1이 발생했지만,  메서드 실행때에는 관련 대상을 한 번에 조회하여 N+1 이슈가 발생하지 않습니다. 로그에서도 left outer join으로 조회해 오는 것을 볼 수 있습니다. 3.2.2 Batch Size 지정 + 즉시 로딩 JPQL 페치 조인 대신 Batch 크기를 지정하는 방법도 있습니다. @BatchSize 어노테이션에 size를 지정하고 fetch 타입은 즉시로 설정합니다. 로 호출할 때마다 where in 쿼리를 이용해서 배치 사이즈만큼 조회해옵니다. 배치 사이즈를 넘는 경우에는 추가로 조회해오는 쿼리가 생성됩니다. Batch 사이즈 지정으로 해결하는 방법은 글로벌 패치 전략을 즉시 로딩으로 변경해야 하고 또한 배치 사이즈만큼만 조회할 수 있어서 N+1 문제를 완벽하게 해결하지 않아 권장하는 해결방법은 아닙니다. 4. FAQ\n4.1 JPA의 글로벌 페치 전략 기본 값은 어떻게 되나요? - 즉시 로딩 (EAGER)\n    - @OneToOne\n    - @ManyToOne\n- 지연 로딩 (LAZY)\n    - @OneToMany\n    - @ManyToMany 4.2 페치 조인 사용시 주의사항은 없나? 페치 조인은 연관된 엔티티를 한번에 조회할 수 있어서 조회 횟수를 줄여 성능 최적화시 많이 사용됩니다. 하지만, 페치 조인은 다음과 같은 한계점이 존재합니다. 참고 - 책 : 자바 ORM 표준 JPA 프로그래밍 - 페체 조인에 alias 별칭을 사용할 수 없다\n- 둘 이상의 컬렉션을 페치할 수 없다\n    - 콜렉션 구현체에 따라서 페치도 가능하지만, 안되는 경우도 있어서 주의가 필요하다\n- 컬렉션을 페치 조인하면 paging API를 사용할 수 없다\n    - 컬렉션을 페치 조인하고 페이징 API를 사용하면 경고 로그를 남기면서 메모리에서 페이징 처리를 한다.\n        - 데이터가 많아지면, 메모리 초과 예외가 발생할 수 있다\n    - 컬렉션(일대다)는  페이징 API를 사용할 수 없다\n        - 단일 값 연관 필드(일대일, 다대일)에서는 페치 조인을 사용할 수 있다. 5. 참고 - JPA N+1\n    - https://cheese10yun.github.io/jpa-nplus-1\n    - https://lng1982.tistory.com/298\n    - https://tech.wheejuni.com/2018/06/16/jpa-cartesian/\n- 책 : 자바 ORM 표준 JPA 프로그래맹\n    - <a href=\"http://www.yes24.com/Product/Goods/19040233?scode=032&OzSrank=2\"></a>",
    "category": "database",
    "tags": [
      "jpa",
      "N1",
      "fetch",
      "spring",
      "database",
      "batchsize",
      "페치조인",
      "배치사이즈",
      "데이터베이스",
      "스프링",
      "스프링부트"
    ],
    "date": "2019-12-10T00:00:00.000Z"
  },
  {
    "id": "database/jpa-다대일-many-to-one-연관관계",
    "slug": "database/jpa-다대일-many-to-one-연관관계",
    "title": "JPA 다대일(N:1)+일대다(1:N) @ManyToOne, @OneToMany 연관관계",
    "excerpt": "",
    "content": "1. 들어가며 JPA 연관관계 매핑에 대한 내용은 JPA 연관관계 매핑 정리 포스팅을 참고해주세요. 이번 포스팅에서는 JPA에서 가장 자주 사용하는 다대일(N:1)과 그 반대 방향인 일대다(1:N) 연관관계에 대해서 알아보겠습니다. > - Post (일)\n> - Comment (다)\n    >   - 테이블에서는 다쪽에 외래 키가 존재한다\n>   - 양방향 관계에서는 다쪽이 연관관계의 주인이 된다\n> <img src=\"image1.png\" style=\"zoom:50%;\" /> 2. 개발 환경 작성한 샘플 코드는 아래 깃허브 링크를 참고해주세요.  OS : Mac OS\n IDE: Intellij\n Java : JDK 1.8\n Source code : github\n     단방향\n     양방향\n Software management tool : Maven 3. 다대일 (N:1) 연관관계 3.1 다대일 연관관계 3.1.1 다대일 단방향 Post와 Comment 코드를 보면서 알아보겠습니다. Post 엔티티에는 연관관계 관련 어노테이션은 없습니다. Comment 엔티티에만 Post 필드가 있어서 @ManyToOne 어노테이션으로 단방향으로 관계를 맺습니다. - @ManyToOne\n    - 다대일 관계로 설정한다\n- @JoinColumn\n    - 외래 키인 postid를 지정한다 JoinColumn과 ManyToOne 옵션 설정에 대한 설명은 다음과 같습니다. 3.1.1.1 @JoinColumn의 속성 @JoinColumn 어노테이션은 외래 키를 매핑할 때 사용하는 어노테이션이고 기본 속성은 다음과 같습니다. | 속성                                                         | 설명                                                         |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| name                                                         | 매핑할 외래 키의 이름을 지정할 때 사용한다<br />기본 값 : 필드명 +  + 참조하는 테이블의 컬럼명 (ex. postpostid) |\n| referenceColumnName                                          | 외래 키가 참조하는 대상 테이블의 컬럼명을 의미한다<br />기본 값 : 테이블의 기본 키 컬럼명(ex. postid) |\n| unique<br />nullable<br />insertable<br />updatable<br />columnDefinition<br />table | @Column의 속성과 같다                                        | 3.1.1.2 @ManyToOne의 속성 @ManyToOne 어노테이션은 다대일 연관관계로 매핑할 때 사용되고 속성에 따라서 쿼리 구문 생성이 조금씩 다르게 생성됩니다. 조회하는 경우에는 쿼리가 어떻게 생성이 되는지 로그로 확인할 필요가 있습니다. | 속성         | 설명                                                         |\n| ------------ | ------------------------------------------------------------ |\n| optional     | true이면 해당 객체에 null이 들어갈 수 있다는 의미이다. <br />참고로 @Column 어노테이션에서도 nullable=true로 세팅해도 null이 들어갈 수 있다<br />기본 값 : true<br />옵션 설정에 따라 select 구문 쿼리 어떻게 생성되는지 FAQ 4.3을 참고해주세요<br /> |\n| fetch        | fetchType이 EAGER이면 연관된 엔티티를 바로 로딩한다. <br />fetchType이 LAZY이면 연관된 엔티티를 바로 로딩하지 않고 실제로 해당 객체를 조회할 때 해당 엔티티를 로딩한다<br />기본값 <br />@ManyToOne=FetchType.EAGER<br />@OneToMany=FetchType.LAZY |\n| cascade      | 영속성 전이 설정을 할 수 있다. 설정 값은 아래 cascadeType을 참고해주세요. |\n| targetEntity | 연관된 언테티의 타입 정보를 설정하는데 거의 사용하지 않는다. | 3.1.1.3 CascadeType의 값 > Post(부모) -> Comment(자식) Cascade 옵션은 부모 엔터티를 영속 상태로 만들 때 연관관계로 맺어진 자식 엔터티도 함께 영속 상태로 만들어 주고 싶을 때 설정할 수 있는 기능입니다. 여러 값은 다음과 같이 설정할 수 있습니다. Post와 Comment관계에서는 별도로 cascade 설정은 하지 않았습니다. Post를 생성할 때 Comment는 없을 수 있으니까요. | 속성 값 | 설명                                                         |\n| ------- | ------------------------------------------------------------ |\n| PERSIST | 부모 엔티티를 저장할 때 자식 엔터티도 같이 저장된다.         |\n| REMOVE  | 부모 엔티티를 삭제하면 자식 엔터티도 같이 삭제된다.          |\n| DETACH  | 부모 엔티티가 detach 상태로 되면 자식 엔터티도 같이 detach 되어 변경사항이 반영되지 않는다. |\n| REFRESH | 부모 엔터티가 DB로부터 데이터를 다시 로드하면 자식 엔터티도 DB로부터 데이터를 다시 로딩한다 |\n| MERGE   | 부모 엔티티가 detach 상태에서 자식 엔터티를 추가/변경한 이후에 부모 엔티티가 merge를 수행하면 자식 엔터티도 변경사항이 적용된다. |\n| ALL     | 모두 cascade 옵셕이 전용된다.                                | 지금까지 ManyToOne 어노테이션에서 적용할 수 있는 여러 옵션을 알아보았습니다. 다대일로 설계한 엔터티가 제대로 저장/조회가 잘되는지 Unit Test에서 확인합니다. Post 객체를 하나를 생성하고 저장이 잘되었는지 조회해서 실제 저장 값을 확인합니다. Comment 엔티티도 저장하고 조회해보겠습니다. Post 객체도 생성해서 저장합니다. 3.1.2 다대일 양방향 다대일 양방향은 Post와 Comment 엔터티에 서로를 참조하는 필드가 존재합니다. Post와 Comment 코드를 보면서 양방향인 경우에는 코드가 어떻게 달라지는 지 알아보겠습니다. Comment 엔티티는 기존과 같습니다. Post -> Comment는 일대다인 관계로 @OneToMany 어노테이션을 사용했고 List<Comment> comments 컬렉션으로 선언하였습니다. 3.1.2.1 연관관계 주인 테이블은 외래 키 하나만 존재하는 반면에 객체를 양방향으로 설정하면 외래 키를 관리하는 곳이 2곳이 생깁니다. 한쪽에서만 관리하도록 하기 위해서 연관관계 주인을 설정할 필요가 있습니다. 다대일에서는 다 쪽이 연관관계 주인이 되므로 @OneToMany에서 mappedBy 속성의 값으로 연관관계 주인을 지정해줘야 합니다. 코드에서는 comments는 연관관계 주인이 아니므로 mappedBy로 Comment 엔티티에 있는 post가 연관관계의 주인이라고 선언하여 알려줍니다. - 연관관계 주인 (ex. Comment.post)\n    - 여기서만 연관관계를 설정할 수 있다.\n        - new Comment().setPost(new Post())\n    - 엔티티 매니저는 연관관계 주인 (ex. Comment.post)를 통해서만 외래 키를 관리한다\n    - DB에 반영이 된다\n- 연관관계 주인 아님 (ex. Post.comments)\n    - 순수한 객체에서만 관리되도록 한다\n        - post.getComments().add(new Comment())\n    - DB에 반영이 안된다 3.1.2.2 연관관계 편의 메서드 양방양으로 설정하여 DB뿐만이 아니라 객체 저장/조회 시에도 제대로 반영하기 위해서 위와 같은 코드를 작성해줘야 합니다. 하지만, 실수로 post.getComments().add(commnet)를 호출하지 않아 양방향이 깨질 수 도 있습니다. 이를 방지 하기 위해서 연관관계 설정 시 실수 없이 설정하도록 편의 메서드를 작성하는 게 좋습니다. > 편의 메서드는 한 곳에만 작성하거나 양쪽 다 작성할 수 있다. 하지만, 양쪽에 다 작성하는 경우에는 무한루프에 빠지지 않도록 체크 조건문을 작성하도록 하자. 3.2 주의사항 3.2.1 무한 루프에 빠지는 경우 영방향 매핑때에는 무한 루프에 빠질 수 있어서 주의가 필요합니다. 예를 들어 Comment.toString()에서 getPost()를 호출하게 되면 무한 루프에 빠질 수 있습니다. - 엔티티를 JSON으로 변환하는 경우\n    - Jackson에서 Infinite Recursion에 해결하는 방법을 참고해주세요\n- toString() 사용시\n    - Lombok 라이브러리 사용시에도 발생할 수 있어 toString(exclude={##, ##})으로 제외시킨다 4. FAQ 4.1 언제 양반향, 단방향을 사용해야 하나? 비지니스 로직에 따라서 무엇을 사용할 지 결정하면 됩니다. - 단반향     - ex. 주문상품(고객이 주문한 상품 정보) -> 상품 (상품에 대한 정보)\n    - 주문상품에서 상품에 대한 정보를 참조할 일은 많지만, 상품이 주문상품에 대해서 참조할 일은 거의 없어서 단반향으로 설정할 수 있다 - 양반향     - ex. 부서 -> 직원, 직원 -> 부서\n    - 직원이 어느 부서에서 근무하는 지를 알기 위해서 알고 싶고 또한 한 부서에 어떤 직원이 있는지 도 알고 싶은 경우에는 양반향으로 설정할 수 있다 어느 것",
    "category": "database",
    "tags": [
      "jpa",
      "database",
      "spring",
      "ManyToOne",
      "OneToMany",
      "단방향",
      "스프링",
      "양방향",
      "다대일",
      "일대다"
    ],
    "date": "2019-12-06T00:00:00.000Z"
  },
  {
    "id": "database/jpa-연관관계-매핑-정리",
    "slug": "database/jpa-연관관계-매핑-정리",
    "title": "JPA 연관관계 매핑 정리",
    "excerpt": "",
    "content": "1. 들어가며 엔티티는 다른 엔티티의 참조(변수)를 가지면서 관계를 서로 맺게 됩니다. 블로그에서 해당 포스트에 댓글을 다는 경우를 예를 들면, 댓글(Comment) 엔티티는 포스트 (Post) 엔티티 필드를 가지면서 서로 연관관계를 맺어 해당 댓글을 단 포스트 정보를 조회할 수 있습니다. 테이블에서는 이런 관계를 외래 키를 사용해서 관계를 맺습니다. JPA에서 테이블 간의 관계를 엔티티의 연관관계로 매핑하는 작업이 가장 먼저 하게 되고 핵심이 되는 작업입니다. 이번 포스팅에서는 JPA에서 엔티티 매핑 작업 시 자주 접하는 핵심 용어에 대해서 알아보겠습니다. 이후 시리즈로 포스팅될 블로그에서는 구체적으로 예제 코드를 통해서 각각의 연관관계에 대해서 알아보겠습니다. 2. 방향성 (Directional) > 시나리오\n>\n> - 포스트 (Post) -> 댓글 (Comment)\n> - 댓글 (Comment) -> 포스트 (Post) 테이블은 외래 키 하나로 테이블을 조인해서 양쪽으로 쿼리가 가능합니다. 하지만, 객체의 경우에는 객체에 참조하는 필드가 존재하면 그 필드를 통해서 연관된 객체를 한쪽으로만 조회가 가능하여 단방향이 되고 서로 각각의 객체를 필드로 가지고 있으면 양방향이 됩니다. - 단방향 (Unidirectional)\n    - 객체 관계에서 한 쪽만 참조하는 경우\n    - 포스트 -> 댓글\n- 양방향 (Bidirectional)\n    - 객체 관계에서 양 쪽 다 참조하는 경우\n    - 포스트 -> 댓글, 댓글 -> 포스트 3. 연관관계 (Associations/Relationships) 위 예제에서 포스트와 댓글의 관계를 보면, 하나의 포스트(일)에 여러 댓글(다)을 달 수 있고 여러 댓글(다)은 하나의 포스트(일)에 포함되는 관계로 다대일, 일대다 관계를 맺을 수 있습니다. 이 외에도 엔티티 간의 관계에서는 아래와 같이 다양한 관계가 존재합니다.  다대일 (N:1)\n     일대다와 같이 가장 많이 사용되는 연관관계이다\n 일대다 (1:N)\n 일대일 (1:1)\n 다대다 (N:N)\n     실무에서는 거의 사용하지 않는다 4. 연관관계의 주인 (Owner) 테이블은 외래 키가 한쪽에 하나만 존재하여 외래 키 하나로 연관관계를 맺습니다. 하지만, 양방향으로 맺어진 엔티티의 경우에는 양쪽에 서로 참조하는 필드가 존재하게 됩니다. 두 엔티티 중에 하나만 외래 키를 관리하는 곳을 연관관계의 주인이라고 합니다. 추후 업로드할 시리즈 포스팅에서 더 자세히 다루겠지만, 연관관계의 주인의 특징은 다음과 같습니다. - mappedBy 속성을 사용하는 엔티티는 연관관계의 주인이 아니다\n    - mappedBy 속성으로 연관관계의 주인이 필드 이름을 지정한다\n- 보통 외래 키를 가진 테이블과 매핑한 언티티(ex. Comment)가 외래 키ㅐ를 관리하는 주인으로 선택한다\n    - 다대일 양방향에서는 다(N)이 연관관계의 주인이 된다 5. 참고 - JPA 관계\n    - [https://minwan1.github.io/2018/12/21/2018-12-26-jpa-%EA%B4%80%EA%B3%84%EC%84%A4%EC%A0%95/](https://minwan1.github.io/2018/12/21/2018-12-26-jpa-%EA%B4%80%EA%B3%84%EC%84%A4%EC%A0%95/)\n    - https://siyoon210.tistory.com/27\n    - https://howtodoinjava.com/hibernate/how-to-define-association-mappings-between-hibernate-entities/\n- 책 : 자바 ORM 표준 JPA 프로그래밍\n    - <a href=\"http://www.yes24.com/Product/Goods/19040233?scode=032&OzSrank=1\"><img src=\"images/JPA-연관관계-매핑-정리/JPAbook.jpeg\" align=\"left\" alt=\"자바 ORM 표준 JPA 프로그래밍\" style=\"zoom:33%;\" /></a>",
    "category": "database",
    "tags": [
      "jpa",
      "database",
      "spring",
      "OneToMany",
      "OneToOne",
      "연관관계",
      "단방향",
      "양방향",
      "다대다",
      "다대일",
      "일대다",
      "일대일"
    ],
    "date": "2019-12-04T00:00:00.000Z"
  },
  {
    "id": "database/jpa-일대일-one-to-one-연관관계",
    "slug": "database/jpa-일대일-one-to-one-연관관계",
    "title": "JPA 일대일(1:1) @One-To-One 연관관계",
    "excerpt": "",
    "content": "1. 들어가며 이번 포스팅에서는 일대일 (1:1) 매핑에 대해서 알아보겠습니다. 2. 개발 환경 포스팅에서 작성한 코드는 깃허브에 올라가 있어요.  OS : Mac OS\n IDE: Intellij\n Java : JDK 1.8\n Source code :\n     주 테이블에 외래 키\n         단방향\n         양반향\n     대상 테이블에 외래 키\n         양반향\n Software management tool : Maven 3. 일대일 (1:1) 연관관계 일대일 관계에서는 반대도 일대일 관계가 됩니다. 다대일 관계에서는 다(N)쪽이 항상 외래 키를 가지고 있지만, 일대일 관계에서는 주 테이블이나 대상 테이블에 외래 키를 둘 수 있어서 개발 시 어느 쪽에 둘지를 선택해야 합니다. 3.1 주 테이블에 외래 키가 있는 경우 주 테이블에 외래 키가 있으면 주 객체에도 객체 참조를 두는 구조로 매핑을 하게 됩니다. - 주 테이블 : \n    - 외래 키(phoneid)가 있는 경우\n- 대상 테이블 :  <img src=\"image1.png\" style=\"zoom:50%;\" /> 3.1.1 일대일 단방향 일대일 단방향으로 설정해보겠습니다. 주 객체인  엔티티에 @OneToOne 선언 이후 대상 테이블인  객체를 선언합니다.  객체를 통해서 사용자의 핸드폰 정보를 조회할 수 있는 구조입니다. <img src=\"image2.png\" style=\"zoom:50%;\" /> 와  객체를 저장하고 조회해보겠습니다. 3.1.2 일대일 양반향 이제 양반향으로 설정해볼까요?  객체에도  객체를 가지도록 합니다. <img src=\"image4.png\" style=\"zoom:50%;\" />  엔티티에 추가로 @OneToOne 어노테이션을 선언합니다. 그리고 양방향이므로  속성으로 연관 관계의 주인을 지정해줍니다.  테이블에 외래 키를 가지고 있음으로 의 을 연관관계 주인으로 설정합니다. Unit Test로 엔티티 저장후 조회해보겠습니다. > 주의사항\n>\n> 일대일 관계에서 지연 로딩으로 설정을 해도 즉시 로딩이 되는 경우가 있습니다. 예를 들면,\n>\n> -  : 지연 로딩이 된다\n> -  : 지연 로딩이 안된다\n    >   - 프록시의 한계로 인해서 외래 키를 직접 관리하지 않는 일대일 관계에서는 지연 로딩으로 설정을 해도 즉시 로딩이 된다\n>\n> 참고로 @OneToOne 어노테이션의 기본 fetch 타입은 즉시 로딩(EAGER)입니다.\n>\n> \n>\n> - (1) 을 호출 할때 SQL 구문이 실행되어 지연로딩이 잘 되는 것을 확인할 수 있다\n> - (2) 는 지연로딩으로 설정되어 있지만,  호출시 즉시 로딩되는 것을 확인할 수 있다 3.2 대상 테이블에 외래 키가 있는 경우 외래 키가 주 테이블이 아니라 대상 테이블에 존재하는 경우에는 어떻게 달라지는 알아보겠습니다. - 주 테이블 : \n- 대상 테이블 : \n    - 외래 키(userid)가 있는 경우 <img src=\"image5.png\" style=\"zoom:50%;\" /> 3.2.1 일대일 단방향 외래 키는  테이블에 있고 아래와 같은 일대일 연관관계는 JPA에서 지원하지 않아 매핑할 수 없습니다. <img src=\"image2.png\" style=\"zoom:50%;\" /> 3.2.2 일대일 양반향 <img src=\"image4.png\" style=\"zoom:50%;\" /> 대상 테이블인 에 외래 키를 두고 싶으면 아래와 같이 설정하면 됩니다.  엔티티에 @OneToOne 어노테이션으로 설정하고  엔티티에서는 @OneToOne 어노테이션과  속성으로 외래 키를 소유하고 있는 의 를 연관관계 주인으로 지정합니다. 4. 참고  일대일\n     https://kwonnam.pe.kr/wiki/java/jpa/one-to-one\n     https://riptutorial.com/ko/jpa/example/22229/%EC%A7%81%EC%9B%90%EA%B3%BC-%EC%B1%85%EC%83%81-%EA%B0%84%EC%9D%98-%EC%9D%BC%EB%8C%80%EC%9D%BC-%EA%B4%80%EA%B3%84\n     https://www.popit.kr/spring-boot-jpa-step-08-onetoone-%EA%B4%80%EA%B3%84-%EC%84%A4%EC%A0%95-%ED%8C%81/\n 책 : 자바 ORM 표준 JPA 프로그래밍\n     <a href=\"http://www.yes24.com/Product/Goods/19040233?\tscode=032&OzSrank=2\"></a>",
    "category": "database",
    "tags": [
      "database",
      "jpa",
      "spring",
      "db",
      "OneToOne",
      "데이터베이스",
      "스프링",
      "스프링부트",
      "연관관계",
      "단방향",
      "양방향",
      "일대일"
    ],
    "date": "2019-12-27T00:00:00.000Z"
  },
  {
    "id": "database/liquibase-사용해서-db-스키마-관리하기",
    "slug": "database/liquibase-사용해서-db-스키마-관리하기",
    "title": "Liquibase 사용해서 DB 스키마 관리하기",
    "excerpt": "",
    "content": "1. 개요 > 같이 일하는 동료분께서 현재 개발중인 프로젝트에 적용해주셔서 스터디 차원에서 정리해본다 는 데이터베이스 변경을 추적하고 관리할 수 있도록 도와주는 오픈 소스 도구이다. 이 도구는 데이터베이스 스키마 변경을 기록하고, 이를 애플리케이션 배포와 연계하여 일관성을 유지하는 데 사용된다. 는 여러 팀이 협력해 일관된 방식으로 데이터베이스를 관리하는 데 유리하며, 코드 버전 관리를 하듯 데이터베이스의 상태를 관리할 수 있는 강력한 기능을 제공한다. 특히 는 데이터베이스 마이그레이션 툴로서, 다양한 방식으로 데이터베이스 변경 사항을 기록하고 롤백할 수 있게 해준다. 의 기본 동작 원리는 변경 사항을 정의한 파일()과 이를 실행할 환경(DB) 간의 동기화를 유지하는 것이다. 1.1 주요기능 - 자동 마이그레이션\n  - 는 변경 사항을 자동으로 적용하며, 데이터베이스 상태를 자동으로 업데이트한다\n- 롤백 기능\n  - 잘못된 변경 사항을 되돌리기 위한 롤백 기능을 제공한다 - 유료 버전에서만 가능\n  - 무료 버전은 사용자가 직접 롤백 스크립트를 작성해줘야 한다. 우리에게 챗GPT가 있어서 괜찮을 듯하다\n- 다양한 형식 지원\n  - , , ,  등 다양한 포맷으로 데이터베이스 변경 사항을 관리할 수 있다\n- 데이터베이스 무관성\n  - 는 다양한 데이터베이스 시스템(, , ,  등)을 지원하여 여러 데이터베이스에 동일한 변경 사항을 적용할 수 있다\n- 데이터베이스 변경 이력 추적\n  - 는 언제, 누가, 어떤 변경을 적용했는 지 추적할 수 있다 1.2 기본 개념 는 데이터베이스 변경 관리를 위한 핵심 개념으로 다음의 요소들을 사용한다. 1.2.1 용어 1. \n   - 여러 개의 을 모아 놓은 파일로 데이터베이스의 스키마 변경 내역을 관리하는 , ,  또는  파일이다\n   - 데이터베이스 변경 내역을 기록하고, 각 이 언제 적용되었는지 추적한다\n2. \n   - 데이터베이스에 적용할 특정 변경 사항(단위 작업)을 기술한 블록이다\n   - 데이터베이스에 적용할 하나의 변경 작업을 설명하고, 각 에는 고유의 ID, author, 그리고 해당 변경 사항이 들어있다\n   - ex. 테이블 추가, 컬럼 수정, 인덱스 추가 등\n3. \n   - 각  내에서 실행할 구체적인 데이터베이스 변경 작업의 유형이다\n   - 데이터베이스의 스키마나 데이터에 적용할 변경 사항의 유형을 정의한다\n   - ex. , , ,  등 1.2.2 기본 동작 원리 를 사용해서 SQL를 관리하는 기본 동작은 다음과 같다. 1. ,  파일을 작성하기\n   - 데이터베이스 스키마 변경 내용을 기록한  파일을 작성한다. 이 파일에는 데이터베이스에 반영할 이 포함된다\n2. Liquibase CLI로 DB에 적용하기\n   -  명령을 사용해 작성된 을 데이터베이스에 적용한다. 는 먼저 데이터베이스의  테이블을 확인하여 이미 적용된 Changeset을 파악하고, 새로운 Changeset만을 실행한다\n   - 이 성공적으로 실행되면, 는 그 기록을  테이블에 저장한다. 이를 통해 이미 적용된 을 다시 실행하지 않도록 방지한다\n3. Rollback의 경우\n   - 잘못된 변경 사항을 되돌릴 때는  명령을 사용하여 데이터베이스를 특정 시점의 상태로 복구할 수 있다 2. Liquibase CLI 설치 2.1 CLI 설치 > 설치는 맥 기준으로 설명한다 Homebrew를 사용하여 를 간편하게 설치한다. 2.2 실습을 위해 MySQL 도커 실행 실습을 위해 MySQL 도커를 실행하고 데이터베이스를 생성한다. DB 서버에 로그인을 해서 아래 데이터베이스를 생성한다. 3. Liquibase 기본 사용 방법 를 사용하려면 아래 단계로 진행하면 된다. - liquibase.properties 파일을 생성해서 DB 접속 정보를 입력한다\n-   파일을 생성한다\n- Liquibase CLI 명령어로 DB에 를 적용한다 3.1 프로젝트 생성하기 3.1.1 (옵션) init project 명령어 생성하기  명령어는 의 샘플 파일을 생성해 주는데, 유료 버전에서 사용할 수 있는 파일도 있고 그래서 굳이 이 명령어를 사용할 필요는 없다. 어떤 명령어인지 스터디 차원에서 추가한다. 3.1.2 수동으로 생성하기  프로젝트의 디렉토리 구조는 아래와 같이 구성할 수 있다.  프로젝트 구성에 대한 내용은 Design Your LIquibase Project를 참고해 주세요. 3.2 Liquibase 설정하기 를 사용하기 위해서는 데이터베이스와 연결할 수 있도록 설정 파일을 작성해야 한다.  파일을 생성하고 아래와 같이 설정한다. > mysql connector jar 파일은 [mysql.com](http://mysql.comhttps://downloads.mysql.com/archives/c-j/) 사이트에서 다운로드할 수 있다 3.3 SQL Changelog 생성하기 에서는 Changelog 파일을 작성하여 데이터베이스의 변경 사항을 기록한다. 의 형식은 각기 다른 데이터베이스에서 구애받지 않기 위해 , , , 로 구성할 수 있다. 여기서는 는 로 작성하고 은 개발자에게 익숙한 SQL 구문으로 작성한다. 이 은 formatted SQL 방식을 따르고 있으며, 아래와 같은 세부 사항들을 포함하고 있다. - \n  - 이 주석은 파일이 Liquibase의 formatted SQL 파일임을 명시한다. 모든 Liquibase formatted SQL 파일은 이 주석으로 시작해야 한다\n  \n- \n  - 는 Changeset 작성자의 이름이고 은 Changeset의 고유 ID이다. 프로젝트 내에서 동일한 작성자 이름과 ID의 조합이 유일해야 한다\n  - : 특정 레이블을 사용하여 Changeset을 필터링할 수 있다 ex. 이라는 레이블을 사용하면 나중에 특정 레이블이 붙은 Changeset만 실행할 수 있다\n  - : Changeset이 실행되는 특정 컨텍스트를 정의한다 여기서는 로 정의되었으며, 해당 컨텍스트에서만 Changeset이 실행된다\n  \n- \n  - Changeset에 대한 설명을 추가할 수 있는 주석이다. 이 예시에서는 단순한 설명인 가 포함되어 있다\n  \n- \n  - 이 주석은 Changeset을 롤백할 때 실행할 SQL 구문을 정의한다 참고 - Example Changelogs: SQL Format 3.4 Changelog를 DB에 적용하기 작성한 를 DB에 적용하려면,  명령어를 실행하면 된다. 이때 도커를 사용하거나,  명령어를 직접 실행할 수 있다. 3.4.1 Liquibase 명령어로 실행하기  명령어를 실행하면 설정 파일에서 지정된  파일을 참조하여 아직 적용되지 않은 변경 사항인()을 데이터베이스에 적용한다. 처음 실행하게 되면 Liquibase 관련 테이블도 같이 생성된 것을 확인할 수 있다. 3.4.2 Liquibase 명령어 도커로 실행하기 도커로 실행하면 별도의 설치 과정 없이 손쉽게 를 실행할 수 있지만, 몇 가지 주의 사항이 필요하다. - 도커 환경에서 를 실행하려면 MySQL과  컨테이너가 서로 통신할 수 있도록 설정해야한다 위 과정을 통해 MySQL 컨테이너()와  컨테이너가 같은 네트워크에서 통신할 수 있게 된다. - liquibase.properties 파일에서 MySQL 주소를 설정이 필요하다 도커 컨테이너 내에서 는 각 컨테이너 자신을 의미한다. 따라서 MySQL이 실행 중인 컨테이너에 접근하려면 MySQL 컨테이너의 이름을 주소로 지정해 줘야 한다. ex.  아래 명령어로 Liquibase를 도커로 실행하면 된다. - : 컨테이너 실행 후 자동으로 삭제한다\n- : 앞에서 생성한  네트워크를 사용하여 MySQL과 Liquibase가 통신할 수 있도록 설정한다\n- : 현재 디렉토리()를 도커 컨테이너 내의  경로에 마운트한다\n- : MySQL JDBC 드라이버를 컨테이너 내에 설치한다\n- : Liquibase 설정 파일()을 지정한다\n- : 변경사항을 데이터베이스에 적용하는 Liquibase 명령어이다 4. Liquibase 명령어 는 아래처럼 다양한 명령어를 제공한다. 자주 사용할 것 같은 명령어 위주로 정리한다. 더 자세한 내용은 Liquibase Commands를 참고해 주세요. 4.1 Update 명령어 의  명령어는 변경 사항을 데이터베이스에 적용하는 기본 명령이다. 하지만 다양한 상황에 맞춰  명령어에는 몇 가지 유용한 변형이 있다. -  : 실제로 데이터베이스에 변경을 적용하지 않고, 변경 사항을 SQL 파일로 출력한다. 이 명령어를 통해 변경 사항을 미리 검토하거나 수동으로 적용할 수 있다 - : 특정 개수의 을 SQL 파일로 출력하여 데이터베이스에 직접 적용하지 않고 검토할 수 있다 -  :  파일에서 지정된 개수만큼의 변경사항()을 데이터베이스에 적용한다. 이 명령어는 부분적으로 변경을 적용할 때 유용하다 4.2 Rollback 명령어 의  명령어는 데이터베이스의 변경 사항을 이전 상태로 되돌릴 수 있는 기능을 제공한다. 특정 조건에 맞춰 다양한 방식으로 롤백을 수행할 수 있다. -  : 특정 조건(태그, 날짜 등)까지의 모든 을 롤백한다. 이를 통해 데이터베이스를 지정된 이전 상태로 복원할 수 있다 -  : 실제 롤백을 수행하지 않고, 롤백에 필요한 SQL 스크립트를 출력해줘서 미리 변경 사항을 검토할 수 있어 자주 사용하게 된다 -  : 지정한 개수의 을 롤백한다 4.3 Database Inspection 명령어 데이터베이스의 현재 상태를 확인하거나 비교할 때 사용하는 명령어이다. -  : 실제 서버의 DB 상태와 적용하려는 를 비교할 때 사용하면 좋다. 실제로 서버에 새로운 를 반영하기 전에 서버와 차이가 있는지 미리 확인하는 게 좋다 -  : 두 데이터베이스 간의 차이점에 대한  파일을 생성한다 가 생성한  파일을 확인하면 아래와 같이 잘 생성해준다. 4.4 Change Tracking 명령어 의 Change Tracking 명령어는 데이터베이스에 적용된 변경 사항을 추적하고, 현재 상태를 확인하거나 변경 사항 기록을 생성하는 데 사용된다. -  : 이 명령어로 어떤 변경이 언제 적용되었는지 추적할 수 있다 -  : 데이터베이스에 적용되지 않은 이 있는지 확인할 수 있다 -  : 현재 데이터베이스의 스키마 상",
    "category": "database",
    "tags": [
      "Liquibase",
      "Flyaway",
      "rollback",
      "migration",
      "롤백",
      "마이그레이션",
      "SQL",
      "MySQL",
      "changelog",
      "changeset",
      "tag",
      "docker",
      "도커",
      "context",
      "label"
    ],
    "date": "2024-09-20T00:00:00.000Z"
  },
  {
    "id": "database/m1-맥북에서-memongo-실행하기",
    "slug": "database/m1-맥북에서-memongo-실행하기",
    "title": "M1 맥북에서 Memongo 실행하기",
    "excerpt": "",
    "content": "M1 맥북 +  +  조합으로 개발하고 있다면 아래와 같은 오류 메시지를 보게 되고 어떻게 해결하면 되는지 검색하게 된다. 팀에 새로운 분들이 올 때마다 설정하는 방법을 까먹게 되어 다시 정리해둔다. 를 M1에서 실행하면 아래와 같이 를 다운로드하는 과정에서 시스템 아키텍처가 맞지 않다고 오류 메시지를 던지고 실행이 안 되는 것을 볼 수 있다. 해결책은 2가지가 있고 개인적으로 매번 옵션을 기억해서 하기보다는 그냥 기본 설정은 두고 다운로드한 MongoDB binary를 사용하도록 하는 게 조금 더 편한듯해서 2번 방식으로 실행하는 것을 추천한다. 1.Memongo 옵션 설정  아키텍처일때 다운로드해야 하는 URL을 넣는 방법으로 M1에서 실행시킬 수 있다. 2.Custom MongoDB binary 사용 MongoDB binary를 다운로드하고 환경변수, ,  값을 설정해주면 다운로드하지 않고 이미 설치된 binary를 사용하도록 할 수 있다. 혹시 실행 시 문제가 발생하는 경우에는 아주 가끔 로 를 kill 해줘야 하는 경우도 있다. brew로 mongodb를 설치한다. MongoDB 가 잘 설치되어 있는지 확인한다. 사용 중인 shell의 설정값에 필요한 환경 변수를 추가한다. 참고로 GoLand에서 실행하는 거면 shell 설정 수정 이후 애플리케이션을 다시 시작해줘야 한다. 다시 memongotest.go를 실행해보면 잘 실행되는 걸 확인할 수 있다. 참고 - https://www.mongodb.com/docs/v4.2/tutorial/install-mongodb-on-os-x/\n- https://github.com/nodkz/mongodb-memory-server/issues/422\n- https://github.com/benweissmann/memongo\n- https://github.com/tryvium-travels/memongo",
    "category": "database",
    "tags": [
      "m1",
      "mac",
      "mongo",
      "mongodb",
      "go",
      "golang",
      "몽고",
      "맥북",
      "memongo"
    ],
    "date": "2023-02-25T00:00:00.000Z"
  },
  {
    "id": "database/mongo-script-collection-모음",
    "slug": "database/mongo-script-collection-모음",
    "title": "Mongo Script Collection 모음",
    "excerpt": "",
    "content": "개발 시 mongoDB를 주 데이터베이스로 사용하면 데이터 마이그레이션을 자주 하게 되는데, MYSQL보다는 덜 익숙한 면도 있어서 종종 하게 되는 migration을 매번 구글링하게 되어 정리 차원에서 블로그에 적어둔다. 실습 전에 간단하게 inventory collection에 데이터를 입력한다. 1.item의 key 이름을 변경 inventory list의 item의 key 이름은  operator를 사용한다. 아래 예제에서는  -> 로 이름을 변경한다. 2. 매칭이 item의 특정 값을 업데이트 SQL에서 where와 같이  operator로 특정 값이 매칭되는 item을 선택해서 값을 t operator로 변경한다. 참고 - https://blog.kevinchisholm.com/javascript/mongodb/getting-started-with-mongo-shell-scripting-basic-crud-operations/ - https://www.mongodb.com/docs/manual/reference/method/db.collection.updateOne/#mongodb-method-db.collection.updateOne",
    "category": "database",
    "tags": [
      "mongo",
      "script",
      "mongodb",
      "몽고",
      "스크립트"
    ],
    "date": "2023-02-25T00:00:00.000Z"
  },
  {
    "id": "database/mongodb-collection-cloning하는-방법",
    "slug": "database/mongodb-collection-cloning하는-방법",
    "title": "Mongodb Collection Cloning하는 방법",
    "excerpt": "",
    "content": "종종 기존의 데이터를 수정하지 않고 테스트를 위해서 기존 collection을 clone을 해서 테스트해보고 싶을 때가 있다. MongoDB 스크립트로 쉽게 clone 하는 방법에 대해서 알아보자 clone 테스트를 위해서 inventory를 생성해서 샘플 데이터를 생성한다. 1.db.collection.find().forEach()  는 collection의 데이터를 하나씩 로 돌면서 다른 inventory2로 삽입하는 방식이다. 하나씩 처리하기 때문에 느리다는 단점이 있다. 2.db.collection.aggregate() 의 를 사용하면 조금 더 빠르게 clone이 가능하다. Syntax 참고 - https://www.mongodbmanager.com/clone-mongodb-collection\n- https://www.mongodb.com/docs/manual/reference/operator/aggregation/out/\n- https://github.com/kenshin579/tutorials-go/blob/master/go-mongo/script/clonecollection.js",
    "category": "database",
    "tags": [
      "mongo, mongodb, clone, collection, script, 몽고"
    ],
    "date": "2022-07-16T00:00:00.000Z"
  },
  {
    "id": "database/mongodb-원격-서버에-있는-collection을-로컬환경-서버로-복사하기",
    "slug": "database/mongodb-원격-서버에-있는-collection을-로컬환경-서버로-복사하기",
    "title": "Mongodb 원격 서버에 있는 Collection을 로컬환경 서버로 복사하기",
    "excerpt": "",
    "content": "개발 시 원격에 있는 데이터를 로컬환경에 그대로 복사해서 테스트할 필요가 종종 생긴다. 그전 포스팅에서는 같은 서버에서 collection을 cloning 하는 방법에 대해서 알아보았다면, 이번에는 원격에서 로컬환경으로 cloning 하는 방법에 대해서 알아보자. 1.mongodump  명령어는  shell 명령어가 아니라 MongoDB 설치 시 같이 설치되는 command line 명령어이다. 는 mongodb의 data를 export 해주는 도구이다. -  : 호스트 명\n- t : 포터\n-  : 사용자\n-  : 암호\n-  : database 이름 실행 후 백업되는 데이터는  폴더 안에 백업 파일이 생성된다. 2.mongoimport dump 뜬 데이터를 로 로컬환경에 생성하려면 를 사용하면 된다. -  : mongodb 주소를 입력한다\n-  : database 이름\n-  : collection 이름 정리 이미 파악한 분도 계시겠지만, mongodump, mongoimport 명령어를 사용하면 아래와 같이 여러 시스템으로 cloning이 가능해진다. - 원격 서버 -> 로컬환경\n- 원격 서버 -> 다른 원격 서버\n- 로컬환경 -> 로컬환경 참고 - https://www.mongodb.com/docs/database-tools/mongodump/",
    "category": "database",
    "tags": [
      "mongo",
      "clone",
      "backup",
      "dump",
      "import"
    ],
    "date": "2022-07-24T00:00:00.000Z"
  },
  {
    "id": "database/qa-jpa-관련-질문-모음",
    "slug": "database/qa-jpa-관련-질문-모음",
    "title": "Q&A JPA 관련 질문 모음",
    "excerpt": "",
    "content": "개인적으로 모르는 부분 적어두고 알게 되는 부분에 대해서 간단하게 정리해둔 자료입니다.\n미 답변중에 알고 계신 부분 있으면 코멘트 달아주세요. 감사합니다. Q&A 전체 목록 <span style=\"color:orange\">[답변완료]</span> <span style=\"color:brown\">1. @EntityListeners 이란?</span> 엔티티를 DB에 적용하기 전후로 Custom 콜백을 요청할 수 있는 어노테이션입니다. 참고\n http://clearpal7.blogspot.com/2017/03/entitylisteners.html <span style=\"color:brown\">2. @PostLoad</span> @PostLoad 어노테이션은 엔티티를 로딩한 후에 호출 할 메서드를 설정하는 어노테이션입니다. 참고\n https://docs.jboss.org/hibernate/orm/4.0/hem/en-US/html/listeners.html\n https://docs.jboss.org/hibernate/orm/4.0/hem/en-US/html/listeners.html <span style=\"color:brown\">3. 자동으로 schema 생성하려면 설정을 어떻게 바꿔야 하나?</span> persistence.xml 파일에서 hibernate 설정에 hiberate.hbm2ddl.auto 속성을 아래와 같이 추가하면 됩니다.\n value\n     create : 매번 실행할 때마다 table을 삭제하고 다시 생성한다\n     update : 테이블 없는 경우에는 테이블을 생성한다 <span style=\"color:brown\">4. JPA에서 객체를 수정하면 기본으로 모든 필드 값을 포함해서 UPDATE SQL 문구가 생성되는데, 수정한 속성만 업데이트하려면 어떻게 설정을 해야 하나?</span> 저장할 필드가 너무 많은 경우에는 수정된 데이터만 포함해서 UPDATE SQL 문구를 생성하려면 @DynamicUpdate 어노테이션을 클래스에 선언하면 됩니다. 추가로 @DynamicInsert는 필드 값이 존재하는 필드(null이 아닌)만 포함해서 INSERT SQL 구문을 생성 할 때 사용됩니다. <span style=\"color:brown\">5. @Transactional</span> 클래스나 메서드에 어노테이션을 선언하면 외부에서 클래스의 메서드를 호출 할때 트랜잭션을 시작하고 메서드 실행이 끝나면 트랜잭션을 커밋해주는 어노테이션입니다. @Transactional은 Unchecked Exception(ex. RuntimeException 하위 예외)인 겨우에만 rollback을 하고 Checked Exception 예외에도 롤백을 적용하려면 @Transactional(rollbackFor = Exception.class) 처럼 rollback을 직접 지정해줘야 합니다. @Transactional 어노테이션은 보통 비지니스 로직이 있는 서비스 계층에서 사용합니다. 이 어노테이션을 유닛테스트 작성시 사용하면 각각의 테스트를 실행 할 때마다 트랜잭션을 시작하고 테스트가 끝나면 트랜잭션을 강제로 롤백합니다. 참고\n Checked Exception vs Unchecked(Runtime) Exception\n     http://www.nextree.co.kr/p3239/ <span style=\"color:brown\">6. @Convert란</span> JPA에서 convertor를 사용해서 엔티티의 데이터를 변환해서 DB에 저장하고 저장한 데이터를 조회할 때도 convertor를 통해서 변환해서 값을 가져올 수 있습니다. 아래 예를 보면 MediaInfoLive 엔티티의 movieRatioTp 필드에 @Convert 어노테이션을 적용해서 DB에 저장되기 직전에 LiveMovieRatioConvertor 클래스가 동작하도록 선언하였습니다. 필드뿐만이 아니라 클래스나 글로벌하게도 적용 가능합니다.  convertToDatabaseColumn() : 조금 더 작성이 필요함\n convertToEntityAttributes() : 조금 더 작성이 필요함 <span style=\"color:brown\">7. findOne이 호출이 안될 때가 있는 것 같은데 왜 그런가?</span> 스프링부트 1.5.x에서 2.0.x로 넘어가면서 JPA의  호출은 나  메서드를 사용해서 호출해야 합니다. 참고 - https://hspmuse.tistory.com/entry/springboot-15x-%EC%97%90%EC%84%9C-springboot-20x-%EB%84%98%EC%96%B4%EA%B0%80%EB%A9%B4%EC%84%9C-%EC%83%9D%EA%B8%B4%EC%9D%BC\n- https://stackoverflow.com/questions/49316751/spring-data-jpa-findone-change-to-optional-how-to-use-this/49317013 - - - - <span style=\"color:orange\">[미 답변 질문]</span> -  @Modifying란\n- DML (삭제, 수정)인 경우에는 @Modifying 어노테이션을 추가해야 함. 하지않으면 Not Supported for DML operation 오류가 발생함  https://winmargo.tistory.com/208\n https://www.baeldung.com/spring-data-jpa-modifying-annotation -  @Param란? - 이름기반 파라미터를 바인디할 때 사용하는 어노테이션임? -  객체의 연관관계를 매핑하려면 객체에서 양쪽 방향을 모두 관리해야 함… 왜 그런가?\n- 객체의 양방향 연관관계는 양쪽 모든 관계를 맺어줘야 하는데 왜 그런가?\n- 연관관계 편의 메서드… -  @NotFound(action = NotFoundAction.IGNORE)은 언제 사용하나? 참고  http://javafreakers.com/notfoundactionnotfoundaction-ignore-annotation-example/\n https://lyb1495.tistory.com/91 - referencedColumnName는 언제 사용하나? 비식별자 연관관계 일때 사용한다. 아래 예제어서는 mediainfolive 테이블에서 liveseqno(PK) 대신 mediaseqno로 join하기 위해서 추가했다. 참고 - https://larva.tistory.com/entry/JPA%EC%97%90%EC%84%9C-ManyToOne-%EA%B4%80%EA%B3%84%EC%97%90%EC%84%9C-%EB%B9%84%EC%8B%9D%EB%B3%84%EC%BB%AC%EB%9F%BC%EA%B3%BC-%EA%B4%80%EA%B3%84%EC%8B%9C-%EB%A7%B5%ED%95%91",
    "category": "database",
    "tags": [
      "Q&A",
      "faq",
      "jpa",
      "mysql",
      "database",
      "db"
    ],
    "date": "2019-10-21T00:00:00.000Z"
  },
  {
    "id": "database/qa-mybatis-관련-질문-모음",
    "slug": "database/qa-mybatis-관련-질문-모음",
    "title": "Q&A Mybatis 관련 질문 모음",
    "excerpt": "",
    "content": "개인적으로 모르는 부분 적어두고 알게 되는 부분에 대해서 간단하게 정리해둔 자료입니다.\n미 답변중에 알고 계신 부분 있으면 코멘트 달아주세요. 감사합니다. Q&A 전체 목록 <span style=\"color:orange\">[답변완료]</span> <span style=\"color:brown\">1. \\long 타입은 뭔가?</span> - \\long : long 타입으로 매핑된다\n- long : Long 타입이 매핑된다 참고\n http://www.mybatis.org/mybatis-3/ko/configuration.html <span style=\"color:brown\">2. IN (…)안에 list을 넘겨서 처리하는 방법은?</span> IN에 들어갈 (…) 값을 <foreach> 태그로 값을 생성할 수 있습니다. 참고  http://pcdate.blogspot.com/2013/05/mybatis-foreach.html <span style=\"color:brown\">3. association columnPrefix 중첩으로 사용할 때 매핑아 인되는 이슈?</span> association을 중첩으로 columnPrefix로 매핑하는 경우에는 prefix가 중첩으로 append 되기 때문에 vrfilenm 형식으로 작성을 해야 합니다. Mybatis Mapper 파일 참고 - https://androphil.tistory.com/733?category=423961 ---- <span style=\"color:orange\">[미 답변 질문]</span> - mybatis에서 @Transactional 어노테이션을 사용해서 unit test을 사용할 수 있나?\n- 잘 안됨 참고\n http://barunmo.blogspot.com/2013/06/mybatis.html\n https://otamot.com/64\n https://wedul.site/133\n https://examples.javacodegeeks.com/enterprise-java/spring/write-transactional-unit-tests-spring/\n https://mycup.tistory.com/185 - mybatis에서 association 속성은 뭔가?\n- resultMap에 다른 객체가 있는 경우에 사용하고 assocation은 has one 타입의 관계를 다룬다.\n- collection인 경우에는 has many 타입의 관계를 다룰 떄 사용한다.\n   참고\n http://noveloper.github.io/blog/spring/2015/05/31/mybatis-assocation-collection.html - mybatis에서 namespace를 위한 alias에 대해 지원을 하나?\n- 하지 않음 참고\n https://github.com/mybatis/mybatis-3/issues/1160 - mybatis에서 cdata를 자주 보게 되는데, 사용하는 이유는? 참고  https://epthffh.tistory.com/entry/Mybatis-%EC%97%90%EC%84%9C-CDATA-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0",
    "category": "database",
    "tags": [
      "Q&A",
      "faq",
      "mybatis",
      "db",
      "mysql",
      "sql",
      "database",
      "legacy"
    ],
    "date": "2018-07-29T00:00:00.000Z"
  },
  {
    "id": "database/qa-mysql-관련-질문-모음",
    "slug": "database/qa-mysql-관련-질문-모음",
    "title": "Q&A MySql 관련 질문 모음",
    "excerpt": "",
    "content": "개인적으로 모르는 부분 적어두고 알게 되는 부분에 대해서 간단하게 정리해둔 자료입니다.\n미 답변중에 알고 계신 부분 있으면 코멘트 달아주세요. 감사합니다. Q&A 전체 목록 <span style=\"color:orange\">[답변완료]</span> <span style=\"color:brown\">1. 테이블 생성시 InnoDB를 왜 설정해야 하나?</span> MySQL의 Storage Engine에는 여러 가지가 존재합니다. 제일 많이 사용되는 버전은 MyISAM와 InnoDB입니다. 테이블 생성시 어떤 엔진을 사용할 지 설정할 수 있습니다. 참고\n MyISAM vs. InnoDB\n     https://ojava.tistory.com/25 <span style=\"color:brown\">2. 왜 autoincrement를 하면 1로 증가하지 않고 4씩 증가하나?</span> 기본 설정은 1씩 증가하지만 autoincrementincrement 값을 다르게 설정하면 지정한 값만큼 증가하게 됩니다. 참고\n https://dba.stackexchange.com/questions/60295/why-does-auto-increment-jumps-by-more-than-the-number-of-rows-inserted\n https://stackoverflow.com/questions/206751/mysql-autoincrement-column-jumps-by-10-why <span style=\"color:brown\">3. Sql 문구에서 가끔씩 '@변수 := …’ 를 발견했다. 무슨 의미일까? </span> 사용자 정의 변수를 저장할 때 사용합니다. 이 경우에는 SELECT로 구한 mediano 값을 mediaNo 변수에 저장합니다. 참고\n http://www.mysqlkorea.com/sub.html?mcode=manual&scode=01&mno=21582&cat1=9&cat2=292&cat3=0&lang=k\n https://crazyj.tistory.com/m/110?category=802841 <span style=\"color:brown\">4. COUNT() vs COUNT(1) vs COUNT(pk)의 차이점? </span>  COUNT()\n     행의 개수를 카운트한다\n     NULL도 포함해서 카운트한다\n COUNT(1)\n     행의 개수를 카운트하지만, 하나의 테이블에 대해서만 쿼리가 되고 JOIN한 Table 쿼리를 안된다\n     사용하지 말라는 의견이 있다\n COUNT(pk)\n     NULL아 아닌것만 카운트한다 참고\n https://stackoverflow.com/questions/2710621/count-vs-count1-vs-countpk-which-is-better <span style=\"color:brown\">5. IFNULL() 함수?</span> IFNULL(expression, altvalue) 형식으로 expressoin이 NULL이면 altvalue를 반환합니다. 참고\n https://www.w3schools.com/sql/funcmysqlifnull.asp <span style=\"color:brown\">6. 'order by 2,1’는 어떻게 정렬을 하라는 건가? <span> 두번째 컬럼으로 정렬하고 중복 값이 있는 경우에는 첫번째 컬럼으로 정렬하라는 의미입니다. 참고  http://www.itmembers.net/board/view.php?id=oracle&page=2&sn1=&divpage=1&sn=off&ss=on&sc=on&selectarrange=headnum&desc=asc&no=29 <span style=\"color:brown\">6. MySQL Error 1093 : You can’t specify target table ..for update in FROM clause가 발생하는 경우, 어떻게 처리하면 되나? </span> 아래 SQL 실행시 오류가 발생하였습니다. 원인은 MySQL은 Oracle과 달리게 UPDATE나 DELETE 할때 자기 테이블의 데이터를 바로 사용하지 못하는 이슈가 있어서 서브 쿼리를 하나 더 생성하여 임시 테이블을 만들어서 해결하면 됩니다. 해결 참고  https://www.lesstif.com/display/DBMS/MySQL+Error+1093+%3A+You+can%27t+specify+target+table+%27cwdgroup%27+for+update+in+FROM+clause <span style=\"color:brown\">8. MySql에서 모든 query에 대해서 로깅을 하려면 어떻게 해야 하나?</span> MySql 설정에서 generallog을 활성화시켜면 됩니다. 참고\n https://skibis.tistory.com/75 <span style=\"color:brown\">8. MySql에서 generallog가 활성화되어 있지 않는 경우 query를 확인하는 방법은 없나? </span> MySql 실행시 모든 명령문을 bin log로 저장한다면 확인할 수 있습니다. bin log 로그 분석에 대한 자세한 사항은 아래 링크를 참조해주세요. 참고\n http://www.enjoyteam.net/?p=128\n http://www.mysqlkorea.com/sub.html?mcode=manual&scode=011&mno=22368&cat1=752&cat2=799&cat3=927&lang=k\n http://blog.naver.com/PostView.nhn?blogId=ncloud24&logNo=221055112009&parentCategoryNo=&categoryNo=79&viewDate=&isShowPopularPosts=false&from=postView <span style=\"color:brown\">9. Slow Query란?</span> Slow Query란 말 그래도 query 수행시 오래 걸리는 쿼리를 의미합니다. 참고  https://itstudyblog.tistory.com/384 <span style=\"color:brown\">10. 페이징에서 offset과 limit은 어떻게 사용되나?</span> 많은 데이터를 한번에 가져올 수 없기 때문에 페이징으로 부분적으로 데이터를 가져올 때 LIMIT과 OFFSET을 사용합니다.  LIMIT : 가져올 데이터의 개수\n OFFSET : 검색된 데이터중에서 몇번째부터 가져올 것인지 정하는 시작번호 참고\n https://needjarvis.tistory.com/259\n http://avilos.codes/database/mysql/mysql-pagination/ <span style=\"color:brown\">11. Server time zone value ‘KST’ is unrecognized… 오류 메시지가 나는 경우 해결책은?</span> 여러 사항에 따라서 해결 방법이 다를 것으로 판단됩니다. 저희 경우에는 pom.xml에서 mysql-connector-java의 버전(ex. 8.0.13 —> 5.1.47)을 변경해서 해결했습니다. 참고\n https://offbyone.tistory.com/318 <span style=\"color:brown\">12. JOIN시 ON과 WHERE의 차이점은?</span> 내부 조인일 경우 ON 절은 WHERE 절을 사용 할 때와 결과가 같아서 외부 조인일 때만 ON을 사용하면 됩니다. 참고 - https://eddyplusit.tistory.com/52\n- https://blog.leocat.kr/notes/2017/07/28/sql-join-on-vs-where <span style=\"color:brown\">13. 변수에 지정한 값 출력을 어떻게 하나?</span> 을 사용하면 됩니다. 참고\n https://stackoverflow.com/questions/40905427/how-to-print-the-string-variable-in-mysql - - - - <span style=\"color:orange\">[미 답변 질문]</span> - 하나의 행의 값을 복사하되 몇 열 값만 변경하고 싶을 때? 참고\n https://stackoverflow.com/questions/2783150/mysql-how-to-copy-rows-but-change-a-few-fields -  데이터 migration은 어떻게 하나? 예. 한 테이블에 있는 데이터를 분리하는 작업은 어떻게 진행하면 되나? - 테이블 복사는 어떻게 하나? - groupconcat이란? 참고  https://fruitdev.tistory.com/16",
    "category": "database",
    "tags": [
      "Q&A",
      "faq",
      "mysql",
      "db",
      "slow query"
    ],
    "date": "2018-07-29T00:00:00.000Z"
  },
  {
    "id": "database/관계형-데이터베이스-설계-및-구축",
    "slug": "database/관계형-데이터베이스-설계-및-구축",
    "title": "관계형 데이터베이스 설계 및 구축",
    "excerpt": "",
    "content": "1. 소개 새로운 직장으로 이직하게 되어 학부 이후 거의 보지 않았던 데이터베이스 책을 다시 보게 되었습니다. 최근에 데이터베이스 스터디하면서 봤던 책은 김연희 교수님의 데이터베이스 개론 책 을 많이 참조하였습니다. 관계형 데이터베이스에 대해서 기본적인 개념을 전체 리뷰하는데 도움이 되었던 책입니다. 혹시 관계형 데이터베이스에 대한 리뷰가 필요하시다면 이 책을 구매하셔서 보시길 권합니다. 본 포스팅에서는 관계형 데이터베이스 설계 과정을 통해서 실제 테이블을 생성하는 MySql 스크립트까지 작성 해보도록 하겠습니다. 설계하면서 사용한 도구와 소스는 아래를 참조해주세요. - Tools\n    - draw.io : ERD 다이어그램\n    - mysql benchwork : EER Diagram 및 물리적 스키마 설계\n        - intellij Database Tool\n- Database : mysql 5.6.42\n- Source files : github 데이터베이스에서 초기 설계 과정이 중요한 부분을 차지합니다. 잘못 설계되면 현재 사용 중인 구조를 나중에 쉽게 변경하기 어려운 부분도 있고 데이터의 일관성과 무결성이 유지되지 않는 문제도 생길 수가 있어서 데이터베이스 설계 과정을 통해서 좋은 데이터베이스를 생성해야 합니다. 관계형 데이터베이스를 설계하는 방법에는 주로 2가지 방법 이 사용됩니다. - E-R 모델과 릴레이션 변환 규칙을 이용한 설계\n- 정규화를 이용한 설계 (이 주제는 다음 포스팅에서 다루겠습니다)\n    - 이상 현상(ex. 삽입, 삭제, 수정시 발생하는 문제)을 제거하면서 중복을 최소화하고 더 좋은 ‘작은' 릴레이션으로 분해하는 작업이다. E-R 모델과 릴레이션 변환 규칙을 이용한 설계는 아래 단계를 거쳐 데이터베이스를 생성합니다. 1. 데이터 요구사항에 대한 분석 (결과 : 요구사항 명세서)\n2. 개념 스키마 설계 (결과 : ERD)\n3. 논리 스키마 설계 (결과 : 릴레이션 스키마의 테이블 명세서)\n4. 내부 스키마 설계 (결과 : DB 스키만 생성 SQL 문) 2. 요구사항 분석하기 데이터베이스에 대한 사용자의 요구사항을 수집하고 분석해서 아래와 같이 요구사항 명세서를 작성해야 합니다. 아래 예제는 데이터베이스 개론 책에 기재된 예제문제입니다. - 한빛 항공사에 회원으로 가입하려면 회원아이디, 비밀번호, 성명, 신용카드 정보를 입력해야 한다\n- 회원의 신용카드 정보는 여러 개를 저장할 수 있는데, 세부적으로는 신용카드번호, 유효기간을 저장할 수 있다\n- 한빛 항공사에서는 보유한 비행기에 대해 비행기번호, 출발날짜, 출발시간 정보를 저장하고 있다\n- 한빛 항공사에서는 좌석에 대한 좌석번호, 등급 정보를 저장하고 있다\n- 회원은 좌석을 예약하는데, 회원 한 명은 좌석을 하나만 예약할 수 있고, 한 좌석은 회원 한명만 예약할 수 있다\n- 비행기에는 좌석이 존재하는데, 비행기 하나에는 좌석이 여러 개 존재할 수 있고 한 좌석은 반드시 하나의 비행기에만 존재해야 한다.\n- 그리고 좌석은 비행기가 없으면 의미가 없다. 3. 개념적 설계로 E-R 다이어그램 만들기 작성한 요구사항 명세서에서 데이터베이스를 구성하는데 필요한 개체, 속성, 개체 간의 관계를 추출하여 ERD를 생성합니다. - 개체와 속성을 추출한다\n    - 대부분 명사로 선별한다\n- 개체 간의 관계를 추출한다\n    - 대부분 동사로 선별한다 (개체간의 관계를 나타내는 동사이여야 한다)\n    - 관계에 속한 속성도 있을 수 있다\n    - 1:1, 1:N, N:M\n    - 필수적인 참여, 선택적인 참여 3.1 개체(Entity)와 속성(attribute) 추출 개념적 설계 단계에서 가장 먼저 해야 할 일은 개체를 추출하는 일입니다. 개체는 현실에서의 사물이나 사람이 생각하는 개념입니다. 개체를 나타내는 속성이 있고 여러 관련 속성이 모여 하나의 정보 단위를 이루는 것이 개체가 됩니다. 요구사항에서 개체는 대부분 명사로 이루어져 있지만, 속성과 구별할 필요가 있습니다. 항공사 명세서에서 개체와 속성을 구분해보겠습니다. 항공사 요구사항에서는 아래와 같이 개체와 속성을 구별할 수 있습니다. | 개체     | 속성                                 |\n| -------- | ------------------------------------ |\n| 회원     | 회원아이디, 비밀번호, 성명, 신용카드 |\n| 신용카드 | 신용카드번호, 유효기간               |\n| 비행기   | 비행기번호, 출발날짜, 출발시간       |\n| 좌석     | 좌석번호, 등급                       | 3.2 개체 간의 관계 추출 개체와 속성이 구별되었다면 개체 간의 관계를 추출합니다. 개체 간의 관계도 여러 가지로 분류해서 정의됩니다. - 일대일(1:1), 일대다(1:N), 다대다(N:M)\n- 관계 : 선택적인 관계, 필수적인 관계 요구사항에서 개체 간의 관계는 동사로 묘사되기 때문에 동사부터 찾으면 됩니다. 빨간색으로 표시된 부분이 앞 써 추출한 개체 간의 관계를 나타냅니다. 관계가 필수 인지 아니면 선택적인지 관계인지를 파악하는 게 개인적으로 쉽지 않아서 조금 더 정리를 해봤습니다. - 필수적으로 관계에 참여 (전체 참여 : Total Participation)\n    - [A : B] 관계에서 개체 집합 B의 모든 개체가 [A : B] 관계에 참여한다\n    - 개체 A에 대해 개체 조건을 만족하는 개체가 반드시 존재할 경우에 필수적인 관계 라고 본다.\n- 선택적으로 관계에 참여 (부분 참여 : Partial Participation)\n    - [A : B] 관계에서 개체 집합 B의 일부 개체만 [A : B] 관계에 참여한다\n    - 객체 A에 대해 개체 조건을 만족하는 개체가 존재할수도, 존재하지 않을 수도 있는 경우에 선태적인 관계 라고 본다.\n- 예제 모음\n    - 예1. [학과 : 교수]\n        - 학과(필수): 한 학과에는 여러 교수가 소속된다\n        - 교수(필수): 한 교수는 한 학과에만 소속된다\n    - 예2. [회원 : 주문]\n        - 회원(선택): 한 회원은 여러번 주문할 수 있다\n        - 주문(필수): 한 주문은 한 회원에 의해 주문된다\n    - 예3. [교수 : 과목]\n        - 교수(선택): 한 교수는 여러 과목을 강의할 수 있다\n        - 과목(필수): 한 과목은 한 교수에 의해서 강의되어야 한다\n    - 예4: [주문 : 주문목록]\n        - 주문(필수): 한 주문은 여러 개의 주문목록을 포함한다\n        - 주문목록(필수): 한 주문목록에 하나의 주문내용에 포함된다\n    - 예5 : [고객 : 책] - 구매 관계\n        - 고개(필수): 모든 고객이 책을 반드시 구해야 한다\n        - 책(선택) : 고객이 구매하지 않은 책이 존재할수 있다. 개채 간의 관계 추출 결과 | 관계 | 관계에 참여하는 개체                                         | 관계유형 | 관계 속성                                         |\n| ---- | ------------------------------------------------------------ | -------- | ------------------------------------------------- |\n| 보유 | 회원(선택) : 한 회원은 여러 신용카드를 가질 수 있다<br/>신용카드(필수) : 한 신용카드는 한 회원이 보유한다 | 1:N      |                                                   |\n| 예약 | 회원(선택) : 한 회원은 한 좌석만 예약할 수 있다<br/>좌석(필수) : 한 좌석은 회원 한명에 의해서 예약된다<br />회원(선택): 한 회원은 여러 비행기를 예약할 수 있다<br/>비행기(필수): 비행기 한 좌석은 한 회원이 예약한다 | 1:N      | 요구사항에서는 없음<br/>ex. 예약번호<br/>예약일자 |\n| 존재 | 비행기(필수) : 비행기 하나에는 좌석이 여러개 존재한다<br/>좌석(필수) : 한 좌석은 하나의 비행기에만 존재해야 한다 | 1:N      | 비행기 없이 좌석이 존재할 수 없음                 | 3.3 분석한 내용으로 ERD 생성하기 객체, 관계 정리만 잘 되면 쉽게 ERD를 만들어볼 수 있습니다. 추출한 내용을 가지고 릴레이션 스키마를 바로 생성할 수도 있지만, 다이어그램으로 다시 정리하면 큰 그림을 보면서 더 쉽게 전체 모델을 이해할 수 있는 장점이 있습니다. ERD 만들 수 있는 여러 도구가 있지만, 저는 draw.Io를 사용해서 ERD를 만들어봤습니다. draw.io은 오픈소스 프로그램으로 애플리에션을 다운로드 받아서사용하거나 아니면 웹 브라우저에서도 프로그램 설치 없이 사용할 수 있습니다. 맥에서 사용하시려면 brew 명령어로 설치하세요. draw.io은 IE(Information Engineering) 표기법을 사용합니다. IE 표기법은 링크 를 참조해주세요. 지금까지 언급한 개체와 관계 추출 정보 바탕으로 그린 ERD다이어그램입니다. #4.논리적 설계 단계를 거치면서 여러 번 수정하였고 아래는 최종 버전입니다. 수정한 이력은 6.ERD다이어그램 변경 이력에 기록해 두었습니다. 4. 논리적 설계로 릴레이션 스키마 및 테이블 명세서 만들기 ERD에서 릴레이션 스키마를 만들려면 다음 5가지 릴레이션 변환 규칙에 따라서 릴레이션 스키마로 변환해주면 됩니다. - 규칙1 : 모든 개체는 릴레이션으로 변환한다\n- 규칙2 : N:M 관계는 릴레이션으로 변환한다\n    - 관계의 이름을 릴레이션 이름으로 하고 관계의 속성도 릴레이션의 속성으로 변환한다\n- 규칙3 : 1:N 관계는 외래키로 표현한다\n    - 규칙3-1: 일반적인 1:N 관계는 외래키로 표현한다\n    - 규칙3-2: 약한 개체가 참여하는 1:N 관계는 외래키로 포함해서 기본키로 지정한다\n- 규칙4 : 1:1 관계는 외래키로 표현한다\n    - 규칙4-1: 일반적인 1:1 관계",
    "category": "database",
    "tags": [
      "sql",
      "ER",
      "EER",
      "ERD",
      "schema",
      "릴레이션",
      "디아어그램",
      "요구사항",
      "스키마",
      "설계"
    ],
    "date": "2019-02-06T00:00:00.000Z"
  },
  {
    "id": "database/관계형-데이터베이스에서-조인-join이란",
    "slug": "database/관계형-데이터베이스에서-조인-join이란",
    "title": "관계형 데이터베이스에서 조인(join)이란?",
    "excerpt": "",
    "content": "1.JOIN에 대한 기본 개념정리\n관계형 데이터베이스에서는 중복 데이터를 피하기 위해서 데이터를 쪼개 여러 테이블로 나눠서 저장합니다. 이렇게 분리되어 저장된 데이터에서 원하는 결과를 다시 도출하기 위해서는 여러 테이블을 조합할 필요가 있습니다. 관계형 데이터베이스에서는 조인(JOIN) 연산자를 사용해 관련 있는 컬럼 기준으로 행을 합쳐주는 연산입니다. 조인에 대해서 공부하다 보면 종류도 많아서 처음에는 많이 헷갈릴 때가 종종 있어서 다시 정리를 해보았습니다. 2. 샘플 데이터 이 포스팅에서 사용한 데이터는 MySql 사이트에서 제공한 샘플 데이터를 참고해서 수정한 버전을 사용했습니다.  MySql dummy 데이터\n     https://github.com/datacharmer/testdb\n 수정 버전 - 예제로 작성한 sql도 확인할 수 있다\n     https://github.com/kenshin579/books-database-intro/tree/master/02join 주어진 샘플 데이터로 직접 SQL을 쳐보면서 여러 조인 타입을 스터디하면 더 좋을 것 같습니다. 3. 조인의 종류 MySQL에서 지원하는 조인 연산입니다. 그외 조인은 설명 위주로 말씀을 드리겠습니다.  내부 조진 (INNER JOIN)\n     교차 조인 (CROSS JOIN - CARTESIN JOIN)\n     등가동등동일 조인(EQUI JOIN)\n     비등가 조인(NON-EQUI JOIN)\n     자연 조인 (NATURAL JOIN)\n 외부 조인 (OUTER JOIN)\n     완전 외부 조인 (FULL OUTER JOIN)\n     왼쪽 (LEFT OUTER)\n     오른쪽 (RIGHT OUTER)\n 셀프 조인 (SELF JOIN)\n 안티 조인 (ANTI JOIN)\n 세미 조인 (SEMI JOIN) SQL 조인 쉽게 이해하기 위한 다이어그램입니다. 3.1 내부 조인 내부 조인을 더 세부적으로 분류하면 아래와 같습니다.  교차 조인 (CROSS JOIN - CARTESIN JOIN)\n 동등/동일 조인(EQUI JOIN)\n 비등가 조인(NON-EQUI JOIN)\n 자연 조인 (NATURAL JOIN) 3.1.1 교차 조인 (CROSS JOIN - CARTESIN PRODUCT) 교차 조인은 두 테이블의 카티션 프로덕트(곱집합)를 한 결과입니다. 특별한 조건없이 테이블 A의 각 행과 테이블 B의 각 행을 다 조합한 결과입니다. 조인 SQL 구문은 두가지 표현법으로 만들 수 있습니다. 구체적인 SQL 구문이 있는 명시적 표현법과 암묵적인 표현 방식이 있습니다. 참고로 교차 조인은 암묵적인 표현법에서 WHERE 문구가 없습니다. 조인 결과 3.1.2 내부 조인 (INNER JOIN) 내부 조인은 가장 많이 사용되는 조인 구문중에 하나입니다. 내부 조인은 조인 조건문에 따라 2개의 테이블(A, B)의 컬럼을 합쳐 새로운 테이블을 생성합니다. 즉, 교차 조인을 한 결과에 조인 조건문을 충족시키는 레코드를 반환한다고 생각하시면 됩니다. 내부 조인을 벤 다이어그램으로 표현하면 아래와 같이 하이라이트된 부분이 조건문을 충족시키는 부분입니다. SQL은 명시적 표현법과 암묵적 표현법 2가지 구문으로 지정할 수 있습니다. 조인 결과 3.1.3 등가 조인 (EQUI JOIN) 등가 조인은 비교기반 조인의 특정 유형으로 동등비교(=)를 사용하는 조인입니다. 이미 위 3.1.2에서 설명한 조인을 등가 조인(=동일 조인)이라고 합니다. 3.1.2 비등가 조인 (NON-EQUI JOIN) 비등가 조인은 동등비교(=)를 사용하지 않는 조인으로 조건문이 크거나 작거나 같이 않은 비교등을 사용하면 비등가 조인이라고 합니다. 조인 결과 3.1.5 자연 조인 (NATURAL JOIN) 자연 조인은 동등 조인의 한 유형으로 두 테이블의 컬럼명이 같은 기준으로 조인 조건문이 암시적으로 일어나는 내부 조인입니다.  같은 이름을 가진 컬럼은 한 번만 추출된다\n     동등 조인에서는 empno가 두번 추출된 것을 확인할 수 있다 (#3.1.2: 그림1) 조인 결과 3.2 외부 조인 (OUTER JOIN) 내부 조인의 경우에는 공통 컬럼명 기반으로 결과 집합을 생성합니다. 반면에 외부 조인은 조건문에 만족하지 않는 행도 표시해주는 조인입니다. 그래서, 조인을 했을 때 한쪽의 테이블에 데이터가 없어도 조인 결과에 포함시키는 조인입니다. 외부 조인은 아래와 같이 3가지 종류가 있고 각각에 대해서 예제를 통해서 알아보도록 하겠습니다.  외부 조인 (OUTER JOIN)\n     왼쪽 (LEFT OUTER)\n     오른쪽 (RIGHT OUTER)\n     완전 외부 조인 (FULL OUTER JOIN)\n         MySql에서는 이걸 지원하지 않지만, SQL UNION 구문으로 사용하면 된다 3.2.1 왼쪽 외부 조인 (LEFT OUTER JOIN) 왼쪽 외부 조인은 테이블 A의 모든 데이터와 테이블 B와 매칭이 되는 레코드를 포함하는 조인입니다. 이해하기 쉽게 간단한 예제를 추가해봤습니다. 조인 결과 = table1의 모든 데이터 + table1과 table2 컬럼(n)과 매칭이 되는 데이터 조인 결과 내부 조인 결과 3.2.2 오른쪽 외부 조인 (RIGHT OUTER JOIN) 왼쪽 외부 조인은 테이블 B의 모든 데이터와 테이블 A와 매칭이되는 레코드를 포함하는 조인입니다. 조인 결과 = table2의 모든 데이터 + table1과 table2 컬럼(n)과 매칭이 되는 데이터 조인 결과 3.2.3 완전 외부 조인 (FULL OUTER JOIN) 완전 외부 조인은 MySQL에서는 명시적인 SQL 구문은 지원하지 않지만, UNION을 사용해서 완전 외부 조인을 할 수 있습니다. 조인 결과 - 왼쪽 외부 조인 + 오른쪽 외부 조인 테이블 employees와 departments의 완전 외부 조인문과 그 결과입니다. 조인 결과 3.3 셀프 조인 (SELF JOIN) 셀프 조인은 자기 자신과 조인하는 조인입니다. 예를 들면, 임직원중에 같은 부서에서 일하는 직원을 알고 싶으면 셀프 조인을 사용하면 좋습니다. 조인 결과 3.4 안티 조인 (ANTI JOIN) 안티 조인은 서브 쿼리내에서 존재하지 않는 데이터만 추출하여 메인 쿼리에서 추출하는 조인입니다. 간단한 예제로 부서 번호(deptno)가 2 이상이 아닌 데이터와 임직원 번호(empno)가 10002이상인 임직원을 추출하기로 보겠습니다. NOT EXISTS나 NOT IN을 사용해서 작성할 수 있습니다. 조인 결과 3.5 세미 조인 (SEMI JOIN) 세미 조인은 안티 조인과 반대로 서브 쿼리 내에서 존재하는 데이터만을 가지고 메인 쿼리에서 추출하는 방식입니다. 조인 결과 4. 참고  조인 종류\n     https://wikidocs.net/3956\n     https://coding-factory.tistory.com/87\n     https://blog.ngelmaum.org/entry/lab-note-sql-join-method\n     http://postitforhooney.tistory.com/entry/DBMARIADB-SQL-예제를-통한-JOIN의-종류-파악\n     http://futurists.tistory.com/17\n     https://ko.wikipedia.org/wiki/Join(SQL )\n     https://coloringpagewiki.com/img/2802678/mysql-whats-the-difference-between-inner-join-left-join-right-also-check-this-post-sql-server-better-performance-left-join-or-not-in.asp\n FULL JOIN\n     https://www.xaprb.com/blog/2006/05/26/how-to-write-full-outer-join-in-mysql/\n ANTI JOIN\n     https://thebook.io/006696/part01/ch06/02/03/\n SEMI JOIN\n     http://wiki.gurubee.net/pages/viewpage.action?pageId=1966761\n 조인 사용시 주의사항\n     http://gywn.net/2012/05/mysql-bad-sql-type/\n 곱집합      https://www.codewars.com/kata/cartesian-product",
    "category": "database",
    "tags": [
      "join",
      "inner",
      "cross",
      "outer",
      "left",
      "right",
      "조인",
      "내부조인",
      "교차조인",
      "비등가조인",
      "외부조인"
    ],
    "date": "2019-02-06T00:00:00.000Z"
  },
  {
    "id": "database/데이터베이스의-키-종류",
    "slug": "database/데이터베이스의-키-종류",
    "title": "데이터베이스의 키 종류",
    "excerpt": "",
    "content": "1. 데이터베이스의 키 종류 이번 포스팅에서는 데이터베이스의 여러 키 종류를 정리해보겠습니다. 키 종류에 대한 설명을 위해 아래 샘플 데이터를 사용하겠습니다. 샘플 데이터는 자동으로 생성해주는 dummy data 사이트에서 얻어왔습니다. 1.1 수퍼키 (super key) - 유일성의 특성을 만족하는 속성들의 집합으로 이루어진 키 를 수퍼키라 한다\n    - 유일성이란? - 하나의 키로 어떠한 행을 바로 찾아낼 수 있는 성질 을 의미한다\n- 예. authors 테이블\n    - id, (id, firstname), (firstname, lastname), email 등이 수퍼키가 된다 1.2 후보키 (candidate key) - 유일성과 최소성을 만족하는 속성 또는 속성들의 집합이다. 즉, 수퍼키중에서 최소성을 만족하는 것이 후보키가 된다\n    - 최소성이란? - 레코드를 식별하는데 꼭 필요한 속성 들로만 구성한다\n- 예. authors 테이블\n    - id와 email이 후보키가 된다 1.3 기본키 (primary key) - 후보키중에 특별히 선택된 키 이다\n- 키본키는 NULL 값이나 중복된 값 을 가질 수 없다\n- 예. authors 테이블 \\ 후보키중에 id를 기본키로 선정할 수 있다. (중복값이나 NULL 값이 없다) 1.4 대체키 (alternate key) - 대체키란 기본키로 선택되지 못한 후보키 를 의미하고 보조키 라고도 한다\n- 예. authors 테이블\n    - email가 대체키가 된다. 1.5 외래키 (foreign key) - 어떤 릴레이션에 있는 속성이 다른 릴레이션의 기본키가 되는 키 를 의미한다\n- 외래키 속성의 도메인과 참조되는 기본키 속성의 도메인은 같아야 한다\n- 외래키는 같은 릴레이션을 참조할 수도 있다\n- 외래키는 NULL 값을 가질 수 있다\n- 예. posts 테이블\n    - authorsid가 외래키가 된다 2. 참고 - 책\n    - \n- 키 종류\n    - https://m.blog.naver.com/dlwjddns5/220620195019\n    - http://limkydev.tistory.com/108 \\ https://m.blog.naver.com/PostView.nhn?blogId=slrkanjsepdi&logNo=90118418840&proxyReferer=https%3A%2F%2Fwww.google.co.kr%2F",
    "category": "database",
    "tags": [
      "database",
      "key",
      "super key",
      "primary key",
      "foreign key",
      "키",
      "후보키",
      "대체키",
      "수퍼키",
      "기본키",
      "외래키"
    ],
    "date": "2018-11-25T00:00:00.000Z"
  },
  {
    "id": "database/맥에서 kubernetes kind로 쉽고 빠르게 클러스터 구성하기",
    "slug": "database/맥에서 kubernetes kind로 쉽고 빠르게 클러스터 구성하기",
    "title": "맥에서 Kubernetes? Kind로 쉽고 빠르게 클러스터 구성하기",
    "excerpt": "",
    "content": "1. 개요 Kind란? (Kubernetes in Docker)는 Docker 컨테이너 내에서  클러스터를 실행할 수 있도록 도와주는 도구이다. 로컬 환경에서 빠르게  클러스터를 실행하고 테스트하는 데 유용하다. Kind의 아키텍처 구조 는  노드를  컨테이너로 실행하는 방식으로 동작한다. 다음은 의 기본 아키텍처 구조이다. - 각 노드는 Docker 컨테이너로 실행되며, 내부에서 , , ,  등의 핵심 Kubernetes 컴포넌트를 구동한다\n- CNI를 사용하여 네트워크를 구성하고, CoreDNS를 활용하여 DNS를 제공하낟\n- 이 방식은 로컬 개발 환경에서 가벼운 Kubernetes 클러스터를 실행하기에 적합하며, CI/CD 테스트 환경으로도 많이 활용된다 다른 Kubernetes 도구와의 차이점 | 항목              |                              |                                |  |          |\n| ----------------- | ---------------------------------- | ---------------------------------------- | --------------------------- | ------------------------- |\n| 실행 방식         | Docker 컨테이너 기반               | 가상화 기반 (Docker, VirtualBox 등 지원) | Docker 내장 K8s 기능 활용   | 여러 K8s 배포판 선택 가능 |\n| 성능              | 가볍고 빠름                        | 다양한 환경 지원, 다소 무거움            | Mac/Windows에서 최적화됨    | 다소 무거움               |\n| LoadBalancer 지원 | 기본적으로 미지원 (추가 설정 필요) | 기본적으로 미지원 (추가 설정 필요)       | 기본 제공                   | 기본 제공                 |\n| 사용 용도         | 개발 및 테스트 환경                | 개발 및 로컬 테스트 환경                 | 로컬 개발 및 간단한 테스트  | 다양한 K8s 환경 실습      |\n| 설치 난이도       | 간단함                             | 비교적 쉬움                              | 기본적으로 포함됨           | 다소 설정 필요            | 2. 맥 로컬환경에서 로  클러스터 구성하기 이제 실제로 를 사용하여  클러스터를 구축하고, Echo Server를 배포한 후 외부에서 접근해보보자. 개인적으로 집에서 Mac Mini에  클러스터 구성하고 여러 애플리케이션을 포트 기반으로 접근하고 있다. 2.1 필요 조건 및 Kind 설치 는 macOS 환경에서도 실행할 수 있으며, Homebrew를 사용하여 쉽게 설치할 수 있다. 2.2 Kubernetes 클러스터 생성 다음과 같은 Kind 설정 파일()을 생성하여 외부에서 접근할 포트를 설정한다. 로  클러스터 구성시 가 필수적으로 필요하다. 이제 클러스터를 생성해보자. 클러스터가 잘 생성이 되었는지 확인한다. 2.3 Echo Server 애플리케이션 배포 클러스터에 Echo Server를 배포해서 외부에서 잘 접근되는지 확인해본다. Echo Server를 배포하는  YAML 파일()이다.  명령어로 manifest로 서버를 생성한다. 외부에서 Echo Server 접근 Echo Server가 정상적으로 배포되었는지 확인한다. Echo Server에  로 API를 호출을 해보자. ------ 3. 마무리 이번 글에서는 를 이용하여 Mac에서  클러스터를 구성하고, Echo Server를 배포하여 외부에서 접근하는 방법을 다뤘다. 다른 Kubernetes 도구와 비슷하게 매우 쉽게 클러스터 생성하고 애플리케이션 배포도 쉽다는 것을 확인할 수 있었다. 이제 Kind를 활용하여 다양한  애플리케이션을 테스트해보세요! 🚀 4. 참고 - 로컬 Kubernetes 클러스터 - kind 설치\n- kind",
    "category": "database",
    "tags": [
      "kind",
      "k8s",
      "kubernetes",
      "클러스터",
      "minikube"
    ],
    "date": "2025-04-05T00:00:00.000Z"
  },
  {
    "id": "devops/maven-jacoco-coveralls-travis-ci-자바-프로젝트-coverage-생성하는-방법",
    "slug": "devops/maven-jacoco-coveralls-travis-ci-자바-프로젝트-coverage-생성하는-방법",
    "title": "Maven + JaCoCo + Coveralls + Travis CI : 자바 프로젝트 Coverage 생성하는 방법",
    "excerpt": "",
    "content": "1. 들어가며 Maven + Java 프로젝트의 코드 커버리지를 확인할 수 있는 방법에 대해서 알아보자. 전체적인 작업 흐름은 JaCoCo로 자바 커버리지를 생성하고 Coveralls 사이트로 업로드하여 결과를 확인할 것이다. - JaCoCo\n    - 코드 커버리지를 체크하는 라이브러리이다\n    - Unit Test 실행 후 커버리지 결과를 여러 형태(ex. HTML)의 파일로 생성해준다\n- Coveralls\n    - 웹 기반의 코드 커버리지 관리 사이트이다\n    - 저장소(ex. Github)와 연동하여 커버리지를 관리한다\n- Travis CI\n    - Github 에서 진행하는 프로젝트를 위한 CI 서비스이다. 이 포스팅에서 적용한 내용은 현재 개발 중인 개인 프로젝트 app-quotes 에서 확인할 수 있다. 2. Maven 설정 2.1 JaCoCo dependency 프로젝트에 Querydsl 파일이 있는 경우 configuration에서 exclude 태그로 제외한다. 2.2 Coveralls dependency Coveralls dependency 추가 시 Repo Token을 Coveralls 사이트에 확인하여 repoToken 태그에 넣어줘야 한다. 이 토큰 값으로 해당 프로젝트로 커버리지 결과가 업로드된다. <img src=\"image-20201212162832132.png\" alt=\"image-20201212162832132\" style=\"zoom:50%;\" /> JDK 높은 버전(ex. 14)으로 실행하는 경우 javax/xml/bind/DatatypeConverter 클래스를 찾지 못하는 오류가 발생할 수 있다. 클래스를 못 찾는 오류이어서 jaxb-api dependency를 추가하면 된다. 3. 실행 3.1 JaCoCo 보고서 생성 JaCoCo dependency를 추가 이후 아래 명령을 실행하면 target/site/jacoco 폴더에 HTML 파일이 생성된다. - skipTests=false\n    - Unit Test가 실행되어야 커버리지를 확인할 수 있다\n- maven.test.failure.ignore=true\n    - 실패 떨어지는 Unit Test가 있더라도 커버리지를 생성할 수 있도록 ignore 옵션을 준다 패키지별로 커버리지를 확인할 수 있다. 3.2 Coveralls 에 커버리지 결과 업로드 JaCoCo 실행 결과를 Coveralls로 업로드하려면 coveralls:report를 추가해서 실행한다. 성공적으로 업로드하면 완료된 job 링크로 확인할 수 있다. app-quotes는 67%의 커버리지 가지고 있다. 프로젝트 개발할 때 생각보다 Unit Test에 많은 신경을 쓰지 못했던 것 같은데, 나쁘지 않은 듯하다. 3.3 Travis 빌드로 코드 커버리지 Coveralls로 업로드하기 app-quotes는 이미 Travis CI에 연동되어 있다. Github + Travis CI 연동에 대한 내용은 다음에 다룰 예정이다.  파일에 지금까지 실행해본 명령어를 추가하면 된다. 4. 정리 이번 시간에는 Maven 프로젝트에서 JaCoCo와 Coveralls로 코드 커버리지를 확인하는 방법에 대해서 알아보았다. 다음 시간에는 여러 모듈로 구성된 한 프로젝트의 코드 커버리지를 확인하는 방법에 대해서도 알아볼 예정이다. 5. 참고 - https://woowabros.github.io/experience/2020/02/02/jacoco-config-on-gradle-project.html\n- https://github.com/trautonen/coveralls-maven-plugin\n- https://jojoldu.tistory.com/275",
    "category": "devops",
    "tags": [
      "jacoco",
      "coveralls",
      "travis",
      "coverage",
      "junit",
      "maven",
      "메이븐",
      "커버리지",
      "테스트"
    ],
    "date": "2020-12-12T00:00:00.000Z"
  },
  {
    "id": "devops/ngrinder-사용법에-대해서-알아보자",
    "slug": "devops/ngrinder-사용법에-대해서-알아보자",
    "title": "nGrinder 사용법에 대해서 알아보자",
    "excerpt": "",
    "content": "1. 들어가며 nGrinder는 스트레스 테스트 도구로 Grinder 오픈소스 기반으로 작성되었고 네이버에 의해서 개발되었다. nGrinder 설치에서부터 API 테스트까지 알아보자. 1.1 nGrinder 구성요소 | 구성       | 설명                                                         |\n| ---------- | ------------------------------------------------------------ |\n| controller | 웹 기반의 GUI 시스템으로 테스트 전반적인 작업이 이 컨트롤러에 의해서 작동된다 |\n| agent      | 컨트롤러 명령어를 받아서 target 머신에 프로세스와 스레드를 실행시켜 부하를 발생시킨다. 테스트하려는 머신에 agent를 설치하면 된다 |\n| target     | 테스트하려는 target 머신이다                                 | 1.2 환경 - 테스트환경\n    - 추가 장비가 없기 때문에 1대에서 테스트 환경을 구축하고 테스트한다\n- 테스트할 API\n    - springboot-quartz-in-memory 이 quartz 프로젝트의 API를 테스트한다 2. nGrinder 사용법 2.1 nGrinder 설치 nGrinder 릴리스 페이지에서 최신 WAR 버전을 다운로드한다. 다운로드이후  -jar 옵션을 두고 실행한다. 구동하려는 애플리케이션도 8080 포트를 사용할 예정이라 컨트롤러는 7070 포트로 띄운다. 이상없이 구동이 되면 http://localhost:7070/login 사이트 주소로 접속할 수 있다. 초기 어드민 id/password는 admin/admin이다. 2.2 Agent 설치 다음은 테스트하려는 target 머신에 agent를 설치해보자. agent는 사이트에 로그인한 이후 메뉴에서 admin > Download Agent를 클릭하면 agent가 다운로드된다. 다운로드한 파일을 아래 명령어로 압축을 푼다. agent 설정파일에 컨트롤러 IP 주소를 수정한다. 이 테스트에서는 로컬에 컨트롤러가 있어서 여기서는 수정없이 그냥 둔다. agent을 아래 shell script로 실행한다. 이상없으면 컨트롤러에 등록이 된다. 컨트롤러에서 잘 등록되었는지 admin > Agent Management 클릭하여 확인할 수 있다. 2.3 API 테스트 2.3.1 Script 작성하기 스크립트 생성후 Validate 버튼을 클릭해서 실제 API 호출에 이상이 없는지 확인후 저장을 한다. 2.3.2 Performance Test 생성하기 Performanc Test > Create Test 클릭후 원하는 테스트 설정을 한다. 2.3.3 테스트 실행 후 결과 테스트 실행시에도 라이브로 테스트 진행되는 것을 볼 수 있고 테스트 종료 이후에는 전체 테스트 Summary와 Detailed Report를 통해서 더 자세하게 결과를 확인할 수 있다. 3. 정리 여기서는 간단하게 nGrinder 테스트 도구의 사용법에 대해서 알아보았다. 실제 개발하는 웹 애플리케이션의 성능을 개선하려고 한다면, 구체적인 테스트 시나리오와 코드 개선 전후로 테스트 결과를 확인하면서 점진적으로 코드를 개선해 나가야 한다. 기회 되면 API 성능 개선한 사례에 대해서 포스팅할 예정이다. 4. 참고 - nGrinder\n    - http://naver.github.io/ngrinder/\n    - https://heedipro.tistory.com/279\n    - https://brownbears.tistory.com/26\n    - https://nesoy.github.io/articles/2018-10/nGrinder-Start\n- 설정\n    - https://programmer.help/blogs/ngrinder-2-stress-test-script-groovy.html",
    "category": "devops",
    "tags": [
      "ngrinder",
      "test",
      "performance",
      "api",
      "테스트",
      "성능테스트"
    ],
    "date": "2020-03-22T00:00:00.000Z"
  },
  {
    "id": "devops/travis-ci에서-slack-연동해서-빌드-notification-받기",
    "slug": "devops/travis-ci에서-slack-연동해서-빌드-notification-받기",
    "title": "Travis CI에서 Slack 연동해서 빌드 notification 받기",
    "excerpt": "",
    "content": "들어가며 Travis CI로 빌드 이후 notification을 Slack으로 받는 방법에 대해서 알아보자. Github 소스를 Travis CI로 배포하는 방법에 대한 설명은 다른 곳에 이미 많이 있기 때문에 부여 설명은 생략한다. 사전에 필요한 작업들이다. 간단하게 언급만 하고 넘어간다. - Slack 워크스페이스 생성 및 채널 생성\n- Github 소스 코드\n- Travis CI로 Github 소스 빌드 및 배포하기 빌드하려는 소스는 app-quotes 이고 이 포스팅에 작성되는 파일도 여기 소스에 포함되어 있다. Travis CI에 Slack 연동하기 1. Slack App Directory에서 Travis CI 추가 slack app directory에서 Travis CI App을 찾아 추가한다. Slack에 추가 > 채널 선택 > Travis CI 통합 앱 추가 버튼 클릭하여 Slack와 Travis CI를 연동한다. Travis CI 통합 앱 추가 이후에는 앱 토큰 정보를 알려준다. 이 정보를 기반으로 추후 작업을 하면 된다. 여기서도 언급되어 있는 것처럼 소스 코드가 public으로 되어 있어 앱 토큰이 노출되면 누구나 메시지를 보낼 수 있기 때문에 해당 토큰을 암호화하도록 하자. 연동이 잘 되면 Slack 채널에 메시지를 받게 된다. 2. travis.yml 설정에 slack 정보 추가하기 2.1 앱 토큰 암호화 하기 암호화에 필요한 travis 명령어를 설치한다. --com은 travis-ci.com을 사용하는 경우에 추가하여 로그인을 하면 된다. travis.yml 파일이 있는 곳에서 실행하면 기존 설정 + slack 정보가 추가되어 전체 설정을 출력해준다. 2.2 travis.yml 설정에 Slack 설정 추가 빌드 성공 + 실패시에도 메시지를 항상 받도록 설정해 두었다. Travis 빌드 설정에 대한 부여 설명은 Travis 문서를 참고하세요. 2.3 Travis CI에서 빌드하기 Travis CI에서 빌드를 직접해보면 Slack에 빌드 메시지를 잘 받는 것을 확인할 수 있다. 마무리 Travis CI 빌드시 Slack으로 빌드 메시지를 받을 수 있도록 연동 작업을 했다. 다음 포스팅에서는 Github Action과 Slack 연동 작업에 대해서 작업할 예정이다. 참고  Slack 연동하기      https://berndrabe.de/enabling-travis-ci-with-slack-integration/\n     https://www.edmondscommerce.co.uk/handbook/Development-Tools/Testing/Travis-CI/\n     https://riverandeye.tistory.com/entry/Travis-Travis-Ci-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-%EB%8F%84%EC%A0%84%EA%B8%B0-2-%EC%8A%AC%EB%9E%99-%EC%97%B0%EB%8F%99%ED%95%98%EA%B8%B0\n     https://docs.travis-ci.com/user/notifications/\n Travis CI로 배포하기      https://teichae.tistory.com/entry/Travis-CI%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%B0%B0%ED%8F%AC-1",
    "category": "devops",
    "tags": [
      "travis",
      "ci",
      "slack",
      "notification",
      "build",
      "빌드",
      "슬랙",
      "자동화"
    ],
    "date": "2020-10-22T00:00:00.000Z"
  },
  {
    "id": "git/git 서브모듈이란 실전 예제로 배우는 서브모듈 활용법",
    "slug": "git/git 서브모듈이란 실전 예제로 배우는 서브모듈 활용법",
    "title": "Git 서브모듈이란? 실전 예제로 배우는 서브모듈 활용법",
    "excerpt": "",
    "content": "1. 개요 Git 서브모듈이란?  서브모듈(Git Submodule)은 하나의  저장소 내에 다른  저장소를 포함할 수 있도록 해주는 기능이다. 이를 통해 별도의 저장소를 독립적으로 관리하면서도, 특정 프로젝트 내에서 재사용할 수 있다. 왜, 언제 사용하나? - 코드 재사용: 여러 프로젝트에서 동일한 라이브러리나 공통 코드베이스를 유지해야 할 때 유용하다\n- 독립적인 버전 관리: 서브모듈은 독립적으로 관리되므로, 메인 프로젝트와 별개로 버전 컨트롤을 할 수 있다\n- 공유 및 협업: 팀 내에서 공통 모듈을 여러 프로젝트에 걸쳐 사용해야 할 경우 편리하다 어떻게 사용하나?  서브모듈을 사용하면, 하나의 Git 저장소 내에서 별도의 저장소를 포함하고 관리할 수 있다. 기본적으로 다음과 같은 과정을 거친다: 1. 서브모듈 추가\n2. 서브모듈을 포함한 저장소 사용\n3. 서브모듈 업데이트 및 유지 관리\n4. 서브모듈 삭제 ------ 2. 서브모듈 사용해보기 2.1 메인 Repo에 서브모듈 Repo 포함시키기  서브모듈을 추가하려면 다음 명령어를 실행한다. 위 명령어를 실행하면, 서뷰모듈이 현재 디렉토리에 포함이 된다. 서브모듈을 추가하면  파일이 생성된다. 이 파일에는 서브모듈에 대한 정보가 포함되어 있다. 이 파일은 메인 저장소가 서브모듈을 어떻게 참조해야 하는지 정의한다. 2.2 서브모듈을 포함한 Repo 사용해보기 기본적으로  저장소를 클론하면 서브모듈은 자동으로 클론되지 않는다. 서브모듈까지 함께 가져오려면 다음과 같이 실행한다. 만약  옵션 없이 클론했다면, 서브모듈을 수동으로 초기화하고 업데이트해야 한다. 2.3 서브모듈 최신 코드 다운로드 받아보기 서브모듈에서 새로운 파일을 추가하고 커밋한 후 푸시한다. 서브모듈이 업데이트되었으므로, 메인 저장소에서도 최신 코드를 받아야 한다. 서브모듈이 최신 상태로 업데이트된 것을 확인할 수 있다. 2.4 메인 Repo에서 서브모듈 제거하기 서브모듈을 제거하려면 다음 단계를 수행하면 된다. 3. 마무리  서브모듈은 하나의 프로젝트에서 별도의  저장소를 효과적으로 관리할 수 있는 강력한 기능이다. 하지만 서브모듈을 사용할 때는 다음과 같은 점을 유의해야 한다. - 서브모듈은 독립적인 저장소이므로, 별도로 업데이트해야 한다\n-  옵션 없이 클론하면 서브모듈이 포함되지 않으므로 초기화 과정이 필요하다\n- 서브모듈을 제거할 때는  파일과  디렉토리를 정리해야 한다 이 가이드를 참고하여  서브모듈을 효과적으로 활용해보세요! > 참고로 예제로 작성한 건 아래 github에서 확인할 수 있습니다\n> - main-repo\n> - submodule-repo 4. 참고 - Git 도구  - 서브모듈\n- Git submodules\n- Git Submodule 사용하기",
    "category": "git",
    "tags": [
      "git",
      "git submodule",
      "깃 서브모듈",
      "서브모듈"
    ],
    "date": "2025-04-03T00:00:00.000Z"
  },
  {
    "id": "git/git-rebase로-병합하기",
    "slug": "git/git-rebase로-병합하기",
    "title": "Git Rebase로 병합하기",
    "excerpt": "",
    "content": "1. 들어가며 Git Rebase 하는 방법에 대해서 알아보겠습니다. Merge와 다르게 Rebase는 한 브랜치에 커밋된 여러 commit을 그래도 다른 브랜치에 병합할 수 있는 방식입니다. 아래와 같은 시나리오는 개발하면서 자주 하게됩니다. 새로 업데이트된 master 브랜치의 여러 커밋 내용을 현재 작업 중인 브랜치로 가져와서 개발을 계속하는 경우가 종종 있지요. Rebase 시나리오 1. 개발자1\n    1. master에서 새로운 작업 브랜치 (feature/GIT-7-working-branch)를 생성한다\n    2. feature/GIT-7-working-branch에서 코드을 수정한다\n2. 개발자2\n    1. master에서 새로운 작업 브랜치(feature/GIT-8)를 생성한다\n    2. 코드를 수정하고 master로 merge를 한다\n3. 개발자1\n    1. 최신 코드를 master로 부터 받아서 개발을 하도록 rebase를 진행한다 Git Client 프로그램 중에서 개인적으로 자주 사용하는 GitKraken 으로 Rebase를 해보고 또한 실제 git 명령어로도 같이 알아보겠습니다. 2. 개발 환경  OS : Mac OS\n GUI\n     IDE : Intellij\n     Git Client : Gitkraken\n Source code : github 3. Rebase 하기 먼저 GitKraken를 사용해서 rebase를 해보고 git 명령어로도 터미널에서 직접 rebase를 해보도록 하겠습니다. 3.1 Git Kraken로 rebase 방식으로 병합하기 3.1.1 기능별로 브랜치를 생성하기 Frank 개발자는 master로부터 feature/GIT-6-working-branch 이름의 새로운 브랜치를 생성하고 기능 개발을 시작합니다. 새로운 브랜치는 아래 스텝으로 브랜치를 생성합니다. master 브랜치를 선택 > 오른쪽 클릭 > Create branch here 선택 > feature/GIT-6-working-branch 브랜치 이름 입력 3.1.1.1 Frank 개발자 코드 수정후 commt하기 Frank 개발자는 체크아웃 이후에 코드 수정 후 3개의 커밋을 push합니다. 3.1.1.2 Joe 개발자 코드 수정후 commit하기 Joe 개발자도 다른 기능을 개발을 위해서 master로부터 새로운 브랜치 (feature/GIT-7)을 생성합니다. 코드 수정이후 3가지를 커밋을 했어요. 변경된 코드를 master에 반영하기 위해서 pull request (pr)를 생성합니다. GitKraken에서 아래 메뉴에서 pr을 생성할 수 있습니다. master 브랜치 선택 > 오른쪽 클릭 > Start a pull request to originmaster from originfeature/GIT-7 클릭 > pr 내용 입력 (ex. 제목, 내용등등) Pull request을 생성하면 Github 사이트로 바로 갈 수 있는 링크가 생성이 되어 링크버튼을 클릭하여 Github 사이트로 들어갑니다. Merge pull request 버튼을 클릭하여 병합을 진행합니다. 코드상 Conflict가 없어서 merge 하면 바로 성공 메시지가 뜹니다. Joe가 커밋한 내용(feature/GIT-7)은 master에 잘 반영되었네요. 3.1.2 Rebase로 최신 master로 rebase로 합병하기 Joe 개발자에 의해 커밋된 최신 코드를 현재 Frank 개발자가 개발 중인 feature/GIT-6-working-branch로 가져와 보도록 할게요. master 브랜치 선택 후 > 오른쪽 클릭 > 아래 둘중에 하나를 선택  Rebase feature/GIT-6-working-branch onto master 클릭\n Interactive Rebase feature/GIT-6-working-branch onto master 클릭\n     이 메뉴를 선택하면 여러 커밋을 선택적으로 Rebase 시킬 수 있다 (참고 : 3.1.3) Rebase를 하면 feature/GIT-6-working-branch 커밋 내용이 master 브랜치에 커밋 내용 다음에 오는 것을 볼 수 있습니다. master 브랜치에 가져올 내용이 3개가 있고 pull로 커밋을 가져온 이후 push를 하면 병합이 완료됩니다. remote/GIT-6-working-branch로 push한 결과입니다. GIT-6-working-branch에서 작업한 내용 (ex. annotation 제거, 어노테이션 제거, 코멘트 수정)이 master 브랜치 커밋 내용 다음으로 커밋되었네요. 3.1.3 Interactive 하게 Rebase 하기 같은 상황에서 여러 방식으로 rebase를 테스트해보기 위해 같은 지점에서 여러 브랜치(ex. GIT-6-working-branch-)를 미리 생성해 두었어요. Rebase를 interactive하게 진행하면 아래와 같이 커밋한 내용을 선택적으로 포함시킬지 시키지 않을 지 rebase 시점에 결정할 수 있습니다. 이 예제에서는 Start Rebase 버튼을 클릭해서 전체 커밋을 포함해서 rebase 시킵니다. 일반적인 방식으로 rebase한 결과와 같습니다. 3.2 Git Command로 직접 rebase해보기 Git Client를 사용하면 쉽게 브랜치를 생성하고 병합할 수 있지만, Git 명령어도 같이 알아보겠습니다. 3.2.1 작업 브랜치로 전환하기 작업 브랜치로 전환합니다. 3.2.2 Rebase 하기 아래 명령어로 현재 작업 브랜치를 master로 rebase 시킵니다. 매우 간단하죠? Git Kraken에서 rebase한 내용을 보면 결과가 같은 것을 확인할 수 있습니다. 3.3 Merge로 병합하기 Rebase를 하면 지금까지 커밋한 여러 내용이 유지가 되어 history가 남게 되는 반면에 Merge로 병합을 하는 경우에는 여러 커밋이 하나의 커밋 내용으로 병합되기 때문에 커밋 history를 잃어버리게 되는 단점이 있습니다. 결과적으로 어떻게 다른지 GitKraken에서 merge로 병합을 해보겠습니다. master 선택후 > 오른쪽 클릭 > Merge master into feature/GIT-6-working-branch-merge-test master 브랜치에 커밋된 내용이 통째로 GIT-6-working-branch-merge-test로 merge가 되었습니다. 4. 결론 Git Client와 터미널상에서 rebase 병합을 진행해보았습니다. 둘 다 쉽게 rebase를 할 수 있고 개인적인 취향에 맞게 가장 익숙한 방식으로 진행하면 될 것 같습니다. 추가로 병합 시 merge와 rebase의 차이점도 간단하게 알아보았는데요. master의 커밋 history를 남기는 상태에서 병합하고 싶은 경우에는 merge 대신에 같이 알아보았던 rebase 방식으로 병합을 하면 되겠습니다. 5. 참고  Rebase란\n     https://git-scm.com/book/ko/v1/Git-브랜치-Rebase하기\n Rebase vs Merge\n     https://elegantcoder.com/git-merge-or-rebase/\n Rebase in GitKaren\n     https://blog.axosoft.com/rebasing-gitkraken-vs-cli/\n Rebase는 언제 해야 하나?\n     http://dogfeet.github.io/articles/2012/git-merge-rebase.html\n 명령어로 직접 rebase하기\n     https://git-scm.com/book/ko/v1/Git-브랜치-Rebase하기",
    "category": "git",
    "tags": [
      "git",
      "github",
      "rebase",
      "merge",
      "깃",
      "깃허브",
      "병합"
    ],
    "date": "2019-08-11T00:00:00.000Z"
  },
  {
    "id": "git/git-reset으로-커밋된-내용-다시-되돌리기-using-gitkraken",
    "slug": "git/git-reset으로-커밋된-내용-다시-되돌리기-using-gitkraken",
    "title": "Git Reset으로 커밋된 내용 다시 되돌리기 (using GitKraken)",
    "excerpt": "",
    "content": "1.Git Reset Git으로 작업하다 보면 커밋된 이력을 다시 되돌려야 할 때가 종종 발생한다. Git Reset에서는 아래와 같이 3가지 옵션을 제공한다. 각 옵션의 차이점에 대해서 알아보자. 1.1 Git Reset 종류 - \n    - 커밋만 되돌리고 싶을 때 사용한다\n    - 변경한 코드는 stage area에 남겨지게 된다\n-  (기본 옵션)\n    - 변경한 인덱스(stage area)의 상태를 원래대로 되돌리고 싶을 때 사용한다\n    - 변경한 코드는 작업 디렉토리에 남는다. 다시 커밋하려면 staged area에 다시 추가해야 한다\n- \n    - 최근의 커밋을 완전히 버리고 이전의 상태로 되돌리고 싶을 때 사용한다\n    - 변경한 코드는 다 사라지기 때문에 hard 코드 사용시 주의가 필요하다 1.1.1 용어 정리 | 용어                 | 설명                           |\n| -------------------- | ------------------------------ |\n| HEAD                 | - 현재 브랜치를 가리킨다       |\n| Index (Staging Area) | - 바로 커밋할 파일들이 있는 곳 |\n| Working Directory    | - 수정할 파일들이 있는 곳      | 1.2 GitKraken에서 Git Reset 해보기 1.2.1 Reset 하는 방법 1.2.1.1 Git 명령어로 Git Reset 명령어는 아래 형식으로 실행된다. 모드 옵션은 위 3가지 중에 하나이고 없는 경우 기본 값으로  모드로 동작한다. 리리셋할 커밋은 commit 번호나 HEAD형식으로 지정할 수 있다. > HEAD 현재 커밋될 위치를 나타낸다. HEAD2의 의미는 이전 2개 커밋을 의미한다. 여러 가지 Git Reset 명령어들이다. 1.2.1.2 GitKraken으로 GitKraken에서는 commit 목록에서 리셋하려는 커밋에서 우 클릭하면 pop-up 메뉴가 뜬다. 여기서 Reset to this commit 메뉴에서 원하는 모드를 선택한다. 1.2.2 Reset 종류별 예제 모음 이제 GitKraken에서 어떻게 3가지 모드로 리셋을 할 수 있는 지 직접 해보자. 현재 GIT-13을 커밋하고 remote repository에 푸시를 한 상태이다. 이제 전 커밋으로 3가지 모드로 되돌려 보자. 1.2.2.1 Soft Reset Soft Reset이다. 그전 브랜치를 가리키고  파일은 이미 에 있어서 다시 커밋을 할 수 있다. 1.2.2.2 Mixed Reset (기본 모드) Mixed Reset이다. 브랜치 위치는 전 커밋으로 위치가 되었고 그전에 커밋한 건 파일들은 에서 제외되어 커밋하려면 에 다시 추가해줘야 한다.  명령어로도 파일이 staged 되어 있지 않아 로 추가해야 한다고 설명해주고 있다. 1.2.2.3 Hard Reset Hard Reset이다. 이전 커밋으로 되돌리면 그전 커밋은 그냥 사라지게 되어 사용시 주의가 필요하다. 1.2.3 제목 변경해서 다시 commit하는 예제 (mixed 모드) 지금까지는 3가지 모드별로 어떤 차이점 있는지 알아보았다. 마지막 예제는 이미 원격으로 푸시된 커밋의 제목을 수정하는 예제로 마무리해보자. 1.mixed 모드로 전 커밋으로 리셋시킨다. 다시 커밋할 파일을 staged area에 추가하고 커밋 제목을 다시 변경해서 커밋한다. 2. 원격으로 강제 푸시를 한다. 3. 변경된 제목으로 잘 푸시되었는지 확인하다. 2. 정리 Git Reset는 이제 마스터했으니 적절하게 사용하면 된다. 본 포스팅에서 작성한 코드는 github에서 확인할 수 있다. 3.  참고 - https://www.devpools.kr/2017/02/05/%EC%B4%88%EB%B3%B4%EC%9A%A9-git-%EB%90%98%EB%8F%8C%EB%A6%AC%EA%B8%B0-reset-revert/\n- https://backlog.com/git-tutorial/kr/stepup/stepup63.html\n- https://git-scm.com/book/ko/v2/Git-%EB%8F%84%EA%B5%AC-Reset-%EB%AA%85%ED%99%95%ED%9E%88-%EC%95%8C%EA%B3%A0-%EA%B0%80%EA%B8%B0\n- https://opentutorials.org/module/4032/24533\n- https://c10106.tistory.com/3930\n- https://antilog.tistory.com/33\n- https://medium.com/@joongwon/git-git-%EC%9D%98-%EA%B8%B0%EC%B4%88-a7801f45091d\n- https://c10106.tistory.com/3943",
    "category": "git",
    "tags": [
      "git",
      "reset",
      "revert",
      "gitkraken",
      "깃",
      "깃허브",
      "리셋"
    ],
    "date": "2021-01-04T00:00:00.000Z"
  },
  {
    "id": "git/git-브랜치-여러개-한번에-삭제하기",
    "slug": "git/git-브랜치-여러개-한번에-삭제하기",
    "title": "Git 브랜치 여러 개 한번에 삭제하기",
    "excerpt": "",
    "content": "Git local, remote 브랜치를 한번에 삭제하는 방법에 대해서 알아보자. 1. 다중 Local 브랜치 삭제하기 1.1 삭제하려는 브랜치 목록보기  명령어로 삭제하려는 브랜치 목록을 확인한다. 1.2 검색 패턴으로 한번에 브랜치를 삭제하기 으로 찾은 브랜치를  명령에 pipeline으로 넘겨줘서 삭제한다. >  명령어는 앞 명령어의 출력 결과를 다음 명령어의 인자로 넘겨주는 명령이다. 2. 다중 Remote 브랜치 삭제하기 2.1 Remote 브랜치 목록 확인하기 Remote 브랜치도 같은 원리로 삭제할 수 있다. git의  () 옵션으로 Remote 브랜치의 목록을 확인할 수 있다. 2.2 Remote 브랜치 다중 삭제하기 검색 패턴으로 찾은 브랜치를 xargs로 하나씩 삭제한다.  명령어는 local 브랜치가 삭제되고 remote 브랜치도 삭제할 때 사용한다. 3. 참고  git branch\n     https://medium.com/@rajsek/deleting-multiple-branches-in-git-e07be9f5073c\n     https://stackoverflow.com/questions/10555136/delete-multiple-remote-branches-in-git\n     https://www.educative.io/edpresso/how-to-delete-remote-branches-in-git\n     https://trustyoo86.github.io/git/2017/11/28/git-remote-branch-create.html\n xargs\n     https://rsec.kr/?p=91",
    "category": "git",
    "tags": [
      "git",
      "github",
      "branch",
      "multiple",
      "delete",
      "깃",
      "깃허브",
      "다중",
      "삭제",
      "브랜치"
    ],
    "date": "2020-07-11T00:00:00.000Z"
  },
  {
    "id": "git/git에서-불필요한-로컬-브랜치-깔끔하게-삭제하는-방법",
    "slug": "git/git에서-불필요한-로컬-브랜치-깔끔하게-삭제하는-방법",
    "title": "Git에서 불필요한 로컬 브랜치 깔끔하게 삭제하는 방법",
    "excerpt": "",
    "content": "개요 을 사용하여 코드 버전 관리를 하다 보면, 작업을 위해 새로운 브랜치를 생성하고 작업한 후  한 뒤에도 로컬에 사용되지 않는 브랜치들이 계속 남아 있게 된다. 원격(Remote) 브랜치의 경우, GitHub의 설정에서  옵션을 활성화하면 자동으로 삭제할 수 있다 하지만 로컬(Local) 브랜치는 자동으로 삭제되지 않으며, 매번 수동으로 해야 한다. 번거로움을 덜어주기 위해서 간단하게 스크립트를 작성해본다. > 스크립트 작성은 의 도움을 받았다. 요세 요구사항만 잘 전달해주면 완벽한 스크립틀 작성해주고 있어서 너무 잘 사용하고 있다. Usage 명령어 사용법 -  옵션이 있는 경우에는  하게 실행하도록 되어 있어 로컬 브랜치를 삭제하기전에  확인을 받고 삭제하도록 되어 있다 실행 예제 Source Code 다음은 해당 스크립트의 전체 소스 코드이다. 이 스크립트를 활용하면 로컬에서 불필요한 Git 브랜치를 효과적으로 정리할 수 있다.",
    "category": "git",
    "tags": [
      "git",
      "github",
      "merge",
      "깃",
      "깃허브",
      "로컬브랜치 정리"
    ],
    "date": "2025-04-02T00:00:00.000Z"
  },
  {
    "id": "git/qa-git-관련-질문-모음",
    "slug": "git/qa-git-관련-질문-모음",
    "title": "Q&A : Git 관련 질문 모음",
    "excerpt": "",
    "content": "개인적으로 모르는 부분 적어두고 알게 되는 부분에 대해서 간단하게 정리해둔 자료입니다.\n미 답변중에 알고 계신 부분 있으면 코멘트 달아주세요. 감사합니다. Q&A 전체 목록 <span style=\"color:orange\">[미 답변 질문]</span> - - - - <span style=\"color:orange\">[답변완료]</span> <span style=\"color:brown\">1. WIP란?</span>\nWork In Progress란 뜻입니다. Gitkraken이 WIP를 자동으로 생성해주었는데요. revert할 때 현재 브랜치에서 작업하는 파일이 있어서 자동으로 stash을 하면서 WIP를 생성한 상황입니다. 참고\n https://stackoverflow.com/questions/15763059/github-what-is-a-wip-branch",
    "category": "git",
    "tags": [
      "faq",
      "git",
      "github",
      "gitkraken",
      "wip",
      "commit"
    ],
    "date": "2019-07-03T00:00:00.000Z"
  },
  {
    "id": "go/go-recover-함수에서-반환값을-반환하는-예제",
    "slug": "go/go-recover-함수에서-반환값을-반환하는-예제",
    "title": "Go Recover 함수에서 반환값을 반환하는 예제",
    "excerpt": "",
    "content": "Validation API 함수를 개발하는 과정에서 복잡한 expression을 evaluation 하는 과정에서 잘못된 표현식의 경우에는 panic이 발생하는 경우가 있었다. panic이 발생하여  함수로 서버가 죽지 않게 되어 있지만, Validation API의 경우에는 client에 잘못된 표현 식이라는 응답 값을 내려줘야 한다. 1.panic(), recover() 함수  함수에서 어떻게 반환값을 리턴할 수 있는지 알아보자. 오늘 사용할 Golang 내장함수들이다. - \n    -  함수가 실행되면 즉시 멈춘다\n    - 함수에  함수를 모두 실행한 후 종료한다\n- \n    - 자바에서는  구문을 사용하여 예외 처리를 하는 것처럼  함수도 비슷한 역할을 한다\n    -  이 발생한 프로그램이 종료되는 것을 막고 복구한다\n    - 와 같이 사용해야 한다 g() 함수에서 이 발생하고  함수들이 call stack에 쌓인 순서대로 실행이 된 것을 볼 수 있다. 그리고  함수에 정의된 함수 실행시  함수를 실행하면서 패틱 상태를 다시 정상 상태로 복구하여 나머지  함수의 코드를 실행한다. 2.recover() 함수에서 반환 값 리턴하기  함수에 반환 값을 반환하려면 의 Return 값에 이름을 부여하여 부여한 이름변수에 값을 지정하면  함수 실행후 반환 값으로 리턴이 된다. Unit Test로 실행하여 확인해보면 반환 값이 로 리턴되는 것을 확인할 수 있다. 3.recover()에서 stack trace 출력하기 Panic 발생 후 recover를 하고 stack trace를 출력하여 조금 더 디버깅을 쉽게 하려면 Debug 패키지에 포함된  함수를 사용한다.  함수에 의해서 stack trace가 아래와 같이 출력된다. 4.참고 - http://golang.site/go/article/20-Go-defer%EC%99%80-panic\n- https://github.com/kenshin579/tutorials-go/pull/287\n- https://stackoverflow.com/questions/68554968/why-does-go-panic-recover-to-return-value-with-local-variable-not-work\n- https://stackoverflow.com/questions/19934641/go-returning-from-defer\n- https://stackoverflow.com/questions/33167282/how-to-return-a-value-in-a-go-function-that-panics\n- https://golangbot.com/panic-and-recover/",
    "category": "go",
    "tags": [
      "golang",
      "go",
      "recover",
      "return",
      "panic"
    ],
    "date": "2022-08-07T00:00:00.000Z"
  },
  {
    "id": "go/go-strings-문자열-함수",
    "slug": "go/go-strings-문자열-함수",
    "title": "Go Strings (문자열 함수)",
    "excerpt": "",
    "content": "문자열 함수 Golang에서 표준 라이브러리중에  패키지에서 많이 유용하게 사용할 수 있는 문자열 함수들을 제공한다. 여러 예제를 통해서 문자열를 다루어보자. 1. Search (Contains, Prefix/Suffix, Index) 2. Replace (Uppercase/Lowercase, Trim, Map) >  함수는 인자로 함수와 문자열을 받고 문자열의 각 character마다 함수를 적용하는 함수이다. 3. Split (Split, Fields) > Fields는 문자열을 white space character (unicode.IsSpace로 정의됨) 기준으로 문자열을  시켜준다 4. Concatenate (+, Sprintf, Builder)  메서드로 원하는 여러 타입을 formatting을 통해서 문자열로 반환해주어 원하는 문자열을 쉽게 만들 수 있다. > r에서 제공하는 메서드를 통해서 문자열 조합을 더 빠르고 효과적으로 할수 있는 기능을 제공한다 5. Join (Join, Repeat) 6. Format, Convert (strconv) 여기서 작성한 예제는 github를 참고해주세요. 참고 - https://yourbasic.org/golang/string-functions-reference-cheat-sheet/\n- http://pyrasis.com/book/GoForTheReallyImpatient/Unit46\n- https://mingrammer.com/gobyexample/string-functions/\n- http://cloudrain21.com/go-how-to-concatenate-strings\n- http://pyrasis.com/book/GoForTheReallyImpatient/Unit46/02\n- https://golang.org/pkg/strings/\n- https://yourbasic.org/golang/string-functions-reference-cheat-sheet/",
    "category": "go",
    "tags": [
      "golang",
      "string",
      "replace",
      "split",
      "join",
      "repeat",
      "스트링",
      "문자열",
      "고랭"
    ],
    "date": "2021-05-28T00:00:00.000Z"
  },
  {
    "id": "go/go-ternary-operator-삼항연산자",
    "slug": "go/go-ternary-operator-삼항연산자",
    "title": "Go Ternary Operator (삼항연산자)",
    "excerpt": "",
    "content": "삼항연산자란? 삼항 연산자 (Ternary Operator)는 아래 형식으로 if 조건문 대신 사용할 수 있는 문법이다. JavaScript, Java와 같은 여러 언어에서 지원하는 문법이고 아래 코드는 자바의 삼항 연산자이다. Golang 언어에서 삼항 연산자가 빠진 이유는 심플한 설계를 유지 하기 위해서 Golang에서 빠졌다. 긴 버전 자체가 가독성도 높고 삼항 연산자를 알지 못해도 긴 버전으로는 누구나 쉽게 이해가 가능하여 언어 설계 단계에서 빠졌다. 삼항연산자를 지원하지 않지만, 함수를 이용해서 삼항 연산자 처럼 코딩할 수도 있다. \"done\" 상태 값인 경우에 true, false를 반환하는 예제이다. 예제 코드는 github를 참고해주세요. 참고 - https://golangdocs.com/ternary-operator-in-golang\n- https://needneo.tistory.com/105",
    "category": "go",
    "tags": [
      "go",
      "golang",
      "ternary",
      "operator",
      "고랭",
      "연산자",
      "삼항",
      "삼항연산자",
      "3항연산자"
    ],
    "date": "2021-05-18T00:00:00.000Z"
  },
  {
    "id": "go/go-test-suite-lifecycle-메서드",
    "slug": "go/go-test-suite-lifecycle-메서드",
    "title": "Go Test Suite (Lifecycle 메서드)",
    "excerpt": "",
    "content": "Golang에서는 testify library에서 제공하는 여러 기능 (ex. assertion, mocking, suite)를 통해서 쉽게 unit test를 작성할 수 있다. 특정 config 설정에 따라 전체 테스트를 skip 해야 하는 경우가 있다. 이런 경우 Test Suite를 이용하면 해당 케이스를 쉽게 해결할 수 있다. - ex. clustertest.go - redis cluster 모드인 경우에만 실행하기  메서드를 실행하면 Suite에 정의된 메서드들이 차례로 실행되기 때문에 그전에 조건을 두어 참인 경우에  메서드를 호출하면 전체 테스트가 skip 처리된다. Suite 패키지는 테스트 실행 전후로 여러 인터페이스 제공하여 테스트 전후로 다양한 처리가 가능하다. 여러 life cycle 메서드에 대해서 알아보자.  실행 로직을 확인해보면 어떻게 suite 인터페이스 메서드들이 테스트 실행 life cycle의 어느 시점에 실행되는지 확인할 수 있다. - 전처리\n    -  - suite에서 전체 테스트 실행 전에 한번만 실행된다\n    -  - suite에서 각 테스트 실행 전에 실행된다\n    -  - 테스트가 실행하기 전에  인자를 받아 실행하는 함수이다\n- 후처리\n    -  - 테스트가 실행후에 suiteName testName 인자를 받아 실행하는 함수이다\n    -  - suite에서 각 테스트 실행후에 실행된다\n    -  - suite에서 모든 테스트가 실행된 후에 실행된다 Test Suite로 테스트를 작성하려면  구조체를 담는 를 생성하고 해당 에 대한 suite 메서드를 정의하면 된다. 작성한 전체 테스트 코드입니다. 테스트를 실행해서 Life cycle를 확인해볼까요? 참고 - https://brunoscheufler.com/blog/2020-04-12-building-go-test-suites-using-testify - https://pkg.go.dev/github.com/stretchr/testify/suite",
    "category": "go",
    "tags": [
      "golang",
      "test",
      "suite",
      "testify",
      "before",
      "after",
      "lifecyle",
      "unit test",
      "test suite"
    ],
    "date": "2021-07-17T00:00:00.000Z"
  },
  {
    "id": "go/golang-기반의-분산-스케줄러-asynq에-대해서-알아보자",
    "slug": "go/golang-기반의-분산-스케줄러-asynq에-대해서-알아보자",
    "title": "Golang 기반의 분산 스케줄러 - Asynq에 대해서 알아보자",
    "excerpt": "",
    "content": "1. 개요 서버 개발을 하다 보면 다양한 작업을 백그라운드에서 처리해야 하는 상황이 자주 발생한다. 이러한 작업은 주기적으로 실행되거나, 특정 이벤트에 반응하여 실행된다. 이 때문에 서버 개발에서 스케줄러가 필수적인 기능이기도 하다. 또한 서버 이중화를 위해 분산 환경에서 다중 서버에서도 스케줄링이 원활하게 동작할 수 있어야 한다. 스프링으로 개발하고 있다면 스프링에서 제공하는 Quartz를 이용하면 된다. Quartz 꽤 오래된 프로젝트라서 스프링에서는 de facto로 사용되고 있다. Golang으로 개발한 서버에서도 분산 스케줄러를 개발해야 해서 어떤 걸 사용하면 좋은지 검토를 해보았다. 1.1 golang 스케줄러 - \n  - Job 정보를 redis 서버에 저장하고 여러 인스턴스를 띄워도 트리거하는 주기가 짧아지는 이슈는 없다\n- \n  - Redislock 으로 한 서버에서 실행 가능하도록 하지만, 서버의 수가 늘어나면 트리거하는 주기가 더 짧아지는 이슈가 있다\n- \n  - 스프링 Quartz 구현에서 영감을 얻어 시작된 버전이지만, 분산 스케줄러 지원은 하지 않는다\n- \n  - Redis 명령어로 atomic 하게 실행되지만, go-cron가 가지고 있는 trigger 이슈는 동일하게 가지고 있다\n- \n  - 최신 업데이트가 없는 걸로 봐서는 유지보수가 안되는 것으로 판단된다\n- 다른 스케줄러도 검토해봤지만, 큰 의미가 없어서 생략한다 조사해 본 결과  스케줄러가 그래도 제일 괜찮은 스케줄러라고 생각이 들어서 조금 더 조사한 내용을 정리해 본다. 참고 - https://awesome-go.com/job-scheduler/ 2.  기능에 대한 정리 는 task 를 queue에 넣고 비동기적으로 worker가 task를 처리하는 library 이다. 내부에서 사용하는 정보 (ex. task, scheduler)는 redis에 저장하고 있다 2.1 기본 동작 - 는 분산 락을 사용하지 않고 queue에 task를 넣고 server가 queue에서 task를 가져가서 각 worker goroutine 에서 처리하도록 내부적으로 동작한다\n- task는 두가지 방식으로 추가 할 수 있다 (redis에 저장됨)\n  -  client로 task를 queue에 넣거나\n  - 주기적으로 실행할 수 있도록 periodic task (cron)로 등록할 수 있다\n- 여러 queue에 우선순위 설정도 가능하다 2.2 주 기능 제공하는 기능이 많지만, 주로 지금 개발하는 어플리케이션에서 필요하다고 생각되는 것 위주로 정리했다. - 최소 한 번의 작업 실행을 보장한다\n  - 서버 인스턴스 수가 늘어나도 트리거 하는 주기가 달라지지 않는다\n- Periodic Tasks 주기적인 작업 등록 가능하다\n  - 주기 작업은 golang cron library 사용해서 구현이 되어 있다\n  - 주기 작업에 대한 정보는 5초마다 주기적으로 redis에 쓰여지고 있다\n- 중복 task 등록에 대한 설정 지원을 한다\n- Redis 지원한다\n  - redis cluster, sentinel\n- Prometheus 연동을 지원을 해서 queue 에 대한 metrics 를 수집하고 시각화 할 수 있다\n- Web UI, asynmon 도 지원한다\n- CLI 를 지원해서 queue 정보를 확인할 수 있다 2.3 샘플 코드 에서는 DB로 Redis를 사용하고 있어서 redis 서버를 아래 명령어로 실행시킨다. > 코드가 unit test로 작성이 되어 있어서 testcontainers로 redis 실행할 수 있는데, 귀찮아서 refactoring은 하지 않았습니다 ^^ 2.3.1 asynmon UI를 실행하기  UI에서 redis에 저장되는 정보를 쉽게 확인할 수 있어서 을 먼저 실행한다. http://localhost:8080로 접속하면 아래와 같이 확인이 뜬다. 2.3.1 한번만 실행하는 경우 아래 코드는 실제로는 task를 실행시키지는 않고 단순히 task를 redis에 저장하는 역할을 한다. 로 task를 등록할 때, 여러 옵션을 줄 수 있는데, 자주 사용하는 옵션만 언급하고 넘어간다. - \n  - Queue 이름을 지정할 수 있는 옵션으로 지정하지 않는 경우에는  값으로 queue가 생성이 된다 - \n  - 현재 시간 기준으로 2초 뒤에 실행 한다는 의미이다 - \n  - task 실행 후 결과를 얼마나 유지할 지 결정하는 옵션이다 - \n  - 이 시간 동안 lock이 걸려서 다른 task가 실행하지 못하게 하는 옵션이다 -    - 로 task id를 지정할 수 있다. 지정하지 않으면 random 값을 사용한다\n  - 같은 taskID가 등록이 되면  오류를 반환한다 코드를 실행하면 default queue에 2개의 task가 추가 된 것을 확인할 수 있다. Task가 등록만 되어서 실제로 task를 실행하려면  서버를 실행시켜야 한다. 실행이 잘되었는지는 에서 쉽게 확인할 수 있다. >  옵션으로 task가 등록이 되어 Completed 란에서 실행 결과를 확인할 수 있다. 2.3.2 주기적인 작업 실행 주기적으로 실행하려면, 로객체를 생성하고  메서드로 task를 등록하면 된다. 주기적인 작업 Task 등록 시점은 서버를 시작하는 시점에 등록하면 된다. 하지만, 분산 환경에서 여러 서버에서 스케줄러를 실행하는 경우에는 중복으로 등록되어 의도와 달리 여러 번 실행하게 되는 이슈가 있다. > 에서는 여러 서버에 같은 task는 하나만 등록하는 방법은 존재를 하지 않는다 같은 Task가 여러 번 등록이 되더라도 한 번만 실행하는 해결책은  Github Issue에 제시된 해결책이 있어서 적용해 보았다. 중복 실행을 방지하는 위한 해결책 1.,  이 두 옵션을 사용한다\n   - 실험 결과 주기가 일정하지 않다. 2초로 실행하라고 했는데, 3,4초에 실행되는 이슈가 있었다 2. Unique Tasks ← 이걸로 선택함\n   - Task 등록시  이 옵션을 사용하면 실행하는 시간동안 RedisLock을 사용해서 서버 인스턴스가 늘어나도 중복으로 여러 번 실행되지 않고 설정한 일정한 주기로 실행된다 참고 - How to prevent duplicate scheduler task registe across multiple Asynq server nodes\n- FEATURE REQUEST Distributed Scheduler 2개 인스턴스로 실행하면 주기적인 Task가 2개 등록된 것을 확인할 수 있다. > Periodic task는 언제 레디스에 쓰여지나?\n>\n> - Periodic task에 등록은 cron library (메모리에 쓰여지게 된다)로 실행이 된다\n> - Scheduler start를 하면 그때 가 주기적으로 5초마다 redis에 쓰게 되어 있음 3. 정리 분산 환경을 고려하지 않는다면 Golang library에서도 쓸만한 스케줄러가 있지만, Production 환경에서는 분산 스케줄러를 고려를 해야 해서 그런 경우에는 asynq 사용을 추천해 본다. > 포스팅에서 작성한 코드는 여기서 확인할 수 있습니다. 4. 참고 - https://github.com/hibiken/asynq/wiki\n- https://github.com/hibiken/asynq",
    "category": "go",
    "tags": [
      "golang",
      "go",
      "async",
      "scheduler",
      "job scheduler",
      "task",
      "task processor",
      "quartz",
      "spring",
      "cron",
      "redis",
      "스케줄러",
      "분산 스케줄러",
      "레디스",
      "고랭",
      "스프링"
    ],
    "date": "2024-06-24T00:00:00.000Z"
  },
  {
    "id": "go/go에서-삼-도트-dot-사용방법-three-dots-usage",
    "slug": "go/go에서-삼-도트-dot-사용방법-three-dots-usage",
    "title": "Go에서 삼 도트 (dot) 사용방법 (Three Dots Usage)",
    "excerpt": "",
    "content": "1. 들어가며 Go에서  삼 도트(dot) 사용법에 대해서 알아보자. Go에서는 아래 4가지 방법으로 사용된다. - 함수의 인자에 가변 인자로 선언하는 경우\n- 가변 인자를 인자로 받는 함수에 slice를 넘겨주는 경우\n- 배열 리터럴에서 길이 지정하는 경우\n- Go 명령어 wildcard로 사용하는 경우 2.삼 도트 사용법에 대해서 알아보자 2.1 함수의 인자에 가변 인자로 선언하는 경우  함수에 가변 인자를 넘겨주기 위해서는 인자 선언시 로 선언하면 가변인자가 된다.  함수 호출시 인자 값을 2, 3개로 자유롭게 넘겨줄 수 있게 된다. 2.2 가변 인자를 인자로 받는 함수에 slice를 넘겨주는 경우  함수는 가변인자로 선언되어 있기 때문에 으로 넘겨줘야 하는데, 슬라이스로 선언된 컬렉션을 하나하나 입력하기는 매우 불편한다. 그래서 Go에서는  슬라이스 뒤에 3개의 도트 표기법을 사용하면 Go에서 가변인자에 unpack을 해서 전달해준다. 2.3 배열 리터럴에서 길이 지정하는 경우 배열 리터럴에서  표기법을 사용하면 리터럴의 요소 수와 동일하게 길이를 지정이 된다. 2.4 Go 명령어 wildcard로 사용하는 경우 Go 명령에서  표기법은 패키지 목록을 wildcard로 사용하겠다는 의미를 가지고 있다. 위 명령어는 현재 폴더에서 모든 폴더에 있는 테스트 파일을 실행하라는 의미이다. 3. 마무리 Go에서  표기법의 여러 사용방법에 대해서 알아보았고 여기서 작성한 코드는 github에서 확인할 수 있다. 오늘 포스팅은 여기까지... 4. 참고 - https://yourbasic.org/golang/three-dots-ellipsis/\n- https://blog.learngoprogramming.com/golang-variadic-funcs-how-to-patterns-369408f19085\n- https://golangbot.com/variadic-functions/",
    "category": "go",
    "tags": [
      "go",
      "golang",
      "dot",
      "three",
      "점",
      "고랭",
      "도트",
      "표기법"
    ],
    "date": "2021-05-08T00:00:00.000Z"
  },
  {
    "id": "go/go에서-컬렉션-정렬하는-방법-go-sort",
    "slug": "go/go에서-컬렉션-정렬하는-방법-go-sort",
    "title": "Go에서 컬렉션 정렬하는 방법 (Go Sort)",
    "excerpt": "",
    "content": "1. 들어가며 Go에서는 여러 컬렉션 타입에 대해서 어떻게 정렬할 수 있는지에 대해서 알아보자. - Primitive 데이터 타입 정렬하기\n- Custom comparator로 정렬하기\n- Sort interface로 정렬하기\n- Map에서 특정 key/value로 정렬하기 2.Sort 4가지 방법 2.1 Primitive 데이터 타입 정렬하기 Primitive 데이터 타입에 대해서는 아래 함수들을 제공해주고 있다. - \n- \n-  2.2 Struct 구조체 정렬하기 사용자 정의  함수를 정의해서 구조체의 속성값 기준으로 정렬할 수 있다. 아래는 의 나이 적은 순으로 정렬한 예제이다. 2.3 정렬 인터페이스로 구조체 정렬하기 정렬 인터페이스로도 구조체를 정렬할 수 있다.  메서드는  정렬 인터페이스를 인자로 받는다. 정렬 인터페이스는 아래 3가지  메서드를 구현하면 해당 구조체를 정렬할 수 있다. 먼저  순으로 정렬하기 위해 을 정의한다. 그리고 정의한 타입에 대해서 위 3가지 메서드를 구현한다.  메서드 인자에  배열 구조체를  타입 변환해서 정렬시킨다. 2.4 Map 의 특정 key나 value 값으로 정렬하기 마지막으로 Map 데이터에서 특정 key나 value 값으로 정렬하는 방법에 대해서 알아보자. 아래 예제에서는 map에서 key 값만 먼저 정렬시킨 후 정렬된 key 값으로 map에서 가져오면 특정 key 값으로 정렬할 수 있다. 3. 마무리 다양한 방법으로 여러 데이터 타입 배열을 정렬해보았다. 여기서 작성한 코드는 github에서 확인할 수 있다. 4. 참고 - https://yourbasic.org/golang/how-to-sort-in-go/\n- https://mingrammer.com/gobyexample/sorting-by-functions/\n- https://mingrammer.com/gobyexample/sorting/\n- http://pyrasis.com/book/GoForTheReallyImpatient/Unit54/01",
    "category": "go",
    "tags": [
      "go",
      "golang",
      "comparator",
      "sort",
      "고랭",
      "정렬"
    ],
    "date": "2021-05-09T00:00:00.000Z"
  },
  {
    "id": "go/go에서의-게터-세터-메서드-getter-setter-in-go",
    "slug": "go/go에서의-게터-세터-메서드-getter-setter-in-go",
    "title": "Go에서의 게터, 세터 메서드 (Getter, Setter in Go)",
    "excerpt": "",
    "content": "캡슐화는 내부 속성값을 외부에서 직접적으로 접근하게 못 하게 하고 공개된 메서드 (ex. getter, setter)로만 접근하여 내부 값을 보호하는 역할을 한다. 즉, 내부 구현을 감추고 데이터 체크를 통해서 유효한 값만 저장하게 한다. Go에서 getter, setter 메서드를 어떻게 만드는지 아래 Person 구조체를 사용해서 작성해보자. > 는 소문자이여서 외부에서 접근할 수 없다 Setter - 는 와 같이 만들면 된다\n    - 외부에서 메서드를 호출하기 위해서 메서드 첫글자는 대문자로 한다\n- 에서는 리시버 인자는 포이터 리시버()가 필요하다\n    - 메서드 실행후 변경된 값을 반환해야 하기 때문이다\n- 에서 data validation 로직을 넣어 유효성 체크를 할 수 있다  setter메서드는 name을 인자로 받아서  구조체에 name에 값을 저장한다. 이 없는 경우에는 를 반환한다. Getter - 는 Get을 붙이지 않고 변수 이름으로만 지정한다\n    - ex.  - X\n    - ex.  - O\n    - 을 붙혀서 코드를 작성해도 동작하는데는 이상이 없지만, go convension에 의해서 get은 쓰지 않는다 > 는 값만 반환하기 때문에 포인터 리시버 인자가 필요가 없다. 하지만, 일관성을 위해 포인터 리시버로 선언하는 게 좋다. 본 포스팅에서 작성한 코드는 github에서 확인할 수 있다. 참고 - https://www.socketloop.com/tutorials/golang-implement-getters-and-setters\n- https://johngrib.github.io/wiki/golang-cheatsheet/\n- https://golang.org/doc/effectivego.html?#Getters",
    "category": "go",
    "tags": [
      "go",
      "golang",
      "setter",
      "getter",
      "encapsulation",
      "고",
      "고랭",
      "캡슐화",
      "게터",
      "세터"
    ],
    "date": "2021-01-14T00:00:00.000Z"
  },
  {
    "id": "go/go에서의-다형성-polymorphism",
    "slug": "go/go에서의-다형성-polymorphism",
    "title": "Go에서의 다형성 (Polymorphism)",
    "excerpt": "",
    "content": "다형성 (Polymorphism) 다형성은 객체지향 패러다임에서는 꼭 알아야 하는 특징 중의 하나이다. 기본 개념은 객체 메서드를 호출했을 때, 그 객체의 메서드가 다양한 구현을 할 수 있게 한다. 다형성을 설명할 때 도형이나 동물을 예제로 자주 설명한다. 본 포스팅에서는 동물을 예제로 설명한다. Go에서 다형성을 구현하는 방법 Go에서는 다형성을 인터페이스로 구현할 수 있다. Go의 인터페이스를 사용하면 다른 언어보다 더 쉽게 구현이 가능하다. 예제를 통해서 알아보자. 와 은 소리를 낼 수 있지만, 각각 다른 소리를 낸다. 공통적인 기능은  인터페이스로 정의한다. 자바의 경우에는 와 같은 키워드를 사용해서 구현하려는 인터페이스를 명시해줘야 하지만, Go에서는 각 데이터 타입에 대해서 인터페이스 메서드만 구현해주면 명시적으로 정의하지 않고도 인식이 된다. > Go에서는 덕 타이핑 ()을 지원한다. 덕 타이핑은 클래스 상속이나 명시적인 인터페이스 구현으로 타입을 구분하는 방식이 아니고, 객체가 어떤 타입에 걸맞는 메서드를 지내게 되면 해당 타입으로 간주한다.\n>\n> \"만약 어떤 새가 오리처럼 걷고, 헤엄치고, 꽥꽥거리는 소리를 낸다면 나는 그 새를 오리라 부르겠다.\"\n>\n> 오리와 같은 행동을 하면 오리로 간주한다는 의미이다. 와  구조체에 대해서  메서드를 구현한다. Unit Test에서 실행해보자. 와 를 별도 생성자로 데이터 타입을 생성한다. 인터페이스 메서드를 호출하면 다른 구조체에 맞게 다른 행동을 하는 것을 확인할 수 있다. 본 포스팅에서 작성한 코드는 github를 참고해주세요. 참고 - https://www.sohamkamani.com/golang/2019-03-29-polymorphism-without-interfaces/\n- https://golangbot.com/polymorphism/\n- https://www.geeksforgeeks.org/polymorphism-in-golang/\n- https://golangbyexample.com/runtime-polymorphism-go/\n- https://www.popit.kr/golang%EC%9C%BC%EB%A1%9C-%EB%A7%8C%EB%82%98%EB%B3%B4%EB%8A%94-duck-typing/\n- https://ko.wikipedia.org/wiki/%EB%8D%95%ED%83%80%EC%9D%B4%ED%95%91",
    "category": "go",
    "tags": [
      "go",
      "golang",
      "duck",
      "typing",
      "duck type",
      "polymorphism",
      "다형성",
      "고랭",
      "덕타입",
      "고언어"
    ],
    "date": "2021-06-06T00:00:00.000Z"
  },
  {
    "id": "go/go에서의-로그깅-logging-in-go",
    "slug": "go/go에서의-로그깅-logging-in-go",
    "title": "Go에서의 로그깅 (Logging in Go)",
    "excerpt": "",
    "content": "1. 들어가며 Go 표준 패키지 중에 log에서 로깅에 필요한 기본 메서드를 제공한다. 표준 출력 stdout, stderr외에 파일로 로그를 저장하는 방법, 그리고 로그 포맷 변경해서 출력하는 방법 등에 대해서 알아보자. 추가 설치 없이 log 패키지를 import 하면 바로 사용할 수 있다. 2. Log 패키지 사용방법 2.1 기본 Logger log 패키지에서는 여러 Logger 지원을 위해 Logger 타입을 제공한다. 2.1.1 기본 Logger로 화면에 로그찍기  함수로 새로운 Logger를 생성할 수 있다. 하지만, 아래와 같이 별도 logger 생성 없이도 바로 사용이 가능하다. log 패키지에서 std 표준 Logger를 미리 생성해두기 때문이다. 을 지정하면 날짜/시간 포맷없이 메시지만 출력된다. 여러 flag 옵션을 지정하여 log 포맷을 변경해보자. or 연산자를 이용해서 여러 옵션을 같이 지정한다. 아래와 옵션은 Prefix INFO도 추가하고 포멧은 날짜/시간외에 실행 파일도 출력하는 옵션이다. 여러 flag 옵션에 대한 설명은 Go Docs을 참고해주세요. 2.1.2 파일로 쓰기 파일로 로그를 쓰기 위해서는 먼저 저장될 파일을 로 생성하고 파일 포인터를  함수로 지정하면  사용시 파일로 쓰여지게 된다. 2.1.3 화면과 파일에 동시에 로그 찍기 동시에 로그를 파일과 화면에 출력하도록 설정할 수도 있다.  함수로 파일 포인터와 os.Stdout를 지정하여 복수 Writer를 생성하고 로 지정하면 복수 target으로 로그를 쓰게 된다. 2.2 Custom Logger  함수로 Custom Logger를 생성해서 로그를 출력해보자.  함수는 3가지 인자 값에 따라서 여러 Logger를 생성할 수 있다. - 1st : 로그 output 지정\n    - 표준콘솔출력(os.Stdout), 표준에러(os.Stderr), 파일포인터등을 지정할 수 있다\n- 2nd : 로그 Prefix\n    - 프로그래명, 카테고리등을 표시할 수 있다\n- 3rd : 로그 포멧 설정 옵션 2.2.1 Custom Logger 생성하기 아래 예제는 표준출력으로 로그를 보내는 logger를 만들어 로깅을 하는 코드이다. \"INFO: \"라는 prefix와 날짜/시간을 출력하는 Logger를 생성해서 출력한다. 2.2.2 로그 파일로 쓰기 2.1.2에서와 같이 파일 Logger를 생성해서 로그를 파일로 쓰는 예제이다. 의 첫번째 인자에 파일 포인터를 넘겨 파일로 출력한다. 2.2.3 여러 Logger 생성하는 예제 다음 예제는 여러 Logger 타입을 생성해서 출력하는 예제이다. 4. 마무리 이번 포스팅에서는 Go에서 기본으로 포함된 log 패키지를 사용해서 로깅하는 방법에 대해서 알아보았다. Go 표준 패키지 log 외에도 다른 log framework(ex. logrus)도 종종 사용한다. logrus에 대한 내용은 다음 포스팅에서 알아보자. 본 포스팅에서 작성한 코드는 github에서 확인할 수 있다. 5. 참고 - http://golang.site/go/article/114-Logging\n- https://www.ardanlabs.com/blog/2013/11/using-log-package-in-go.html\n- https://jusths.tistory.com/128\n- https://www.honeybadger.io/blog/golang-logging/\n- https://golang.org/pkg/log/#pkg-examples",
    "category": "go",
    "tags": [
      "go",
      "golang",
      "log",
      "logging",
      "logger",
      "logrus",
      "고",
      "고랭",
      "로그",
      "로깅"
    ],
    "date": "2021-01-02T00:00:00.000Z"
  },
  {
    "id": "go/go에서의-메서드-method-in-go",
    "slug": "go/go에서의-메서드-method-in-go",
    "title": "Go에서의 메서드 (Method in Go)",
    "excerpt": "",
    "content": "Go에서는 함수외에도 메서드를 제공한다. 메서드는 리시버 인자(Receiver Parameter)를 가진 함수를 말한다. 기능적으로 보면 일반 함수와 별 차이가 없고 아래 문법과 같이 func 키워드와 메서드이름 사이에 리시버 인자를 추가할 수 있다. 1. Go 메서드 예제 1.1 리시버 인자 (Receiver Parameter)가 있는 메서드 1.1.1 밸류 리시버 (Value Receiver)  타입의 값을 메서드 형식으로 반환하려면 메서드 이름 앞에 리시버 인자로  타입을 선언하면 된다.  메서드에서는  값을 반환한다. Go에서 메서드는 객체지향 프로그래밍 언어에서 지원하는 메서드처럼 dot(.)으로 메서드를 호출한다.  메서드를 호출해 자동차 색깔을 출력하였다. 1.1.2 포인터 리시버 (Pointer Receiver) 위 예제에서는 리시버 인자를 밸류 인자로 선언하였기 때문에 메서드 실행 후에는 데이터 타입 값에 반영이 안된다. 메서드 실행이후 변경된 값을 유지하려면 포인터 리시버를 사용해야 한다.  메서드에서는  값을 증가하는 메서드로 포인터 리시버로 선언해줘야 메서드 실행 이후에 변경된 값이 계속 유지가 된다.  메서드 실행 이후에도 증가된 값으로 출력되는 것을 확인할 수 있다. 1.1.3 메서드에 대한 컨벤션 메서드 정의 시 Go에서는 아래와 같은 컨벤션을 일반적으로 따른다. - 리시버 인자 정의\n    - 리시버 인자의 변수 이름은 리시버 타입 이름의 첫 글자를 사용한다\n    - 변수는 하나의 글자로만 선언한다\n- 밸류 vs 포인터 선언\n    - 값을 변경할 필요가 없는 경우에는 배류 리시버로 선언해야 하지만, 통일성을 위해서 밸류와 포인터를 섞어서 선언하지 않고 포인터로 선언한다 (참고 : Head First Go) > 밸류와 포인터를 섞어서 사용하는 곳도 있어서 팀내에 협의한 켄벤션으로 통일해서 사용하면 될 것 같다. 1.1.4 비구조체(Non-struct)가 있는 메서드 지금까지 구조체 타입에 대해서만 메서드를 정의했다. 비구조체 타입에 대한 메서드를 정의하는 것도 가능하지만, 주의가 필요하다. 리시버 타입의 정의와 메서드의 정의는 동일한 패키지 내에 있어야 한다. 위 예제에서는  타입과  메서드가 같은 패키지 레벨에 존재하지 않아서 컴파일 오류가 발생한다. 비구조체를 리시버 인자로 받으려면 를 별도 타입으로 선언하면 메서드로 선언이 가능해진다. 1.2 메서드와 포인터 역참조 (Pointer indirection/dereference) 포인터를 다루는 데 있어서 함수와 메서드간의 차이점이 존재한다. 어떤 차이점이 있는지 예제를 통해서 알아보자. - 함수에 포인터 인자로 선언한 인자는 포인터 인자만 인자로 받을 수 있다\n- 메서드의 리시버 인자의 경우에는 포인터와 밸류 인자 둘 다 받을 수 있다 area(r Rectangle) 함수의 인자는 포인터 인자로 선언되어 밸류 값을 넘기면 컴파일 오류가 발생하고 포인터 인자만을 넘길 수 있다. r.area(), r은 포인터가 아닌 밸류 값이지만, 포인터 리시버 인자의 메서드가 호출될 때 Go에서 자동으로 r.area() -> (&r).area()로 해석을 해서 실행해준다. 리시버 인자가 밸류인 경우에도 함수와 메서드의 차이점을 알아보자.  함수는 밸류 인자로 선언되어  포인터 값을 인자로 넘겨주면 컴파일 오류가 발생한다. 리시버 인자의 경우,  (&r).perimeter() 호출 시 Go는 리시버 인자는 밸류 인자로 선언되어 (r).perimeter()로 자동으로 해석해서 실행해준다. 정리 Go에서는 함수와 메서드가 존재를 한다. 메서드는 함수에 리시버 인자를 추가한 버전으로 생각하면 이해하기 쉽다. 본 포스팅에서 작성한 코드는 github에서 확인할 수 있다. 참고 - https://tour.golang.org/methods/4\n- http://golang.site/go/article/17-Go-%EB%A9%94%EC%84%9C%EB%93%9C\n- https://gobyexample.com/methods\n- https://golangbot.com/methods/\n- https://hoony-gunputer.tistory.com/entry/golang-part4Method-Pointer-and-Method-Interface-Stringers-Error-Readers\n- https://go101.org/article/method.html\n- https://www.geeksforgeeks.org/methods-in-golang/",
    "category": "go",
    "tags": [
      "golang",
      "method",
      "receiver",
      "parameter",
      "pointer",
      "메서드",
      "리시버",
      "인자",
      "포인터"
    ],
    "date": "2021-02-19T00:00:00.000Z"
  },
  {
    "id": "go/go에서의-열거형-상수-enums-in-go",
    "slug": "go/go에서의-열거형-상수-enums-in-go",
    "title": "Go에서의 열거형 상수 (Enums in Go)",
    "excerpt": "",
    "content": "1. 들어가며 Go에서는 Java에서 제공하는 Enums 타입은 존재하지 않는다. 하지만, Go에서도 를 이용해서 Enums과 같은 상수값을 쉽게 선언하여 사용할 수 있다.  키워드는  선언에서 사용할 수 있는 로 연속적인 정수 상수 0, 1, 2, ...를 나타낸다. 예제를 통해서 어떻게 사용하는지 알아보자. #2.  iota 예제 2.1 기본 예제 의 시작 값은 0이고 이후부터는 +1 증가된 값으로 선언된다. 매번  키워드를 사용하지 않고 한번만 선언하면 나머지 변수에는 연속적인 값이 선언된다. 2.2 시작 값 변경하기 시작 값도 쉽게 변경할 수 있다. 2.3 중간 값 스킵하기 2.3.1 하나의 값을 스킵하는 방법 중간 값을 스킵하려면 를 사용해서 중간 값을 스킵한다. 2.4.2 하나 이상의 값 스킵하는 방법 아래와 같이 하나 이상의 값을 스킵하고 상수 값을 선언하는 방법에 대해서 알아보자. > 1, 2, 100, 101, 500, 501 2.4.2.1 jump해야 하는 값을 직접 계산하기 스킵해야 하는 구간의 값을 직접 계산해서 선언하면 된다. 2.4.2.2 const 그룹 별로 나눠서 선언하기 jump 뛰어야 하는 구간을 직접 계산하기보다는 아래와 같이 여러 그룹으로 나눠서 선언하면 조금 더 쉽게 선언이 가능하다. 3. Enums 예제 모음 Enums 식으로 사용할 수 있는 예제들이다. 3.1 ByteSize 상수 값 선언하기 데이터 단위를 아래와 같이 선언한다. KB 단위만 선언하면 나머지 MB, GB... 단위도 쉽게 선언이 된다. 3.2 Bitwise 연산으로 플래그 값이나 옵션 확인하기 비트 연산을 통해서 여러 옵션이나 플래그를 적용하고 AND 연산으로 적용되었는지 확인하는 예제이다. 이 예제에서는 사용자에게 여러 역할을 부여하고 역할을 확인한다. 3.3 선언한 Enums iterate해보기 상수 선언시 마지막 값을 이용해서 iterate한다. 4. 마무리 Go 언어에서 Enums 형식으로 선언하는 방법에 대해서 알아보았다. 쉽게 상수 값을 선언하기 위해서 iota 키워드를 사용했고 다양한 예제도 볼 수 있었다. 본 포스팅에서 작성한 코드는 github에서 확인할 수 있다. 5. 참고  https://stackoverflow.com/questions/64178176/how-to-create-an-enum-in-golang-an-iterate-over-it\n https://stackoverflow.com/questions/57053373/how-to-skip-a-lot-of-values-when-define-const-variable-with-iota/57053431#57053431\n https://yourbasic.org/golang/iota/\n https://golang.org/ref/spec#Iota\n https://www.popit.kr/%EC%A2%8C%EC%B6%A9%EC%9A%B0%EB%8F%8C-%EA%B0%9C%EB%B0%9C%EA%B8%B0-golang-%EC%97%90%EC%84%9C-enum-%EC%9E%90%EB%A3%8C%ED%98%95-%EC%82%AC%EC%9A%A9%ED%9B%84%EA%B8%B0/",
    "category": "go",
    "tags": [
      "go",
      "golang",
      "enums",
      "iota",
      "열거형",
      "상수",
      "고",
      "고랭"
    ],
    "date": "2020-12-20T00:00:00.000Z"
  },
  {
    "id": "go/jq-명령어-json-처리기-사용법",
    "slug": "go/jq-명령어-json-처리기-사용법",
    "title": "jq - 명령어 JSON 처리기 사용법",
    "excerpt": "",
    "content": "는 자주 사용하지는 않지만, 필요할 때는 용이하게 사용하는 경우가 종종 있어서 기록상 블로그에 남겨둡니다. jq 사용법 모음 1. jq로 array에서 특정 필드로 매칭되는 필드 값을 추출하려면? 아래와 같은 json array에서 특정 필드, 가 매칭되는 item에서 원하는 필드를 추출하면  쿼리는 어떻게 작성하면 될까요? 의  syntax는 bool 표현 식이 참이 되는 값을 선택하는 함수여서 아래와 같이 쿼리를 작성하면 id가 매칭되는 item이 선택되고 보고자 하는 필드를 선택해서 출력하면 된다. 2. JSON 문자열에서 배열의 수를 출력하려면? 배열의 수는  함수로 얻을 수 있다. 참고 - https://phpfog.com/count-json-array-elements-with-jq/ 참고 - https://stackoverflow.com/questions/51184524/get-parent-element-id-while-parsing-json-data-with-jq\n- https://stackoverflow.com/questions/18592173/select-objects-based-on-value-of-variable-in-object-using-jq\n- https://stedolan.github.io/jq/manual/#select(booleanexpression)\n- https://www.44bits.io/ko/post/clijsonprocessorjqbasicsyntax",
    "category": "go",
    "tags": [
      "jq",
      "gojq",
      "json"
    ],
    "date": "2023-01-27T00:00:00.000Z"
  },
  {
    "id": "go/jwks-json-web-key-set이란",
    "slug": "go/jwks-json-web-key-set이란",
    "title": "JWKS(JSON Web Key Set)이란?",
    "excerpt": "",
    "content": "1. 개요 (JSON Web Token)는 웹 애플리케이션에서 인증 및 정보 교환을 안전하게 수행하기 위해 사용된다. 는 자체적으로 정보를 담고 있으며, 일반적으로 사용자 식별 및 권한 정보를 포함한다. 는 보통 사용자 인증, 사용자 정보에 사용되고 웹 애플리케이션에서 안전한 인증을 위해 사용할 수 있는 도구이다. 의 핵심은 서명을 통해 토큰의 무결성을 보장하는 것이다. JWT의 서명에는 대칭 암호화() 와 비대칭 암호화(, EC 방식을 사용할 수 있다. - 대칭 암호화(, 예: ): 같은 비밀 키(secret key)를 사용하여 서명과 검증을 수행한다\n- 비대칭 암호화(, , 예: RS256, ES256): 개인 키로 서명하고 공개 키로 검증한다 비대칭 암호화 방식을 사용할 경우, 클라이언트가 의 서명을 검증할 수 있도록  Endpoint를 제공하여 공개 키를 공유하기도 한다. 이때 사용되는 것이 (JSON Web Key Set)이다. 는  형식으로 공개 키를 제공하는 표준 규격으로, 클라이언트는 이를 통해 의 서명을 검증할 수 있다. 이 글에서는 의 개념, 구조, 구현 방법, 그리고 실전 사용 사례에 대해 살펴본다. 2. 에 대해서 알아보자 2.1  vs  -   (JSON Web Key)\n  - JSON 형식으로 표현된 단일 키(비대칭 키 쌍의 공개 키 또는 대칭 키)이다\n  - JWK는 키의 식별자, 키 타입, 알고리즘 등과 같은 메타데이터를 포함한다 -  (JSON Web Key Set)\n  - JWKS는 JWK를 여러 개 포함한 키의 집합이다\n  - 보통 HTTPS를 통해 노출되는 엔드포인트에서 제공되며, 클라이언트는 이를 통해 JWT를 검증한다. 2.2  구조 > 예제. JWKS 응답 구조 는  배열 형태로 의 집합을 나타낸다. 각  객체는 다음과 같은 필드를 가질 수 있다. -  (key type)\n  - 이 파라미터는 암호 알고리즘을 식별할 때 사용한다. ,  같은 값을 갖는다\n  - \n-  (public key use)\n  - 이 파라미터는 공개 키(public key) 사용 목적을 식별한다. (signature), (encryption) 같은 값을 갖는다\n  - \n-  (algorithm)\n  - 이 파라미터는 키와 함께 사용되는 알고리즘 정보를 식별할 때 사용한\n  - \n-  (key id)\n  - JWKS 객체에 담긴 JWK 객체들은 자신을 구분할 수 있도록 서로 다른  값을 가져야 한다\n  - \n- 추가 속성\n  - 과 에 따라서 추가로 필요한 속성 값이 다르다\n  - RSA key type의 경우에는 (public key modulus), (exponent)가 추가로 필요하고 Base64URL로 인코딩된 값이다\n  - EC key type인 경우에는 (curve type), , (coordinates)\n  -  2.3 JWKS Endpoint 는 보통 를 통해 제공되며 클라이언트는 이를 활용하여 를 검증한다. 일반적으로  형식은 다음과 같다. > JWKS API 주소 포멧\n>\n>  2.3.1 RSA 공개 키, 개인 키 생성하는 방법 여기서는 비대칭 암호화로 RSA를 사용한다. 공개 키와 개인 키를 생성하기 위해  명령어를 사용한다. - 파일명: \n- 키 크기: 2048비트 (보안 요구사항에 따라 4096비트로 증가 가능) 개인 키가 생성이 되었고 개인 키에서 다음 명령어로 공개 키를 추출한다. - 파일명: \n- 개인 키로부터 공개 키를 추출하여 파일에 저장한다 2.3.2 JWKS 생성하기 JWKS 생성은 JWKSet (MicahParks) 라이브러리를 사용하면 쉽게 구현이 가능하다. 실제로 API endpoint로 구현은 하지 안호 간단한 예제로 unit test 형식으로 작성했다. 파싱된 공개 키로부터  함수를 사용하여 를 생성한다. 생성된 를 메모리 내 JWK cache에 저장하고  하여 데이터를 반환하는 코드이다.  생성한 데이터를 보면 위와 같이 나온다. 2.3.3 jwset.com 소개  라이브러리를 개발한 개발자가 추가로 만든 사이트이기도 하다. 웹 상에서 를 직접 생성하고 테스트해볼 수 있는 사이트이다. - JWK Generator\n- JWK Inspector 3. FAQ 3.1 공개 키와 개인 키는 각각 언제 어떻게 사용되나? - 서명(Signature)\n  - 서명을 생성하려면 개인 키를 사용한다\n  - 서명을 검증하려면 공개 키를 사용한다\n  - ex. 도 개인 키로 암호화하고 공개 키로 를 검증할 수 있다\n- 암호화(Encryption)\n  - 데이터를 보낼 때는 공개 키로 암호화한다\n  - 데이터를 복호화할 때는 개인 키를 사용한다 4. 마무리 와 는 를 활용한 인증 시스템에서 필수적인 구성 요소이다. 특히 를 통해 키 관리와 서명 검증을 효율적으로 처리할 수 있다. 본 글이 와 의 이해와 구현에 도움이 되길 바란다. 5. 참고\n- Understanding JWKS: JSON Web Key Set Explained",
    "category": "go",
    "tags": [
      "golang",
      "JWKS",
      "JWK",
      "JWT",
      "JSON Web Key Set",
      "RSA",
      "EC",
      "HMAC",
      "비대칭 암호화",
      "jwkset"
    ],
    "date": "2025-02-17T00:00:00.000Z"
  },
  {
    "id": "go/타입-단언-type-assertion",
    "slug": "go/타입-단언-type-assertion",
    "title": "타입 단언 (Type Assertion)",
    "excerpt": "",
    "content": "Go는 타입 단언(Type assertion)을 통해서 인터페이스  변수가 타입 인지를 확인하고 인터페이스의 실제 구현된 타입의 값을 가져올 수 있다. 타입 단언 문법으로 인터페이스  변수가 타입 인지 확인하고 실제 값을  변수에 값을 할당한다. 타입 단언 사용 시 주의해야 하는 사항은 다음과 같다. -  변수는 인터페이스이여야 한다\n    - 인터페이스 가 실제 값을 가지고 있지 않은 경우에는 panic이 발생한다\n- 는  인터페이스를 구현한 타입인지를 확인하다\n    - 타입 T의  인터페이스의 메서드를 구현하고 있지 않으면 panic이 발생한다 타입 단어 예제에서 사용할 인터페이스와 메서드이다.  인터페이스의 메서드는  구조체 데이터 타입에 맞게 구현했다. 1.타입 단언 예제 1.1 정상적으로 타입 단언하는 경우 1.1.1 실제 구조체로 타입 단언하는 경우 인터페이스  변수는  구조체 값을 보유하고 있다.  타입 단언으로  인터페이스값에서 실제  구조체 값을 얻어와 해당 데이터 타입의 메서드를 실행했다. 1.1.2 다른 인터페이스로 타입 단언하는 경우  인터페이스에서 다른 인터페이스인 으로 타입 단언을 하여  인터페이스의 메서드를 실행할 수 있었다. 1.2 타입 단언시 panic이 발생하는 경우 타입 단언시 발생할 수 있는 에러에 대해서 알아보자. 1.2.1 인터페이스가 타입 T의 동적 값을 가지고 있지 않는 경우 타입 단언 시  타입 가 인터페이스 메서드를 구현하고 있지 않으면, 인터페이스 가 타입 의 동적 값을 보유할 수 없기 때문에  컴파일 에러가 발생한다. 1.2.2 인터페이스가 타입 T의 실제 값을 가지고 있지 않는 경우 타입 는 구현된 메서드가 있지만, 인터페이스 가 실제 값을 가지고 있지 않으면 Go에서 런타임시 panic이 발생한다. 런타임시 panic 발생을 피하려면 타입 단언 시 ok 반환 값을 추가로 받으면 된다.  변수를 통해서 타입 가 인터페이스 를 구현했는지, 가 실제 타입 를 갖고 있는지 확인할 수 있다. 모두 만족하면 는 를 반환하고 아닌 경우에는 를 반환한다. 1.2.3 다른 인터페이스가 타입 T를 구현하지 않고 있는 경우  구조체는  인터페이스를 구현하지 않았기 때문에  타입 단언시 panic이 발생한다. 정리 타입 단언은 인터페이스 변수에서 실제 타입 값을 얻어와 해당 타입에 맞는 메서드를 실행할 때 사용된다. 본 포스팅에서 작성한 코드는 github에서 확인할 수 있다. 참고 - Type assertions\n    - https://yourbasic.org/golang/type-assertion-switch/\n    - https://feel5ny.github.io/2017/11/18/Typescript05/\n    - https://pronist.tistory.com/92\n    - https://tour.golang.org/methods/15\n    - https://www.geeksforgeeks.org/type-assertions-in-golang/\n    - https://www.sohamkamani.com/golang/type-assertions-vs-type-conversions/\n    - https://hoonyland.medium.com/%EB%B2%88%EC%97%AD-interfaces-in-go-d5ebece9a7ea\n    - https://velog.io/@kykevin/Go%EC%9D%98-Interface\n- Type conversions\n    - https://tour.golang.org/basics/13",
    "category": "go",
    "tags": [
      "go",
      "golang",
      "type",
      "assertion",
      "타입",
      "형단언",
      "타입단언",
      "단언",
      "고",
      "고랭"
    ],
    "date": "2021-01-16T00:00:00.000Z"
  },
  {
    "id": "go/타입-변환-type-conversion",
    "slug": "go/타입-변환-type-conversion",
    "title": "타입 변환 (Type Conversion)",
    "excerpt": "",
    "content": "타입 변환은 데이터 타입을 변경하는 것이다. Java에서는 명시적 타입 변환(explicit type conversion)과 암시적 타입 변환(implicit type conversion) 둘 다 지원하지만, Go에서는 명시적인 타입 변환만을 지원한다. 타입 변환 문법은 아래와 같이 val 값을 타입 T로 변환한다. 예제에서는 int 형을 float64와 uint32 형으로 변환해주고 있다. 본 포스팅에서 작성한 코드는 github에서 확인할 수 있다. 참고 - Go 형변환\n    - https://tour.golang.org/basics/13\n    - https://www.geeksforgeeks.org/type-casting-or-type-conversion-in-golang/\n- Java 형변환\n    - https://opentutorials.org/course/1223/5330",
    "category": "go",
    "tags": [
      "golang",
      "type",
      "conversion",
      "cast",
      "casting",
      "형변환",
      "타입변환",
      "타입",
      "변환",
      "명시적",
      "캐스팅"
    ],
    "date": "2021-01-16T00:00:00.000Z"
  },
  {
    "id": "go/타입-스위치-type-switch",
    "slug": "go/타입-스위치-type-switch",
    "title": "타입 스위치 (Type switch)",
    "excerpt": "",
    "content": "티입 스위치는 형 단언을 실행하여 해당 변수의 타입이 스위치 문의 조건에 일치하는 블럭을 실행한다. 타입 스위치 선언문은 형 단언 와 같은 구문을 가진다. 그러나 T는  키워드로 대체된다. 인자로 넘겨진  변수의 실제 타입에 따라서 각 케이스 블럭 구문이 실행된다. 여러 인자 값에 따라서 스위치 구문이 실행된다. 본 포스팅에서 작성한 코드는 github에서 확인할 수 있다. 참고  https://yourbasic.org/golang/type-assertion-switch/\n https://riptutorial.com/go/example/14736/type-switch\n https://www.geeksforgeeks.org/type-switches-in-golang/\n https://tour.golang.org/methods/16",
    "category": "go",
    "tags": [
      "go",
      "golang",
      "type",
      "switch",
      "형스위치",
      "타입스위치",
      "고",
      "고랭"
    ],
    "date": "2021-01-16T00:00:00.000Z"
  },
  {
    "id": "java/Keycloak으로-자체-인증-서버-구축",
    "slug": "java/Keycloak으로-자체-인증-서버-구축",
    "title": "Keycloak으로 자체 인증 서버 구축",
    "excerpt": "",
    "content": "1. 개요 회사 내부 서비스에서는 NCP(Naver Cloud Platform) 인증 서버를 사용해 왔지만, 다른 해외 사이트에서는 NCP 인증 서버를 사용할 수 없는 환경에 적용을 해야 해서 인증 로직을 구현하거나, 별도의 솔루션을 도입해야 하는 상황이 생겼었다. 이 과정에서 여러 IAM 솔루션중에 오픈소스 기반으로 널리 사용되고 있는 Keycloak 을 도입하게 되었다. 이 과정에서 여러 IAM 솔루션을 검토하다가, 오픈소스 기반으로 널리 사용되고 있는 Keycloak을 접하게 되었고, 실제로 적용해보기 위해 스터디를 진행했다. 이번 글은 그 과정에서 학습한 내용을 정리한 것으로, Keycloak이 무엇인지, 어떤 기능을 제공하며, 어떻게 구성되어 있는지를 살펴보려 한다. Keycloak 이란? Keycloak은 Red Hat에서 주도하는 오픈소스 IAM(Identity and Access Management) 솔루션으로, 사용자 인증과 권한 관리를 통합적으로 처리할 수 있는 플랫폼이다. OAuth 2.0, OpenID Connect, SAML 같은 표준 프로토콜을 지원하며, 로그인, 로그아웃, 세션 관리와 같은 복잡한 인증 로직을 애플리케이션에서 직접 구현하지 않고 Keycloak에 위임할 수 있다.  > 왜 사용해야 하나? 최근 애플리케이션 환경은 다수의 마이크로서비스, 웹/모바일 클라이언트, 외부 API 연동 등으로 점점 복잡해지고 있다. 이때 각 서비스마다 별도의 로그인 기능을 구현하면 보안 및 유지보수 비용이 커지게 된다. Keycloak 을 사용하면 중앙집중식 인증 관리, Single Sign-On(SSO), 소셜 로그인 연동이 가능하여 보안 강화와 개발 효율성을 동시에 얻을 수 있다.  주요 기능 - Single Sign-On (SSO): 한 번 로그인으로 여러 애플리케이션 접근\n- 소셜 로그인: Google, Facebook, GitHub 등 연동 가능\n- 다양한 인증 프로토콜 지원: OAuth2, OpenID Connect, SAML\n- User Federation: 외부 사용자 저장소(LDAP, Kerberos)와 연결하여 사용자 인증, 동기화를 제공\n- Admin Console / REST API: 직관적인 관리 UI와 API를 통한 자동화\n- MFA(다단계 인증): OTP, 이메일, SMS 등 다양한 인증 수단 제공\n- 사용자/그룹/역할 관리: 세분화된 권한 제어 가능\n- 다양한 DB 지원: mysql, msqql, oracle 등 Keycloak 구성 요소 Keycloak은 인증/인가를 처리하기 위해 여러 핵심 구성 요소를 제공한다.  - Realm: 사용자, 클라이언트, 정책 등을 묶어 관리하는 격리된 공간/영역\n- Client: Keycloak과 연동되는 애플리케이션(웹, 모바일, API 서버 등)\n- User / Group / Role: 사용자와 권한 구조 정의\n- Authentication Flow: 로그인 과정에서 어떤 단계를 거칠지 정의 (예: 아이디/비밀번호 + OTP)\n- Protocol Mapper: 토큰에 사용자 속성/권한 등을 매핑 2. 로컬환경에서 Keycloak 인증 서버 구축해보기 로컬환경에서 도커를 활용해 몇 분 안에 Keycloak 인증 서버를 띄워서 어드민 콘솔에 접속해봅니다 2.1 Keycloak 설치 Keycloak 서버를 구동하는 방법은 여러 가지가 있다. Keycloak 소스 코드를 다운로드 받아 직접 구동할 수도 있지만, 여기서는 가장 간단한 방법으로 도커로 실행한다.  2.1.1 Keycloak 도커 실행 > 옵션에 대한 설명 - , :\n  - Keycloak 관리 콘솔에 로그인할 때 사용할 초기 관리자 계정/비밀번호 지정\n- :\n  - 개발 모드(dev mode)로 Keycloak을 실행\n  - TLS/캐시/클러스터링 같은 운영 환경 설정은 생략되어 빠르게 시작 가능\n  - 실습/로컬 테스트 시 가장 적합 참고 - https://www.keycloak.org/getting-started/getting-started-docker 2.1.2 어드민 컨솔에 로그인 http://localhost:8080 에 접속해  /  으로 로그인한다. admin으로 로그인하면 Keycloak 을 설정할 수 있는 어드민 페이지가 로딩된다. 다음 섹션에서 인증 서버로 사용하기 위해 기본적으로 설정해야 하는 부분을 다룬다.  2.2 Keycloak 설정 Keycloak을 인증 서버로 활용하려면 최소한의 기본 설정이 필요하다. 여기서 말하는 기본 설정은 내 애플리케이션에서 Keycloak 로그인 화면으로 리디렉션 → 로그인 성공 후 토큰 발급 까지 가능한 상태를 만드는 것을 목표로 한다. 1. Realm 생성 - Realm은 Keycloak의 인증 단위이다\n  - 사용자, 클라이언트, 정책이 모두 Realm 단위로 관리되므로 프로젝트별, 환경별로 Realm을 분리하는 것이 일반적이다\n- Manage realms → Create realm 버튼 클릭해서 myrealm 같은 이름으로 생성한다 2. Client 등록 - Client는 과 연동되는 애플리케이션을 의미한다\n  - 웹앱, API 서버 등 을 통해 인증할 대상이 여기에 해당한다\n- Clients → Create client 버튼을 클릭해서 아래 값으로 클라이언트를 생성한다\n  - Client ID: \n  - Authentication flow:  (기본 값으로 설정 되어 있음)\n  - Access settings\n    - Valid redirect URIs: \n    - Valid post logout redirect URIs: \n    - Web origins:  Authentication Flow: 기본적으로 Standard Flow 가 활성화되어 있다. 이는 OAuth 2.0의 Authorization Code Flow 에 해당하며, 가장 안전하고 널리 사용되는 방식이다. >  옵션에 대한 설명 - Client authentication\n  -  클라이언트가 토큰을 요청할 때 자격 증명(clientid + clientsecret) 을 반드시 사용하도록 할지 여부이다\n     -  : 클라이언트가 공개(Public) 앱일 때 (SPA 등)\n     -  : 클라이언트가 Confidential 앱(백엔드 서버)일 때 - Authorization\n  - Keycloak의 Authorization Services(정책 기반 접근 제어, UMA 2.0 지원)를 활성화할지 여부이다. 주로 세밀한 권한 제어가 필요한 경우 사용한다 - Authentication flow\n  - 클라이언트가 어떤 OAuth2/OIDC 플로우를 사용할지 정의한다. (아래 표 참고) - PKCE Method\n  - (Proof Key for Code Exchange)를 어떤 방식으로 사용할지 선택한다\n    - 권장:  (SHA-256 해시 기반)   - Public Client (예: SPA, 모바일 앱)에서는 필수적으로 PKCE 사용을 권장한다 Authentication flow 옵션 정리 | 옵션                                     | OAuth2 대응                                 | 설명                                                         | 사용 사례                       |\n| ---------------------------------------- | ------------------------------------------- | ------------------------------------------------------------ | ------------------------------- |\n| Standard flow                        | Authorization Code Flow                     | 가장 안전하고 권장되는 방식. Authorization Code → Token 교환 과정에서 서버가 관여 | 웹 앱(React, Vue) + 백엔드 서버 |\n| Implicit flow                        | Implicit Flow (Deprecated)                  | 브라우저에서 토큰을 직접 받는 방식. 토큰 노출 위험으로 최신 앱에서는 권장되지 않음 | 구형 SPA, 테스트 용도           |\n| Direct access grants                 | Resource Owner Password Credentials         | 사용자가 앱에 직접 ID/PW 입력 → 앱이 토큰 요청. 보안상 위험  | CLI 툴, 레거시 환경             |\n| Service accounts roles               | Client Credentials Flow                     | 사용자 없이 클라이언트(앱) 자체가 인증. 주로 B2B API 통신에서 사용 | 마이크로서비스 간 통신          |\n| Standard Token Exchange              | Token Exchange (RFC 8693)                   | 받은 토큰을 다른 토큰으로 교환. 마이크로서비스 간 권한 위임 시 유용 | 게이트웨이/프록시 토큰 변환     |\n| OAuth 2.0 Device Authorization Grant | Device Code Flow                            | 디바이스(스마트 TV 등)에서 브라우저 없는 환경에서 인증 시 사용 | IoT 기기, TV 앱                 |\n| OIDC CIBA Grant                      | Client Initiated Backchannel Authentication | 백채널로 사용자 인증을 요청하는 플로우. 금융/보안 특화 환경  | Open Banking, 고보안 인증       | Access settings에서 반드시 설정해야 할 값이다.  - Valid Redirect URIs\n  - 로그인 후",
    "category": "java",
    "tags": [
      "keycloak",
      "java",
      "redhat",
      "인증 서버",
      "oauth",
      "oauth2",
      "oidc",
      "openid connect",
      "saml",
      "sso",
      "mfa",
      "user federation",
      "realm",
      "client",
      "admin console",
      "ncp",
      "iam",
      "identity and access management"
    ],
    "date": "2025-09-08T00:00:00.000Z"
  },
  {
    "id": "java/jackson에서-infinite-recursion-이슈-해결방법",
    "slug": "java/jackson에서-infinite-recursion-이슈-해결방법",
    "title": "Jackson에서 Infinite Recursion 이슈 해결방법",
    "excerpt": "",
    "content": "1. 들어가며 Jackson에서 양방향 관계 (Bidirectional Relationship)로 맺어진 객체는 무한 재귀가 발생하는 문제가 있습니다. 구체적인 예를 들어 어떤 상황에서 발생하는 지 살펴보고 어떤 방법으로 해결 가능한지도 알아보겠습니다. 1.1 무한 재귀 Customer와 Order 두 객체는 서로 순환 참조 (Circular Reference)를 하고 있습니다. Customer 객체가 Order 객체를 가지고 있고 Order 객체가 Customer 객체를 가지고 있는 경우입니다. Customer 객체를 Jackson에서 직렬화(serialization) 할 경우 JsonMappingException 예외가 발생하게 됩니다. JsonMappingException 예외 오류 메시지입니다. 2. 개발 환경 이 포스팅에서 작성한 코드는 아래 github를 참조해주세요.  OS : Mac OS\n IDE: Intellij\n Java : JDK 12\n Source code : github\n Software management tool : Maven 3. 해결책 3.1 @JsonManagedReference와 @JsonBackReference 어노테이션 사용 Jackson 2.0 버전 이전에 순환 참조를 해결하기 위해서 사용했던 어노테이션입니다.  @JsonManagedReference\n     양방향 관계에서 정방향 참조할 변수에 어노테이션을 추가하면 직렬화에 포함된다\n @JsonBackReference\n     양방향 관계에서 역방향 참조로 어노테이션을 추가하면 직렬화에서 제외된다 Customer 객체에서 Order 객체에 @JsonManagedReference를 추가하고 Order에서는 Customer 객체에 @JsonBackReference 어노테이션을 추가하여 직렬화에서 Customer 객체를 제외 시켰습니다. 실행 화면 @JsonBackReference 어노테이션 선언으로 Order 객체에서 Customer 객체에 대한 정보는 빠져 있어요. 3.2 @JsonIdentityInfo - 추천방식 Jackson 2.0 이후부터 새롭게 추가된 어노테이션입니다. @JsonIdentityInfo 어노테이션을 추가해서 직렬화에 포함 시킬 속성 값을 ‘property’ 속성에 지정합니다.  @JsonIdentityInfo(generator = ObjectIdGenerators.PropertyGenerator.class)\n     generator = ObjectIdGenerators.PropertyGenerator.class 클래스는 순환 참조시 사용할 Id를 생성하는데 사용되는 클래스이다\n @JsonIdentityInfo(property = “id\")\n     property 속성은 해당 클래스의 속성 이름을 지정한다\n     예제에서는 id는 Customer#id를 가리키고 직렬화/역직렬화할 때 Order#customer의 역참조로 사용된다 @JsonIdentityReference 어노테이션은 @JsonIdentityInfo과 같이 자주 사용되는데, 이것에 대한 설명은 다음 섹션에서 알아볼께요. 실행 화면 3.2.1 @JsonIdentityReference 란? @JsonIdentityReference 어노테이션은 객체를 직렬화할 때 전체 POJO로 처리하는 대신 단순히 객체 ID로만 노출되도록 해줍니다. @JsonIdentityReference 어노테이션을 사용하지 않을 경우에는 아래와 같이 객체의 전체 내용을 직렬화 합니다. 실행 화면 하지만, @JsonIdentityReference 어노테이션을 사용하는 경우에는 직렬화시 객체 ID로만 직렬화 합니다. 실행 화면 3.3 @JsonIgnore 어노테이션 사용 마지막으로 제일 간단하게 해결할 수 있는 방법은 직렬화 할때 순환 참조 되는 속성에 @JsonIgnore 어노테이션을 추가하여 직렬화에서 제외시키는 방법입니다. 실행 화면 Unit Test을 실행해보면 customer 정보는 Order 객체에서 제외된 것을 확인할 수 있습니다. 4. 정리 웹 어플리케이션에서 데이터 전송시 일반적으로 JSON 포멧을 사용하고 있고 서버단에서를 JSON 처리를 위해 Jackson 라이브러리를 많이 사용하고 있습니다. 객체 간에 서로 참조하여 순환 참조가 발생하게 되면 무한 재귀로 StackOverflowError가 발생합니다. 이를 해결 하기 위해 3가지 방법을 알아보았습니다. 5. 참고  Jackson Infinite Recursion\n     https://www.baeldung.com/jackson-bidirectional-relationships-and-infinite-recursion\n     https://www.logicbig.com/tutorials/misc/jackson/json-identity-reference.html\n     http://www.appsdeveloperblog.com/infinite-recursion-in-objects-with-bidirectional-relationships/\n     http://springquay.blogspot.com/2016/01/new-approach-to-solve-json-recursive.html\n     https://www.logicbig.com/tutorials/misc/jackson/json-identity-info-annotation.html\n @JsonIdentityReference란\n     https://www.baeldung.com/jackson-advanced-annotations",
    "category": "java",
    "tags": [
      "TAG Circular",
      "infinite recursion",
      "Jackson",
      "jsonignore"
    ],
    "date": "2019-10-12T00:00:00.000Z"
  },
  {
    "id": "java/java-jayway-jsonpath-사용법",
    "slug": "java/java-jayway-jsonpath-사용법",
    "title": "Java Jayway JsonPath 사용법",
    "excerpt": "",
    "content": "1. 들어가며 Jayway JsonPath는 Stefan Goessner의 JsonPath 구현을 자바로 포팅한 라이브러리입니다. XML의 가장 큰 장점은 XPath(XML Path Language)로 XML 문서에서 원하는 부분을 바로 추출 할 수 있다는 점입니다. w3school 예제 위 XML 예제에서 bookstore의 첫 번째 책 요소를 추출하는 XPath 표현은 아래와 같습니다. - \\bookstorebook[1] : 첫 번째 책 요소를 추출한다\n- \\bookstorebook[last()] : 여러 책중 맨 마지막 책을 추출한다 2. 개발 환경 및 Maven 의존성 설정 사용한 환경은 아래와 같습니다. 여기서 작성한 소스는 아래 github 링크를 참고해주세요. - OS : Mac OS\n- IDE: Intellij\n- Java : JDK 1.8\n- Source code : github\n- Dependency management tool : Maven Java Jayway JsonPath를 사용하기 위해서는 아래 Maven 의존성을 추가해야 합니다. 현재 최신 버전은 2.4.0 (2017/7/5)입니다. JSON 샘플 파일은 Json Generator 에서 가져왔고 작성한 소스 코드는 실제 Jaway JsonPath 소스를 많이 참조해서 작성했습니다. 2.1 Jayway JsonPath Evaluator JsonPath 표현식에 아직 익숙하지 않다면, JsonPath Online Evaluator 에 접속해서 표현식을 테스트해보세요. 3. Jayway JsonPath 사용법 JsonPath의 표기법과 대표적인 연산자를 알아보고 예제를 통해서 어떻게 데이터에 접근하여 가져올 수 있는지 알아보도록하겠습니다. Jayway JsonPath의 연산자, 함수, 필터에 대한 전체 목록은 JsonPath Github를 참조해주세요. 3.1 JsonPath 표기법 JsonPath는 2가지 표기법을 사용할 수 있습니다. Dot과 bracket 표현식이 있습니다. - dot 표현식\n    - \\$.store.book[0].title\n- bracket 표현식\n    - \\$[’store’][‘book’][0][’title’] 3.2 JsonPath 대표적인 연산자 (Operator) 대표적으로 많이 사용하는 연산자입니다. | 연산자 | 설명 |\n| -------- | ------- | \n| \\$ | 루트 노드로 모든 Path 표현식은 이 기호로 시작된다. |\n| @ | 처리되고 있는 현재 노드를 나타내고 필터 조건자에서 사용된다.|  \n| \\ | 와일드카드로 모든 요소와 매칭이 된다 |\n| . | Dot 표현식의 자식노드 |\n| [start:end] | 배열 slice 연산자 |\n| [?(\\<expression\\>)] | 필터 표현식으로 필터 조건자가 참인 경우에 매칭되는 모든 요소를 만을 처리한다 ex. book[?(@.price == 49.99)] | 3.3 JsonPath 함수 및 필터 JsonPath 함수는 min(), max(), avg(), length() 등을 제공하고 표현식 맨 마지막에 붙여서 실행할 수 있습니다. - \\$.length() : 요소의 길이를 반환한다. 배열인 경우에는 배열 크기를 반환한다\n- \\$.range.avg() : 요소 range 배열의 평균 값을 계산한다 JsonPath에서 필터도 제공합니다. 필터 [?(\\<expression\\>)] 표현 식을 가지며 <expression>에는 논리 연산자(ex. ==, <, >)와 기타연산자(ex. in, size, empty)로 true, false 값을 반환하는 표현 식이 들어갑니다. @는 현재 처리되는 요소를 나타냅니다. - \\$[?(@.age == 23 )] : age가 23인 데이터만 반환한다\n- \\$[?(@.name == ‘Frank’)] : 이름인 Frank인 데이터만 반환한다 3.4 JsonPath 표현식 예제 | JsonPath 표현식 | 결과 및 설명 |\n| ------------- | --------- |\n| $.. | 전체 요소 (.. 딥 스캔) |\n| $[?('pariatur' in @['tags'])] | tags에 pariatur가 있는 모든 사람들 |\n| $[?(@.age == 26 )] | age가 26인 모든 사람들 |\n| $[0][‘balance’] | 첫번째 사람의 balance |\n| $[]['age'] | 모든 사람들의 나이 |\n| $..[’name’][‘first] | 모든 사람들의 이름 | 3.5 Java JsonPath 예제 Jayway JsonPath로 원하는 데이터를 추출하려면 parse()와 read()를 사용하면 됩니다. 유닛 테스트로 작성된 여러 버전을 보면 사용법을 쉽게 이해할 수 있습니다. - static parse() : 여러 입력 타입(ex. String, InputStream, File)에 따라서 JSON을 읽어드리는 정적 메서드이다.\n- read() : XPath 표현식을 읽고 해당 데이터를 추출한다\n    - <T> T read(String path, Predicate... filters) 3.5.1 Id로 검색하기 배열 중에 \\id 값이 ‘5c2c3278acd492387a5223d7'인 데이터를 추출하는 예제입니다. 필터를 사용하여 Id가 같은 데이터만 얻어와서 Object 객체로 반환합니다. JsonPath Output 결과 3.5.2 Filter API를 사용하기 위와 같은 예제이고 Jayway에서 제공하는 Filter API를 사용하여 작성하였습니다. Filter API를 사용하려면, 메서드이름을 익히고 익숙해져야 하므로 그냥 JsonPath 표현식 사용을 추천합니다. 3.5.3 Tags에 특정 값이 있는 모두 사람 찾기 스캔하는 @[’tags’]에 ‘pariatur’ 값이 있는 모든 사람을 찾는 예제입니다. 필터 조건가가 있는 경우에는 결과가 여러 개일 수 있으므로 List로 반환합니다. JsonPath Output 결과 3.5.4 JsonPath 쿼리로 얻은 결과 자바 객체와 자동 매핑하기 지금까지 JsonPath로 쿼리한 결과를 Object로 저장하였지만, 실제 클래스 객체로 결과를 매핑받아 볼 수 있습니다. read() 메서드에 targetType으로 객체(ex. Person)를 넘겨주면 자동으로 캐스팅되어 타입 객체(ex. Person)를 반환합니다. 3.5.5 JsonPath 함수 사용 첫번째 사람에서 range 속성의 평균 값을 계산하는 예제입니다. JsonPath Output 결과 3.5.6 모든 사람의 총 계좌 잔고 계산하기 이번에도 함수를 사용하였습니다. \\$.length()를 사용해서 총 사람의 수를 얻은 다음, 각 사람들의 balance 속성의 값을 얻어 총 합을 구하는 예제입니다. 3.5.7 제일 어린 사람 찾기 마지막 예제는 제일 어린 사람 찾기입니다. #1에서는 \\$[\\][‘age] 표현식으로 모든 사람의 나이를 List로 결과를 담습니다. #2에서 제일 최소 나이를 구한 다음, #3에서는 구한 최소 나이의 값이 매칭되는 사람을 얻어 결과를 저장합니다. 지금까지 예제를 통해서 Jayway JsonPath 사용법을 알아봤습니다. JSON 데이터를 사용할 때 Gson이나 Jackson 라이브러리를 많이 사용합니다. 이런 라이브러리를 사용해도 원하는 중간값을 얻어올 수 있지만, 빠르고 쉽게 작성하기는 좀 어려움이 있습니다. JsonPath는 이런 부분을 보완해줍니다. 그래서 빠르고 쉽게 체크할 때 많이 사용되어 유닛 테스트를 작성할 때 많이 사용되고 있습니다. 4. 참고 - JSONPath\n    - https://goessner.net/articles/JsonPath/\n- Jayway JsonPath\n    - https://github.com/json-path/JsonPath\n    - https://www.baeldung.com/guide-to-jayway-jsonpath\n    - https://www.baeldung.com/jsonpath-count\n    - https://www.pluralsight.com/blog/tutorials/introduction-to-jsonpath",
    "category": "java",
    "tags": [
      "jayway",
      "java",
      "jsonpath",
      "xpath",
      "json-path",
      "자바"
    ],
    "date": "2019-01-19T00:00:00.000Z"
  },
  {
    "id": "java/junit-rules이란",
    "slug": "java/junit-rules이란",
    "title": "JUnit Rules이란",
    "excerpt": "",
    "content": "1. 들어가며 JUnit Rules은 테스트 케이스를 실행하기 전후에 추가 코드를 실행할 수 있도록 도와줍니다. @Before와 @After로 선언된 메서드에서도 실행 전후처리로 코드를 넣을 수 있지만, JUnitRules로 작성하면 재사용하거나 더 확장 가능한 기능으로 개발할 수 있는 장점이 있습니다. JUnit에서 기본적으로 제공하는 Rules은 다음과 같습니다. | Rules             | 설명                                                         |\n| ----------------- | ------------------------------------------------------------ |\n| TemporaryFolder   | 테스트 전후로 임시 폴더 및 파일을 자동으로 생성하고 삭제한다 |\n| ExternalResource  | 외부 리소스에 대한 전후처리를 한다                           |\n| ExpectedException | 테스트 클래스 전체에 적용되며 예외 발생에 대해 직접 확인이 가능한다 |\n| ErrorCollector    | 여러 테스트 실패시에도 연속적으로 테스트가 진행되며 발생한 오류를 수집한다 |\n| Verifier          | 테스트 실행시 추가 검증을 하도록 도와준다.                   |\n| TestName          | 실행하는 테스트 메서드 이름을 알려준다                       |\n| RuleChain         | 여러 Rule을 체인형식으로 묶어 적용할 수 있도록 도와준다      |\n| ClassRule         | 테스트 클래스 슈트 전체에 적용할 수 있는 Rule이다            |\n| Timeout           | 테스트 클래스 전체 테스트에 timeout을 설정한다               | 기본으로 제공하는 Rule 외에 직접 나만의 Rule은 어떻게 생성하는지도 같이 알아보겠습니다. 2. 개발 환경 - OS : Mac OS\n- IDE: Intellij\n- Java : JDK 1.8\n- Source code : github\n- Software management tool : Maven 포스팅을 위해 여러 예제를 작성하였지만, 다양한 사용법을 더 보고 싶으면, JUnit4소스코드에 포함된 테스트 케이스들을 보면 더 다양하게 사용되는 예제를 확인할 수 있습니다. 메이븐 의존성으로 pom.xml 파일에 JUnit을 추가합니다. 3. JUnit Rules 3.1 기본으로 제공하는 Rules JUnit Rules에서 대표적으로 많이 사용되는 예제들을 보겠습니다. 3.1.1 TemporaryFolder TemporayFolder Rule은 테스트 실행 시 파일이나 폴더를 자동으로 생성하고 테스트 종료 시에도 자동으로 삭제해주는 Rule입니다. 임의 파일을 생성하게 되면 맥에서는 아래와 같은 폴더에 생성됩니다. 3.1.2 ExpectedException ExpectedException Rule은 @Test(expected = RunTimeException.class) 대신 사용할 수 있고 예외 타입과 예외 메시지도 직접 확인이 가능하게 해주는 Rule입니다. 3.1.3 Timeout Timeout Rule은 모든 테스트에 대해 같은 timeout 설정을 할 수 있게 하는 Rule입니다. 테스트 결과 Timeout 설정을 2초로 해서 2초이상 실행되면 TimeOutException을 발생시킵니다. 3.1.4 ErrorCollector ErrorCollector Rule은 assertion이 실패하더라도 테스트를 계속 실행하여 전체 오류를 수집하는 기능입니다. 테스트 실행 시 발생하는 장애(ex. 네트워크)가 있더라도 테스트를 계속 진행하고 싶을 때 이 Rule을 사용하면 좋습니다. ErrorCollector 사용시 아래 2가지 메서드를 사용하면 됩니다. - addError() : 예외가 발생했을 때 해당 예외와 메시지를 같이 출력하도록 오류를 추가해준다\n- checkThat() : 기대 값과 실제 값이 같은지 체크하고 값이 다르더라도 테스트는 계속 진행한다 테스트 결과 기대 값과 실제 값이 다르면 addError()로 추가된 메시지를 출력만 하고 일단 테스트를 계속 실행합니다. 테스트 실행이후에 각 실패에 대한 결과를 출력합니다. 3.1.5 Verifier Verifier Rule은 테스트 실행할 때마다 실행되며 사용자 정의 검증 로직을 추가로 넣어 특정 조건을 만족하는지 검증하는 데 사용됩니다. 테스트 결과 모든 테스트 실행 시 추가로 사람의 나이가 25 이상 인지를 검증합니다. 두 번째 테스트 personTest2에서 나이가 30이라서 실패로 떨어졌습니다. 3.1.6 TestName TestName Rule은 현재 실행되는 메서드 이름을 불러오도록 해줍니다. 3.1.7 RuleChain RuleChain Rule은 테스트 실행 시 여러 Rule을 순차적으로 실행하도록 도와주는 Rule입니다. 예제에서는 사용자 정의로 생성한 LoggingRule을 체인형식으로 적용하였습니다. LoggingRule은 각 테스트 전후로 시작… 끝…. 로그 메시지를 출력하는 Rule로 이해하시면 되고 더 자세한 내용은 #3.2에서 다루도록 하겠습니다. 테스트 결과 3.1.8 ExternalResource ExternalResource Rule은 테스트 전에 외부 리소스(ex. 파일, 네트워크 소켓, 서버, 데이터베이스 연결 등)에 접근할 수 있도록 자원에 연결해주고 테스트 종료 후에도 연결을 자동으로 끊어주는 Rule입니다. 테스트 결과 테스트 실행 전후로 서버에 연결하고 종료 후에는 연결을 끊고 있습니다. 3.1.9 ClassRule ClassRule 어노테이션을 @Rule 어노테이션과 같이 사용하면 TestSuite로 묶여 있는 클래스를 통합하여 실행해줍니다. 테스트 결과 여러 테스트 클래스가 시작하기 전에 서버 연결을 먼저하고 모든 테스트가 끝나고 나서 서버 연결을 끊는 것을 확인할 수 있습니다. 3.1.10 TestWatcher TestWatcher Rule은 테스트 실행에 대한 성공 실패를 모니터링 하는 기능을 제공하여 테스트 로그를 쓰도록 도와줍니다. @FixMethodOrder는 테스트 실행 순서를 결정해주는 어노테이션으로 이 예제에서는 NAMEASCENDING으로설정되어 메서드 이름의 순서대로 실행됩니다. TestWatcher에 정의된 starting(), finished(), succeeded(), failed() 메서드을 오버라이트하면 메서드 이름에 맞게 테스트 시작, 끝, 성공, 실패에 따라서 메서드들이 호출됩니다. 이 예제에서는 매번 실행할 때마다 watchedLog 스트링값에 로그형식으로 저장하여 화면에 출력합니다. 테스트 결과 메서드 이름의 순서대로 테스트가 실행되며 하나씩 실행할 때마다 로그가 쌓이고 있습니다. 3.2 Custom Rules 지금까지 JUnit에서 기본으로 제공하는 Rules을 알아보았습니다. 직접 Rule을 어떻게 생성하는지는 지금까지 소개해 드렸던 코드를 보면 더 이해가 쉽습니다. 예로. TemporaryFolder Rule을 살펴보도록 하겠습니다. TemporaryFolder 클래스는 ExternalResource 클래스를 상속받아 before()와 after() 메서드를 구현하였습니다. 테스트 실행 전에 before() 메서드가 실행되며 create() 메서드를 호출합니다. createTemporaryFolderIn() 메서드에서 알 수 있듯이 임의의 폴더를 만들어 File 클래스를 반환합니다. 테스트 이후에는 after() 메서드가 실행되며 create()에서 생성한 폴더를 삭제합니다. ExternalResource 클래스는 앞 써 봤던 before()와 after() 메서드를 가지고 기본적인 전후처리 알고리즘을 담고 있습니다. apply() 메서드가 호출되며 전후처리 로직이 실행되는 구조입니다. 실제로 apply() 메서드가 언제 호출되는지는 JUnit4소스 코드로 확인해보시면 좋을 듯합니다. TemporaryFolder 클래스와 거의 유사한 코드이기는 하지만, 테스트 실행 전후로 ‘시작, 끝’을 출력하는 Rule을 만들어보겠습니다. base.evaluate()은 테스트가 실행되는 시점이고 전후로 생성자로 넘겨준 name과 같이 로그를 출력하는 코드를 추가하였습니다. 테스트 결과 4. 결론 JUnit에 Rule이라는 여러 기능을 있는지는 이번 스터디 기회를 통해서 알게 되었습니다. 프로젝트를 하면서 테스트 코드를 많이 작성하는 편인데, JUnit Rule을 통해서 더 유용하게 적용할 수 있는 부분들이 있을 듯합니다. 이만 오늘 포스팅을 마무리하겠습니다. 5. 참고 - JUnit Rules\n    - https://github.com/junit-team/junit4/wiki/rules\n    - https://www.swtestacademy.com/junit-rules/\n    - https://www.alexecollins.com/tutorial-junit-rule/\n    - https://stefanbirkner.github.io/system-rules/ \\ http://kwonnam.pe.kr/wiki/java/junit/rule",
    "category": "java",
    "tags": [
      "junit",
      "junit rules",
      "unit test",
      "test",
      "rules",
      "java",
      "자바"
    ],
    "date": "2018-07-29T00:00:00.000Z"
  },
  {
    "id": "java/lombok-기본-사용법-익히기",
    "slug": "java/lombok-기본-사용법-익히기",
    "title": "Lombok 기본 사용법 익히기",
    "excerpt": "",
    "content": "1. 들어가며\nLombok는 자바에서 작성해야 하는 boilerplate code(ex. getter/setter, constructor, toString)를 선언한 어노테이션을 통해서 자동으로 생성해주는 라이브러리입니다. 코드 자체가 더 간결해져 가독성도 높아지고 더 빠르게 개발할 수 있는 장점이 있습니다. 하지만, Lombok 사용 시 주의가 필요한 부분이 분명히 존재합니다. (참조 : Lombok Pitfall)\n그래서 올바로 알고 주의해서 사용하기를 권장합니다. 이 포스팅에서는 자주 사용되는 어노테이션 위주로 작성하도록 하겠습니다. 2. 환경 설정 아래 환경 기반으로 코드가 작성되어 있습니다. IDE에서 Lombok 플러그인을 설치해야 어노테이션이 인식됩니다. - OS: Mac OS\n- IDE: Intelij\n- Java : JDK 11\n- Source code : github\n- Software management tool : Maven\n- IDE Plugin :  Lombok Plugin  Java Bytecode Decompiler : Enable - class 파일을 decompile 해서 소스를 볼 수 있다 pom.xml 파일에 lombok dependency를 추가해줍니다. 3. 대표적인 예제 자주 사용되는 어노테이션 위주로 살펴보도록 하겠습니다. - @NonNull\n- @Getter, @Setter\n- @ToString\n- @AllArgsConstructor, @AllArgsConstructor, @NoArgsConstructor\n- @EqualsAndHashCode\n- @Builder\n- @Data\n- @Slf4j, @Log, @Log4j, @Log4j2\n- Lombok Configuration @NonNull 메서드나 생성자 인자에 @NunNull 어노테이션을 추가하면 Lombok가 null 체크 구문을 생성해줍니다. 위와 같이 바닐라 버전의 자바 소스를 보고 싶다면 IDE에서 Java Bytecode Decompiler 플러그인 활성화 이후 해당 class를 클릭하면 소스코드를 직접 확인할 수 있습니다. @Getter, @Setter 클래스 필드에 대한 getter와 setter 메서드를 생성해주고 여러 옵션으로 다양한 코드를 자동생성할 수 있습니다. Setter는 필드가 final이 아닌 필드에 대해서 메서드가 생성됩니다. - 클래스 전체에 적용 - 각 클래스 필드에 적용 - 메서드의 접근제어자 변경     - @Setter(AccessLevel.PROTECTED) - 메서드 체이닝 (setter)     - @Accessors(chain = true) Lombok 사용 @ToString 클래스의 toString 메서드를 자동으로 생성해주고 옵션을 주어 toString에 제외 시킬 필드 속성도 지정할 수 있습니다. - 메서드에서 제외시킬 필드 지정     - @ToString.Exclude - 메서드에서 추가하고 싶은 필드 지정 @EqualsAndHashCode equals()와 hashCode()를 자동 생성해주는 어노테이션입니다. - 제외시킬 필드 지정\n    - @EqualsAndHashCode.Exclude 바닐라 자바 코드가 너무 길어서 간출렸습니다. 실제 코드는 컴파일된 클래스 파일으로 확인해보세요. @NoArgsConstructor, @AllArgsConstructor, @RequiredArgsContructor 생성자를 자동으로 생성해주는 어노테이션입니다. 필드 선언순서에 따라 생성자 인자가 정해집니다. 나중에 리펙토링을 하게 되면 인자 순서가 변경될 수 있다는 점을 기억하면 좋을 것 같습니다. - NoArgsConstructor : 인자 없는 생성자\n- AllArgsConstructor : 모든 필드를 인자로 받는 생성자\n- RequiredArgsContructor(staticName=“of\") : static factory 메서드를 생성함 @Data @Data 어노테이션은 아래 모든 어노테이션이 적용되는 어노테이션입니다. - @ToString\n- @EqualsAndHashCode\n- @Getter, @Setter\n- @RequiredArgsConstructor @Builder 어노테이션 하나로 Builder Pattern 을 생성해줍니다. 빌더 패턴은 여러 설정하고 객체를 만들어주는 패턴입니다. 더 자세한 내용은 아래 참조를 확인해주세요. @Slf4j, (@Log, @Log4j, @Log4j2, etc) 원하는 로깅 프레임워크를 선택해서 선언하면 보다 쉽게 로그를 사용할 수 있습니다. Lombok Configuration Lombok에서 제공하는 기능에 대해서 사용하지 못하게 하는 설정등이 가능합니다. 프로젝트 루트에 lombok.config 파일을 생성해서 원하는 설정를 key=value 형식으로 작성하면 됩니다. 구체적인 설정은 해당 Lombok Configuration system 을 참조해주세요. 예 - 설정(@NonNull 사용 금지) IDE에서는 표시되지 않지만, 컴파일시 오류가 발생합니다. 4. 참고 - Lombok\n    - http://jnb.ociweb.com/jnb/jnbJan2010.html\n    - https://projectlombok.org/\n    - http://www.vogella.com/tutorials/Lombok/article.html\n    - http://www.daleseo.com/lombok-popular-annotations/\n    - https://projectlombok.org/features/all\n    - http://edoli.tistory.com/99\n    - https://codeburst.io/how-to-write-less-and-better-code-or-project-lombok-d8d82eb3e80a\n    - https://www.baeldung.com/intro-to-project-lombok\n- Lombok Pitfall\n    - http://kwonnam.pe.kr/wiki/java/lombok/pitfall\n    - https://www.popit.kr/실무에서-lombok-사용법/\n- Lombok Config\n    -  https://projectlombok.org/features/configuration\n- Builder 패턴\n    -  https://en.wikipedia.org/wiki/Builderpattern#Java",
    "category": "java",
    "tags": [
      "lombok",
      "java",
      "annotation",
      "자바",
      "어노테이션"
    ],
    "date": "2018-12-16T00:00:00.000Z"
  },
  {
    "id": "java/qa-cache-ssm-관련-질문-모음",
    "slug": "java/qa-cache-ssm-관련-질문-모음",
    "title": "Q&A Cache-SSM 관련 질문 모음",
    "excerpt": "",
    "content": "개인적으로 모르는 부분 적어두고 알게 되는 부분에 대해서 간단하게 정리해둔 자료입니다.\n미 답변중에 알고 계신 부분 있으면 코멘트 달아주세요. 감사합니다. Q&A 전체 목록 <span style=\"color:orange\">[답변완료]</span> <span style=\"color:brown\">1. @CacheKeyMethod 란?</span> SSM관련 어노테이션으로 key 값 제공 메서드이고 없는 경우에는 toString()을 호출하게 됩니다. 추가로 캐쉬내 namespace에 toString()로 해서 같은 key 있으면 충돌이 발생합니다. 참고\n https://m.blog.naver.com/PostView.nhn?blogId=kbh3983&logNo=220934569378&proxyReferer=https%3A%2F%2Fwww.google.co.kr%2F <span style=\"color:brown\">2. 왜 DB 데이터를 캐싱을 해야 하나?</span> 매번 DB에서 데이터를 가져오면 많이 느린 이슈가 있습니다. 참고\n https://charsyam.wordpress.com/2016/07/27/입-개발-왜-cache를-사용하는가/ - - - -\n- \n<span style=\"color:orange\">[미 답변 질문]</span> -",
    "category": "java",
    "tags": [
      "Q&A",
      "faq",
      "ssm",
      "cache-ssm",
      "cache",
      "캐쉬"
    ],
    "date": "2018-07-29T00:00:00.000Z"
  },
  {
    "id": "java/qa-jackson-관련-질문-모음",
    "slug": "java/qa-jackson-관련-질문-모음",
    "title": "Q&A Jackson 관련 질문 모음",
    "excerpt": "",
    "content": "개인적으로 모르는 부분 적어두고 알게 되는 부분에 대해서 간단하게 정리해둔 자료입니다.\n미 답변중에 알고 계신 부분 있으면 코멘트 달아주세요. 감사합니다. Q&A 전체 목록 <span style=\"color:orange\">[답변완료]</span> <span style=\"color:brown\">1. @JsonInclude(Include.NONNULL)?</span> 이 어노테이션은 클래스 필드에서 null이 되는 필드는 JSON으로 serialize할 때 제외하도록 하는 어노테이션입니다. 위 코드에서는 stringValue 변수는 JSON으로 저장되지 않습니다. 용어\n 자바 객체를 JSON 으로 변환하는 작업을 serialize 라고 표현하고 JSON -> 객체는 deserialize이라고 한다 참고\n http://multifrontgarden.tistory.com/172 <span style=\"color:brown\">2. @JsonIgnore?</span> 직렬화시 해당 필드를 포함시키지 않고 싶을 때 변수위에 선언하는 어노테이션입니다. 이 예제에서는 JPA를 통해 domain 객체를 얻어올 때 암호가 있으면 안되기 때문에 @JsonIgnore 어노테이션을 적용하였습니다. 참고  http://eglowc.tistory.com/28 <span style=\"color:brown\">3. @JsonIgnoreProperties(ignoreUnknown = true)?</span> 객체에는 속성이 없지만, JSON에 포함되면 Exception이 발생하는데 무시하도록 하는 어노테이션이빈다. 참고\n https://www.javacodegeeks.com/2018/01/ignore-unknown-properties-parsing-json-java-jackson-jsonignoreproperties-annotation-example.html <span style=\"color:brown\">4. @JsonIgnoreProperties에서 allowGetters를 true로 세팅하면 어떻게 되나?</span> @JsonIgnoreProperties로 무시하려는 프로퍼티를 지정할 때 allowGetters를 true로 해주면, JSON serialization (Object -> JSON)으로는 지정한 필드는 적용이 되지만, deserialization (JSON -> Object)에서는 제외된다는 의미입니다. 참고\n https://www.concretepage.com/jackson-api/jackson-jsonignore-jsonignoreproperties-and-jsonignoretype#allowGetters <span style=\"color:brown\">5. Jackson에서 직렬화, 역직렬화의 의미는?</span> - Jackson\n    - 직렬화 (Serialization)\n        - 자바 Object -> Jackson JSON 변화해주는 것이다.\n    - 역직렬화 (Deserialization)\n        - Jackson JSON -> 자바 Object로 변환해주는 것이다\n- Java (참고용)\n    - 직렬화\n        - 자바 Object -> byte 형태\n    - 역직렬화\n        - byte 형태 -> 자바 Object 참고 - Jackson\n    - https://homoefficio.github.io/2016/11/19/%EC%A1%B0%EA%B8%88%EC%9D%80-%EC%8B%A0%EA%B2%BD%EC%8D%A8%EC%A4%98%EC%95%BC-%ED%95%98%EB%8A%94-Jackson-Custom-Deserialization/\n    - https://thepracticaldeveloper.com/2018/07/31/java-and-json-jackson-serialization-with-objectmapper/\n- Java - https://nesoy.github.io/articles/2018-04/Java-Serialize - - - - <span style=\"color:orange\">[미 답변 질문]</span> - @DateTimeFormat vs @JsonFormat의 차이점은?\n DateTimeFormat : DateTimeFormat\n JsonFormat : jackson - @JsonTypInfo, @JsonSubTypes? 참고\n https://www.slipp.net/questions/442\n https://seongtak-yoon.tistory.com/70 - @JsonManagedReference\n- @JsonBackReference - @JsonIdentityInfo, @JsonIdentityReference란? - @OneToMany, @ManyToOne로 연결된 엔터티에서 데이터를 조회하면 Infinite recursion JsonMappingException이 발생함\n- 해결방법\n     Jackson 1/6+\n         @JsonManagedReference, @JsonBackReference를 이용함\n     Jackson 2.0+\n         @JsonIdentityInfo를 사용함",
    "category": "java",
    "tags": [
      "Q&A",
      "QA",
      "faq",
      "jackson",
      "java",
      "자바"
    ],
    "date": "2018-03-25T00:00:00.000Z"
  },
  {
    "id": "java/qa-java-관련-질문-모음",
    "slug": "java/qa-java-관련-질문-모음",
    "title": "Q&A Java 관련 질문 모음",
    "excerpt": "",
    "content": "개인적으로 모르는 부분 적어두고 알게 되는 부분에 대해서 간단하게 정리해둔 자료입니다.\n미 답변중에 알고 계신 부분 있으면 코멘트 달아주세요. 감사합니다. Q&A 전체 목록 <span style=\"color:orange\">[미 답변 질문]</span> - java8 에서 mapToInt(ToIntFunction mapper)는 뭔가?\n- Intermediate operation으로 스트림의 각 요소에 mapper를 실행하고 IntStream을 반환한다 참고  https://www.geeksforgeeks.org/stream-maptoint-java-examples/ - bridge 메서드는 무엇인가? - 이거의 의미는 뭔가?\n- JobKey가 상속받을 수 있는 걸 제한하는 건가? - facade 패턴이란?\n- strateggy 패턴이란? - @Constraint(validateBy…?) 참고 - https://dzone.com/articles/create-your-own-constraint-with-bean-validation-20 --- <span style=\"color:orange\">[답변완료]</span> <span style=\"color:brown\">1. 왜 Constants 값을 인터페이스에 정의하나? final class에 하지 않나?</span> 인터페이스에도 Constants 값을 지정할 수 있습니다. 이런 방식은 비추천하는 방식으로 언급되는데요. 찾아보면 오픈소스에서도 인터페이스에 정의하는 프로젝트도 종종 찾을 수 있습니다. 참고  http://www.javapractices.com/topic/TopicAction.do?Id=32\n https://stackoverflow.com/questions/40990356/interface-constants-vs-class-constants-variables\n https://veerasundar.com/blog/2012/04/java-constants-using-class-interface-static-imports/ <span style=\"color:brown\">2. 메서드에서 제너릭으로 선언을 할 때 & 가 의미하는 건 뭔가?</span> 인자로 들어오는 게 Enum 타입이여야 하고 인터페이스 CodeEnum을 구현하는 타입이여야 한다는 의미입니다. 결국 둘 다 만족하는 타입이어야 한다는 의미입니다. 참고  https://stackoverflow.com/questions/21142467/generics-ambiguity-with-the-operator-and-order\n https://stackoverflow.com/questions/745756/java-generics-wildcarding-with-multiple-classes <span style=\"color:brown\">3. ExecutorService에서 submit와 execute()의 차이점은?</span> - submit : task을 실행하고 실행된 결과를 Future 객체로 받아 cancel(), get()을 호출하여 추후 task 관리를 할 수 있게 해준다.\n- execute : task을 실행하고 결과를 따로 받지 못한다 참고  https://stackoverflow.com/questions/18730290/what-is-the-difference-between-executorservice-submit-and-executorservice-execut <span style=\"color:brown\">4. 언제 Collections.singleton() 메서드를 사용하나?</span> singleton() 메서드는 인자로 넘겨진 오브젝트 하나로 immutable한 set를 반환하는 메서드입니다. 리시트의 removeAll()와 같이 컬렉션 인터페이스를 인자로 받는 경우에 하나의 요소만을 전달할 때 사용합니다. 일일이 컬렉션 생성, 요소 추가등을 할 필요없이 singleton(값) 메서드 호출로 객체를 반환하여 쉽게 사용할 수 있습니다. List<Integer> list = Arrays.asList(1, 2, 3, 4, 4);\nlive.removeAll(Collections.singleton(4); - Set : Collections.singleton(T o)\n- List : Collections.singleList(T o)\n- Map : Collections.singleMap(K, V) 참고  https://java2free.tistory.com/entry/%EC%9E%90%EB%B0%94-Collection\n https://www.javatpoint.com/java-collections-singleton-method\n https://stackoverflow.com/questions/4801794/use-of-javas-collections-singletonlist\n https://stackoverflow.com/questions/4801794/use-of-javas-collections-singletonlist",
    "category": "java",
    "tags": [
      "Q&A",
      "QA",
      "faq",
      "java",
      "자바"
    ],
    "date": "2019-03-21T00:00:00.000Z"
  },
  {
    "id": "java/qa-jsonpath-관련-질문-모음",
    "slug": "java/qa-jsonpath-관련-질문-모음",
    "title": "Q&A JsonPath 관련 질문 모음",
    "excerpt": "",
    "content": "개인적으로 모르는 부분 적어두고 알게 되는 부분에 대해서 간단하게 정리해둔 자료입니다.\n미 답변중에 알고 계신 부분 있으면 코멘트 달아주세요. 감사합니다. Q&A 전체 목록 <span style=\"color:orange\">[답변완료]</span> <span style=\"color:brown\">1. com.jayway.jsonpath 라이브러리는 어디에서 사용하나?</span> Jayway JsonPath는 JsonPath 표기법(ex. $.data[0])으로 json 데이터의 값은 부분적으로 extract할 수 있는 라이브러리입니다. 참고\n Java Jayway JsonPath 사용법 - - - -\n<span style=\"color:orange\">[미 답변 질문]</span>\n-",
    "category": "java",
    "tags": [
      "Q&A",
      "faq",
      "json",
      "jsonpath",
      "java",
      "자바"
    ],
    "date": "2018-07-29T00:00:00.000Z"
  },
  {
    "id": "java/qa-lombok-관련-질문-모음",
    "slug": "java/qa-lombok-관련-질문-모음",
    "title": "Q&A Lombok 관련 질문 모음",
    "excerpt": "",
    "content": "개인적으로 모르는 부분 적어두고 알게 되는 부분에 대해서 간단하게 정리해둔 자료입니다.\n미 답변중에 알고 계신 부분 있으면 코멘트 달아주세요. 감사합니다. Q&A 전체 목록 <span style=\"color:orange\">[답변완료]</span> <span style=\"color:brown\">1. @AllArgsConstructor란?</span> - @AllArgsConstructor : 모든 필드를 파라미터로 가지는 생성자를 만든다\n- @NoArgsContructor : 파라미터가 없는 생성자를 만든다 참고  http://partnerjun.tistory.com/54 --- <span style=\"color:orange\">[미 답변 질문]</span> -",
    "category": "java",
    "tags": [
      "Q&A",
      "QA",
      "faq",
      "lombok",
      "annotation",
      "java",
      "자바",
      "어노테이션"
    ],
    "date": "2019-03-23T00:00:00.000Z"
  },
  {
    "id": "java/qa-maven-관련-질문-모음",
    "slug": "java/qa-maven-관련-질문-모음",
    "title": "Q&A Maven 관련 질문 모음",
    "excerpt": "",
    "content": "개인적으로 모르는 부분 적어두고 알게 되는 부분에 대해서 간단하게 정리해둔 자료입니다.\n미 답변중에 알고 계신 부분 있으면 코멘트 달아주세요. 감사합니다. Q&A 전체 목록 <span style=\"color:orange\">[답변완료]</span> <span style=\"color:brown\">1. maven으로 특정 클래스의 메서드 unit test 실행은 어떻게 하나?</span>\n-DTest= 옵션에 패키지 이름.파일명#메서드이름 형식으로 지정하면 원하는 메서드를 실행시킬 수 있습니다. 메이븐에서 -D 옵션은 system property를 지정하는 옵션입니다. <span style=\"color:brown\">2. Maven 실행시 webxml attribute is required... ?</span> maven 컴파일시 webxml attribute is required...이라는 오류가 발생하는 경우에 대한 해결책은 다음과 같습니다. 1. 서블릿 컨테이너 3이하인 경우, WEB-INF/web.xml을 생성해줘야 한다\n2. 서블릿 컨테이너 3이상인 경우, web.xml은 없는 경우에는 failOnMissingWebXml=false로 지정하여 무시하도록 설정한다 참고 -  https://www.mkyong.com/maven/maven-webxml-attribute-is-required/ --- <span style=\"color:orange\">[미 답변 질문]</span>",
    "category": "java",
    "tags": [
      "Q&A",
      "faq",
      "maven",
      "java",
      "메이븐",
      "자바"
    ],
    "date": "2018-07-29T00:00:00.000Z"
  },
  {
    "id": "java/qa-개발관련-질문-모음",
    "slug": "java/qa-개발관련-질문-모음",
    "title": "Q&A 개발관련 질문 모음",
    "excerpt": "",
    "content": "새로운 직장에 와서 보니 모르는 거 천지라 전혀 모르고 어설프게 아는 것들을 질문 형식으로 정리를 해봤습니다. 책도 보면서 많이 알게 내용도 있습니다. 다시 구체적으로 정리하면 더 좋겠지만, 초반에 전혀 모르는 건 그냥 적어두고 검색하면서 조금씩 알게 된 내용으로 정리를 했습니다. 더 자세한 설명은 별도의 포스팅으로 작성해서 참고 링크로 달아두었습니다.  Cache\n     Q&A : Cache-SSM 관련 질문 모음\n Database\n     Q&A : MySql 관련 질문 모음\n     Q&A : Mybatis 관련 질문 모음\n     Q&A : JPA 관련 질문 모음\n Git\n     Q&A : Git 관련 질문 모음\n Java\n     Q&A : Java 관련 질문 모음\n     Q&A : Lombok 관련 질문 모음\n     Q&A : Spring Boot 관련 질문 모음\n     Q&A : Spring Boot Annotation 모음\n     Q&A : Spring JPA Annotation 모음\n JavaScript\n     Q&A : JavaScript 관련 질문 모음\n Json\n     Q&A : Jackson 관련 질문 모음\n     Q&A : JsonPath 관련 질문 모음\n Software Management Tool\n     Q&A : Maven 관련 질문 모음",
    "category": "java",
    "tags": [
      "Q&A",
      "QA",
      "faq",
      "질문",
      "개발"
    ],
    "date": "2018-07-29T00:00:00.000Z"
  },
  {
    "id": "java/맥-환경에서-여러-jdk-버전-설치하고-변경하기",
    "slug": "java/맥-환경에서-여러-jdk-버전-설치하고-변경하기",
    "title": "맥 환경에서 여러 JDK 버전 설치하고 변경하기",
    "excerpt": "",
    "content": "자바 개발을 하다 보면 하나의 JDK 버전이 아니라 여러 버전을 설치해야 할 때가 종종 있습니다. 진행하는 프로젝트마다 개발하는 JDK 버전이 조금씩 다를 수 있고 새로 릴리스한 버전을 설치해서 스터디하고 싶을 때 여러 버전이 존재하게 됩니다. 한 시스템에 여러 버전이 존재하지만, 쉽게 한 버전에서 다른 버전으로 변경할 수 있는 명령어를 JDK에서는 제공하지는 않습니다. 개발자가 알아서 수동으로 변경해야 합니다. 본 포스팅에서는 맥 환경을 대상으로 어떻게 여러 버전의 JDK로 쉽게 변경할 수 있는지 알아보겠습니다. 1. 여러 JDK 버전 설치하기 일단, 먼저 여러 JDK 버전을 설치해 볼까요? brew 명령어로 3가지 JDK 버전을 설치하도록 하겠습니다. - java : OpenJDK 11\n- java8 : Oracle JDK 8\n- zulu8 : Azul Zulu Java JDK 2. 여러 버전으로 변경해보기 현재 설치된 모든 JDK를 확인하려면, javahome -V 명령어로 확인할 수 있습니다. 제 맥에서는 총 4가지 JDK가 설치되어 있습니다. 원하는 버전의 JDK로 자바 프로그램을 컴파일하고 실행하려면 기본적으로 아래 2가지를 기본적으로 해줘야 합니다. - JAVAHOME 환경 변수를 수정한다\n    - JAVAHOME=“/Library/Java/JavaVirtualMachines/jdk1.8.0121.jdk/Contents/Home\"\n- PATH에도 JDK/bin 폴더를 추가한다\n    - PATH=$PATH:$JAVAHOME/bin 환경변수는 대부분 사용하는 shell의 환경 파일을 손 보면 됩니다. 저는 zsh shell을 사용해서 .zshrc를 아래처럼 수정했습니다. 소스 코드 넣기 실행 화면입니다. 도움말로 더 쉽게 이해할 수 있는 부분이라 별도의 설명은 생략하겠습니다. 3. 참고 - OS X에서 기본 자바 JDK 변경하기\n    - https://stackoverflow.com/questions/21964709/how-to-set-or-change-the-default-java-jdk-version-on-os-x",
    "category": "java",
    "tags": [
      "java",
      "mac",
      "jdk",
      "version",
      "자바",
      "버전",
      "맥"
    ],
    "date": "2018-11-11T00:00:00.000Z"
  },
  {
    "id": "java/새로운-기능-및-개선-사항-목록-자바10에서의-변화",
    "slug": "java/새로운-기능-및-개선-사항-목록-자바10에서의-변화",
    "title": "새로운 기능 및 개선 사항 목록 - 자바10에서의 변화",
    "excerpt": "",
    "content": "자바10\n 언어\n     JEP 286: Local Variable Type Inference\n JVM/Compiler\n     JEP 304: Garbage-Collector Interface\n     JEP 307: Parallel Full GC for G1\n     JEP 316: Heap Allocation on Alternative Memory Devices\n     JEP 317: Experimental Java-Based JIT Compiler\n 기타 변경 및 개선사항\n     JEP 296: Consolidate the JDK Forest into a Single Repository\n     JEP 310: Application Class-Data Sharing\n     JEP 312: Thread-Local Handshakes\n     JEP 313: Remove the Native-Header Generation Tool (javah)\n     JEP 314: Additional Unicode Language-Tag Extensions\n     JEP 319: Root Certificates\n     JEP 322: Time-Based Release Versioning 자바10에 추가된 여러 기능 및 개선 사항은 다음 링크를 참조해주세요.  http://openjdk.java.net/projects/jdk/10/\n https://www.oracle.com/technetwork/java/javase/10-relnote-issues-4108729.html\n https://dzone.com/articles/whats-new-in-java-10 JEP 286: Local Variable Type Inference\n타임 추론이란 자바 컴파일러가 각 메서드 호출과 정의된 메서드 선언문을 보고 인자의 타입을 추론하는 기능을 말합니다. 타임 추론(type inference)은 자바5부터 지속적으로 개선해 왔었습니다.  Java 5 : 제네릭 메서드와 타입 인지 타입추론\n Java 7 : 다이아몬드 연산자(<>)\n Java 8 : 람다식 인자 타입\n Java 10 : 지역변수 타입추론 타입추론 개선 내역 Java 5 : 제네릭 메서드 타입 추론 Java 7 : 다이아몬드 연사자(<>) Java 8 : 람다식 안자 타입 Java 10 : 지역변수 타입 추론\n자바에서도 var를 도입하여 암시적 타이핑을 지원하게 되었습니다. var는 keyword(ex.abstract)가 아니라 reserved type name이라서 변수, 함수 이름으로도 사용할 수 있습니다.\n추가로 var의 도입으로 dynamic type을 지원하는 것은 아닙니다. compiler가 알아서 타입을 추론해서 compile 해주는 것입니다. 이전 자바 자바10 제약사항으로 타입 추론이 안되는 경우도 있습니다.  nulll로 assign하는 경우      \n local 변수가 아닌 경우    explicit initialization이 없는 경우      \n initiailization이 있어도 안되는 경우도 있음 - array initializer      \n method의 인자로도 안됨      \n 람다 표현식에는 explicit target type이 필요함       var를 사용할때는 주의가 필요합니다. var를 사용하면 어떤 타입인지를 알수 없게 되어 가속성이 떨어지게 됩니다. 변수이름에 타입을 추가하여 이름을 사용하여 가독성을 높여주는게 좋습니다.\n더 자세한 사항은 java.net에서 제공한 가이드라인( Style Guidlines for Local Variable Type Inference in Java )을 참조해주세요. 참고  로컬 변수 타입 추론\n     https://www.baeldung.com/java-10-local-variable-type-inference\n     https://blog.codefx.org/java/java-10-var-type-inference/\n     https://www.slideshare.net/OracleDeveloperkr/main-session-java\n reserved type이란\n     https://stackoverflow.com/questions/49102553/what-is-the-conceptual-difference-between-a-restricted-keyword-and-reserved-t",
    "category": "java",
    "tags": [
      "java10",
      "java",
      "jdk",
      "openjdk",
      "자바",
      "자바10"
    ],
    "date": "2018-09-11T00:00:00.000Z"
  },
  {
    "id": "java/새로운-기능-및-개선-사항-목록-자바11에서의-변화",
    "slug": "java/새로운-기능-및-개선-사항-목록-자바11에서의-변화",
    "title": "새로운 기능 및 개선 사항 목록 - 자바11에서의 변화",
    "excerpt": "",
    "content": "자바11 - JEP 181: Nest-Based Access Control\n- JEP 309: Dynamic Class-File Constants\n- JEP 315: Improve Aarch64 Intrinsics\n- JEP 318: Epsilon: A No-Op Garbage Collector\n- JEP 320: Remove the Java EE and CORBA Modules\n- JEP 321: HTTP Client (Standard)\n- JEP 323: Local-Variable Syntax for Lambda Parameters\n- JEP 324: Key Agreement with Curve25519 and Curve448\n- JEP 327: Unicode 10\n- JEP 328: Flight Recorder\n- JEP 329: ChaCha20 and Poly1305 Cryptographic Algorithms\n- JEP 330: Launch Single-File Source-Code Programs\n- JEP 331: Low-Overhead Heap Profiling\n- JEP 332: Transport Layer Security (TLS) 1.3\n- JEP 333: ZGC: A Scalable Low-Latency Garbage Collector (Experimental)\n- JEP 335: Deprecate the Nashorn JavaScript Engine\n- JEP 336: Deprecate the Pack200 Tools and API 자바11에 추가된 여러 기능 및 개선 사항은 다음 링크를 참조해주세요. - http://openjdk.java.net/projects/jdk/11/ JEP 321: HTTP Client (Standard) 자바 9 & 10에서 incubated된 HTTP client는 자바11에서는 표준화된 버전으로 릴리스 되었습니다.\n패키지 : java.net.http HTTP2에 대한 더 자세한 사항은 나만 모르고 있던 - HTTP/2 를 참조하세요. JEP 323: Local-Variable Syntax for Lambda Parameters JDK 10에서 var가 도입되었지만, 암묵적 타입의 람다 표현식에는 사용할 수 없었습니다. 자바11부터는 람다 표현식에서도 var 키워드를 사용할 수 있게 되었습니다. 참고 - 자바11\n    - https://blog.takipi.com/java-11-will-include-more-than-just-features/?utmsource=10countdown&utmmedium=readmore\n    - https://medium.com/antelabs/what-is-new-in-java-11-442af9315f07",
    "category": "java",
    "tags": [
      "java",
      "java11",
      "upgrade",
      "JEP",
      "자바",
      "자바11",
      "개선사항"
    ],
    "date": "2018-09-09T00:00:00.000Z"
  },
  {
    "id": "java/새로운-기능-및-개선-사항-목록-자바8에서의-변화",
    "slug": "java/새로운-기능-및-개선-사항-목록-자바8에서의-변화",
    "title": "새로운 기능 및 개선 사항 목록 - 자바8에서의 변화",
    "excerpt": "",
    "content": "자바8 - 언어\n    - 람다식 표현\n    - 스트림\n    - Method Reference\n    - Default Method\n    - Type inferece 개선\n    - Optional\n- 새 자바스크립트 엔진 (Nashorn)\n    - JDK11에서 제거될 예정\n- Joda Time 방식의 새 날짜 API 변경\n- JavaFX\n- 메타 데이터 지원 보완\n- IO/NIO 확장\n- Concurrency API 개선\n- Heap에서 영속 세다(Permanent Generation) 제거 자바8에 추가된 여러 기능 및 개선 사항은 다음 링크를 참조해주세요. - https://www.oracle.com/technetwork/java/javase/8-whats-new-2157071.html\n- http://openjdk.java.net/projects/jdk8/features 자바8에서는 정말로 많은 기능과 개선사항이 있었습니다. 자바8에서의 가장 큰 변화는 자바 언어에 있지 않을 까 싶습니다. \n자바8부터 함수형 패러다임을 지원하게 되어 개발자라면 필수적으로 스터디해야 하는 부분입니다. 시간되는대로 다시 정리를 해보려고 합니다.",
    "category": "java",
    "tags": [
      "java",
      "java8",
      "jdk8",
      "jdk1.8",
      "openjdk",
      "JEP",
      "자바",
      "자바8"
    ],
    "date": "2018-09-02T00:00:00.000Z"
  },
  {
    "id": "java/새로운-기능-및-개선-사항-목록-자바9에서의-변화",
    "slug": "java/새로운-기능-및-개선-사항-목록-자바9에서의-변화",
    "title": "새로운 기능 및 개선 사항 목록 - 자바9에서의 변화",
    "excerpt": "",
    "content": "자바9 - Java Platform Module System\n- JEP 222: Jshell - REPL\n- JEP 158: Unified VM logging\n     JVM component에 대한 공통 로깅 시스템 제공 (-Xlog) - HTML5 Javadoc\n     HTML5 형식의 API를 생성할 수 있는 도구 - Language Update\n     try-with-resources 개선\n     private interface method\n         interface에서는 항상 public로 정의해야 했는데, private도 가능하도록 함\n     diamond operator\n         익명 내부 클래스에서도 diamond operator가 가능하도록 함 - New Core Libraries\n   Process API\n   JEP264: Platform Logging API and Service\n   http://openjdk.java.net/jeps/264\n   CompletableFuture API 강화\n   Reactive Streams - Flow API\n   https://thepracticaldeveloper.com/2018/01/31/reactive-programming-java-9-flow/\n   Collections(List, Set, Map)를 위한 팩토리 메소드 (Factory Method for Collections: List, Set, Map)\n   정적 팩토리 메소드로 작성된 콜렉션은 불변임\n   Enhanced Deprecation\n   Stack-Walking API\n   Other Improvements\n   Stream 개선\n   iterate(), takeWhile()/dropWhile(), ofNullable()\n   Optional 개선 - JEP 11 : HTTP 2.0 (Incubator Modules)\n   http://openjdk.java.net/jeps/110\n- Client Technologies\n   Multi-Resolution Images\n   TIFF Image I/O Plugins\n- Internationalization\n   Unicode 8.0\n   Java 8은 Unicode 6.2을 지원\n   UTF-8 Properties files\n   Default Localte Data Change 자바9에 추가된 여러 기능 및 개선 사항은 다음 링크를 참조해주세요. - https://docs.oracle.com/javase/9/whatsnew/toc.htm#JSNEW-GUID-C23AFD78-C777-460B-8ACE-58BE5EA681F6 자바9에서의 큰 변화중에 하나는 모듈 시스템의 도입입니다. 이 부분도 큰 변화이고 스터디할 부분이 많아서 정리되는 대로 포스팅할 계획입니다.",
    "category": "java",
    "tags": [
      "java",
      "java8",
      "jdk8",
      "jdk1.8",
      "openjdk",
      "JEP",
      "자바",
      "자바8"
    ],
    "date": "2018-09-03T00:00:00.000Z"
  },
  {
    "id": "java/새로운-기능-및-개선-사항-목록-자바beyond에서의-변화",
    "slug": "java/새로운-기능-및-개선-사항-목록-자바beyond에서의-변화",
    "title": "새로운 기능 및 개선 사항 목록 - 자바 Beyond에서의 변화",
    "excerpt": "",
    "content": "자바 Beyond - JEP 301: Enhanced Enums - 현재 보류중 - JEP 302: Lambda Leftovers - Candidate - JEP 305: Pattern Matching - Candidate\n- JEP ???: Data Classes\n- JEP 312: Switch Expressions - Proposed to Target (JDK12)\n- JEP 326: Raw String Literals - Candidate 자바의 릴리스 주기가 6개월로 변경되면서 새로운 기능, 개선사항들이 빠르게 반영될 것으로 기대하고 있습니다. 아직 반영되지는 않았지만, 추후 자바 버전에서 반영될 것으로 기대되는 부분을 정리해보았습니다. JEP 301: Enhanced Enums - 현재 보류 상태 이 제안 문서는 현재 보류된 상태입니다. 나중에 도입될 수 있어서 어떤 개선 사항인지 간단하게 알아봅니다.\nEnum에서도 제너릭을 지원하고 Enum에 정의된 상수 값에 대해 보다 정교한 타입(shaper type)을 지원하는 것입니다.\n참고로, Enum은 자바5에 추가된 기능으로 JVM에서는 Enum에 대한 별로의 처리는 없고 컴파일러가 Enum을 일반 클래스로 변환합니다.\n개선전\n개선후 참고 - Enum 개선사항\n    - http://openjdk.java.net/jeps/301\n    - https://www.infoq.com/news/2017/01/java-enhanced-enums JEP 302: Enhancements to Lambda Expressions : Lambda Leftovers - 상태: Candidate 1. 사용하지 않는 lambda 인자를 underscore로 표현 람다 인자에서 사용되지 않는 변수는 underscore로 표현해 가독성을 높여주는 기능입니다. 2. 인자 은닉화 변수 은닉화(variable shadowing)란 inner 스코프에서 정의된 변수가 outer 스코프에서 정의된 동일한 변수 이름을 가지는 경우를 일컫습니다. 인자 은닉화 개선 내용은 Inner 스코프에서도 동일한 변수를 선언할 수 있도록 하는 것입니다. 3. 옵션: 함수 호출에 대한 명확성에 대한 개선\\\\ 오버로딩된 여러 메서드를 호출할 타임을 명시하지 않아도 컴파일러가 알아서 잘 추론하여 컴파일 해주는 기능입니다. 참고 - 람다 개선 작업\n    - http://openjdk.java.net/jeps/302\n    - https://www.infoq.com/news/2017/01/java10-lambda-leftovers JEP ???: Data Classes 데이터 클라스란 데이터를 담고 로직 구현이 없는 순수한 데이터 객체를 말합니다. 데이터 속성에 접근하기 위한 여러 메서드(ex. getter, setter)를 가집니다. DTO(Data Transfer Object)나 VO(Value Object)이라고도 합니다. 데이터 클라스 작성하려면 기본적으로 getter, setter, equals, hashCode등과 같은 메서드를 개발자가 만들어줘야 했는데, 이 JEP에서 제안하는 것은 컴파일러가 알아서 생성하도록 하는 기능입니다. 이 아이디어는 스칼라의 case, 코틀린의 데이터 클래스에서 가져왔습니다. - Scala - 케이스 클래스\n    - case class Coordinate (lat: Double, lon : Double)\n- Kotlin - 데이터 클래스\n    - data class Coordinate(val lat: Double, val lon: Double) 자바 데이터 클래스 (제안)\nrecord Coordinate(double lat, double lon) {}\n데이터 클래스를 지원함으로써 코드도 더 간결해지는 장점도 생기게 됩니다. 객체 패턴 매칭(JEP 305)을 지원하면 더 쉽고 빠르게 코드를 작성할 수 있게 됩니다. 참고 - 데이터 객체\n    - https://refactoring.guru/smells/data-class\n    - https://jungwoon.github.io/common%20sense/2017/11/16/DAO-VO-DTO/\n    - http://cr.openjdk.java.net/briangoetz/amber/datum.html\n    - https://www.infoq.com/news/2018/02/data-classes-for-java\n- 다른 언어\n    - https://www.baeldung.com/kotlin-data-classes JEP 305 : Pattern Matching - Candidate 이 JEP에서는 matches 키워드를 도입하고 switch 패턴 매칭 개선 작업을 다루고 있습니다. 컴파일러는 x matches Integer i 구문을 개선전의 코드(아래코드)로 인식합니다.\nInteger i 문은 type test pattern 이라고 하고 i은 새로운 변수의 선언으로 인식합니다.\n타켓 변수(ex. obj)가 Integer 인스턴스이면 Integer로 캐스팅하고 i 변수를 블록에서 사용할 수 있는 코드로 해석합니다. 기존 자바에서의 switch문은 Number, String, Enum만 매칭이 가능했는데, 이 개선작업이 포함되면 객체도 매칭이 가능해집니다. 참고 - 개선작업\n    - http://openjdk.java.net/jeps/305\n    - http://cr.openjdk.java.net/briangoetz/amber/pattern-match.html",
    "category": "java",
    "tags": [
      "java",
      "upgrade",
      "JEP",
      "자바",
      "개선사항"
    ],
    "date": "2018-09-10T00:00:00.000Z"
  },
  {
    "id": "java/아마존-s3-bucket-api-사용법",
    "slug": "java/아마존-s3-bucket-api-사용법",
    "title": "아마존 S3 Bucket API 사용법",
    "excerpt": "",
    "content": "1. 들어가며 기업에서도 그렇고 이제 아마존 서비스를 쓰지 않은 곳이 없을 정도로 회사마다 아마존의 서비스를 많이 사용하고 있습니다. 최근에 이직을 한 곳에서도 S3 (Simple Storage Service) 스토리지 서비스를 이용하고 있어 S3 API를 학습할 겸 해서정리를 해봤습니다. S3는 REST/HTTP 기반으로 파일을 저장하기 위한 스토리지이며 아래와 같은 특징을 가지고 있어 많은 곳에서 S3를 사용하고 있습니다. - S3 서비스 특징\n    - 3 copy 복제를 지원하여 데이터 신뢰도(99.9999%)를 보장한다\n    - 용량과 파일 수에 대한 제한이 없다 (ex. 파일당 1B  5TB)\n    - 버전 관리 기능을 제공하여 실수로 삭제한 파일도 복원 가능하다\n    - 다른 아마존 서비스(ex. CloudFount, Glacier)에 쉽게 연동이 가능하다 - 용어 정리\n    - 객체 : S3에서 저장되는 기본 단위로 하나의 파일이라고 생각하면 된다\n    - 키 : 버킷 내 객체를 저장하기 위해 사용되는 고유한 식별자이다\n        - ex. test.xls, thumbs/main.jpg\n    - 버킷 : 디렉토리와 비슷한 개념으로 버킷에 객체를 저장한다\n        - 하위 버킷 또는 하위 폴더의 계층 구조는 없지만, 키 이름 접두사와 구분 기호를 이용하여 논리적인 계층 구조를 만들 수 있다 (ex. develop/test.xls) 2. 개발 환경 및 S3 기본 설정 소스 코드는 대부분은 Amazon SDK 와 Baeldung 에 있는 예제들을 보면서 작성하였습니다. - OS : Mac OS\n- IDE: Intellij\n- Java : JDK 1.8\n- Source code : github\n- Software management tool : Maven pom.xml 파일에 아마존 SDK 의존성을 추가해줍니다. 2.1 S3 기본 설정 AWS SDK를 사용하려면, 아래 3가지는 완료해야 코드상에서 S3에 접속하여 작업을 할 수 있습니다. - AWS 계정 생성\n    - 계정이 없는 경우 AWS 계정 을 생성한다\n- AWS 보안 자격 증명 (코딩시 이게 필요하다)\n    - S3에 접속하려면, 액세스 키 ID와 보안 액세스 키가 필요하다\n    - 코드에서 사용할 보안 자격 증명 사이트에 접속하여 아래와 같이 얻어온다\n       - AWS 지역 선택하기\n    - 지역마다 S3 가격이 다르기 때문에 가장 가까운 지역을 선택하는것이 좋다 3. S3 Bucket에서 파일 다루기 3.1 Client Connection S3 서비스에 접근하기 위한 client connection을 생성해야 합니다. 위에서 생성한 KEYID와 SECRETACCESSKEY가 필요합니다. 3.1.1 명시적으로 지정하는 방법 소스코드 안에서 KEYID와 SECRETACCESSKEY를 인자로 전달하여 BasicAWSCredentials 객체를 생성하여 client 연결을 얻어오는 방법이 있습니다. KEY가 외부로 노출이 되면 누구나 아마존 서비스를 사용할 수 있으므로 잘 못 하면 다른 사람들에 의해서 요금 폭탄을 맞을 수 있습니다. 3.1.2 환경 설정으로 지정하는 방법 Client connection을 가져오는 두 번째 방법입니다. 환경설정 파일에 저장된 key 값을 가져오는 방식입니다. 아래와 같이 AWS 자격 증명 설정이 되어 있어야 합니다. 3.2 S3 Bucket에서 파일 다루기 3.2.1 S3 bucket 생성하기 먼저 버킷을 생성해보겠습니다. 간단합니다. createBucket() 메서드를 사용하면 됩니다. 위 유닛테스트에서 버킷이 생성되었는지 확인해보겠습니다. 3.2.2 파일 업로드 버킷에 파일을 올리려면 pubObject()를 사용합니다. 인터넷에서 이미지 다운로드해서 S3에 올리는 예제입니다. 3.2.3 파일 다운로드 버킷에 올린 파일을 컴퓨터로 다운로드할 수 있습니다. getObject() 메서드로 원하는 파일을 저장합니다. 3.2.4 파일 삭제 버킷에서 하나의 파일을 삭제할 때는 deleteObject() 메서드를사용하고 여러 파일을 한 번에 삭제하려면 deleteObjects() 메서드를 사용합니다. 3.2.5 버킷 삭제 버킷을 삭제하려면 버킷 안에 있는 모든 파일과 버전 관리가 되는 객체들도 다 삭제해야만 버킷을 삭제할 수 있습니다. 4. 참고 - Amazon S3\n    - https://www.baeldung.com/aws-s3-java\n    - https://aws.amazon.com/ko/sdk-for-java/\n    - http://bcho.tistory.com/693\n    - https://opentutorials.org/course/608/3006\n- S3 Credentials 설정 방법\n    - https://dwfox.tistory.com/55\n    - https://docs.aws.amazon.com/kokr/sdk-for-java/v2/developer-guide/setup-credentials.html",
    "category": "java",
    "tags": [
      "s3",
      "amazon",
      "sdk",
      "bucket",
      "버킷",
      "아마존"
    ],
    "date": "2018-07-29T00:00:00.000Z"
  },
  {
    "id": "java/자바-garbage-collection이란",
    "slug": "java/자바-garbage-collection이란",
    "title": "자바 Garbage Collection이란",
    "excerpt": "",
    "content": "1. 가비지 컬랙션이란? C/C++ 언어와 달리 자바는 개발자가 명시적으로 객체를 해제할 필요가 없습니다. 자바 언어의 큰 장점이기도 합니다. 사용하지 않는 객체는 메모리에서 삭제하는 작업을 Gargabe Collection(GC)라고 부르며 JVM에서 GC를 수행합니다. 기본적으로 JVM의 메모리는 총 5가지 영역(ex. 클래스, 스택, 힙, 네이티브 메서드, PC)으로 나뉘는데, GC는 힙 메모리만 다룹니다. 코드상에서 어떨 때 객체가 가비지 대상이 될까요? 간단하게 생각해보면 프로그램이 실행되면서 코드상에서 참조되지 않는 객체들이 대상이 될 것입니다. 일반적으로 아래 같은 경우에 가비지대상이 됩니다. - 객체가 null인 경우 (ex. String str = null)\n- 블럭 안에서 생성된 객체는 블럭 실행 종료후 대상이 된다\n- 부모 객체가 null이 되면, 포함하는 자식 객체들도 자동으로 가비지 대상이 된다 JVM에서 가비지 대상을 어떻게 결정하는지는 링크 #3을 참조해주세요. 1.1 Heap 영역의 구조 Heap 영역은 크게 2가지 영역으로 나뉩니다. Permanent Generation 영역은 Heap 영역은 아닙니다. - Young Generation - 객체 사용 시간이 짧은 객체들\n    - 영역의 종류\n        - Eden\n        - Survivor 2개\n    - 새롭게 생성한 객체는 여기에 위치한다\n    - 매우 많은 객체가 Young 영역에 생성되었다가 사라진다\n    - 이 영역에서 객체가 살아지면 Minor GC가 발생했다고 한다\n- Old Generation (Tenured space) - 오래 사용되는 객체들\n    - Young 영역에서 살아남은 객체가 여기로 복사된다\n    - Young 영역보다 크게 메모리가 크게 할당되어 Young 영역보다 GC는 적게 발생한다\n    - 이 영역에서 객체가 살아지면 Major GC (Full GC)가 발생했다고 한다\n- (Non-heap) Permanent Generation\n    - 이 영역에는 JVM에 의해서 사용하는 클래스와 메서드 객체 정보를 담고 있다\n    - JDK8부터는 PermGen은 Metaspace로 교체된다 https://www.journaldev.com/2856/java-jvm-memory-model-memory-management-in-java 일반적으로 객체를 생성하면, Young 영역에 먼저 위치하게 되고 오랫동안 사용되는 객체는 GC 과정을 통해서 Old 영역으로 이동하게 됩니다. https://www.oracle.com/technetwork/java/javase/memorymanagement-whitepaper-150215.pdf Heap 영역을 왜 두 가지 영역으로 나뉘서 관리하게 되었을까요? 여러 연구를 진행한 결과 애플리케이션에서 객체가 생성되고 살아지는 패턴은 크게 2가지 특징을 가지게 된다고 합니다. - 대부분의 생성된 객체는 금방 사용하지 않는다\n- 객체들은 대개 (아주) 오랜 시간 동안 남아 있지 않는 것들이다 (객체 짧게 사용됨) 아래 그래프에서 보이는 것처럼 객체의 라이프는 짧게 사용되다가 오랫동안 남은 것들은 계속 쌓이게 되는 것을 볼 수 있습니다. 이런 특징으로 두 영역으로 나뉘어서 관리하고 GC 알고리즘도 이 기반으로 설계되었습니다. 2. Garbage Collection 타입 각 영역에 따라서 실행되는 GC는 다릅니다. Minor나 Major GC가 실패하게 되면 Full GC가 발생할 수도 있습니다. - Minor GC\n    - 대상 : Young 영역\n    - 트리거 되는 시점 : Eden이 full이 경우에\n- Major GC\n    - 대상 : Old 영역\n    - 트리거 되는 시점 : Minor GC가 실패하는 경우\n- Full GC\n    - 대상 : 전체 Heap + MetaSpace(Permanent 영역)\n    - 트리거 되는 시점 : Minor나 Major GC가 실패하는 경우 3. Garbage Collection 알고리즘 GC 알고리즘은 오랫동안 개선됐고 아래와 같이 여러 종류로 발전해 왔습니다. 최근 자바에서 기본적으로 사용되는 GC 알고리즘은 G1 GC를 사용합니다. 각각의 알고리즘이 어떻게 동작하는지 알아보겠습니다. - Serial\n- Parallel\n- Parallel Old(Parallel Compacting GC)\n    - JDK5u6부터 제공\n- Concurrent Mark & Sweep (CMS)\n- G1(Garage First)\n    - JDK7u4부터 도입\n    - JDK9부터 기본 GC로 변경됨 GC에서 자주 사용되는 용어로 stop-the-world가 있습니다. GC를 실행하면 JVM이 애플리케이션 실행을 멈추게 되는데, 이를 stop-the-world라고 합니다. GC가 일어나면 GC를 실행하는 쓰레드를 제외한 나머지 쓰레드는 모두 멈추게 됩니다. 이런 멈추는 시간에 의해 애플리케이션 성능에 많은 영향을 주게 됩니다. 여러 GC 알고리즘에서 이 부분을 개선하려고 큰 노력을 해왔습니다. 3.1 Serial (-XX:+UseSerialGC) Serial collector는 single 쓰레드로 동작하며 Young와 Old를 serial 하게 GC을 합니다. Young과 Old 영역에서 객체가 어떻게 관리되는지는 조금 더 구체적으로 알아보겠습니다. - Young 영역 (single thread)\n    - mark and copy\n- Old 영역 (single thread)\n    - mark-sweep-compact : 안쓰는 객체를 표시한 이후 삭제하고 한 곳으로 모으는 알고리즘이다 Young 영역의 Minor GC 절차 - mark and copy - 처음에 생성된 객체는 Eden에 쌓인다\n- Eden이 어느 정도 쌓이면 GC가 발생하고 살아남은 객체는 Survisor(Empty) 영역으로 이동한다\n    - Survisor 영역중에 한 영역은 반드시 비어 있어야 한다\n- Survisor 영역이 차게 되면 GC가 발생하고 Eden 영역에 있는 객체와 꽉 찬 Survisor 영역에 있는 객체가 비어 있는 다른 Survisor 영역으로 이동한다\n- 이 과정을 반복하다가 계속 살아남아 있는 객체들은 Old 영역으로 이동한다 | GC 전 | GC 이후 |\n| ----- | ------- |\n| | | 3.2 Parallel (-XX:+UseParallelGC) Parallel collector는 serial collector의 동작과 유사합니다. 다른 점은 GC 속도를 높이기 위해 Young 영역을 multiple 쓰레드로 GC를 수행합니다. 이로 인해 stop-the-world하는 시간이 줄려 애플리이케이션 성능을 개선하였습니다. - Young 영역 (multi thread)\n    - mark and copy\n- Old 영역 (single thread)\n    - mark-sweep-compact 3.3 Parallel Compacting Collector (- XX:+UseParallelOldGC) Parallel compacting collector는 JDK5u6부터 제공되었으면 JDK7u4부터는 XX:+UseParallelGC 사용 시에도 -XX:+UseParallelOldGC로 설정됩니다. Young과 Old 영역이 병렬로 처리됩니다. 쓰레드 개수는 -XX:ParallelGCThreads=n 옵션으로 조정 가능합니다. - Young 영역 (multi thread)\n    - mark and copy\n- Old 영역 (multi thread)\n    - mark-summary-compact\n    - mark : 살아 있는 객체를 식별하여 표시한다\n    - summary : 이전에 GC를 수행하여 컴팩션된 영역에 살아 있는 객체의 위치를 조사한다\n    - compact : 쓰레기 객체들을 수거하고 살아있는 객체들을 한곳에 모은다 3.4 Concurrent Mark Sweep(CMS) (-XX:+UseConcMarkSweepGC) CMS collector는 heap 메모리 영역의 크기가 크고 2개 이상의 프로세서를 사용하는 서버에 적합합니다. XX:+CMSIncrementalMode 옵션은 Young 영역의 GC를 더 잘게 쪼개어 서버의 대기 시간을 줄일 수 있지만, 예기치 못한 성능 저하가 발생할 수 있습니다. CMS는 Old 영역에 대한 compact 작업을 하지 않기 때문에 memory fragmentation이 발생할 수 있습니다. CMS collector의 경우에는 추후 릴리스에서 제거되는 거로 결정이 되었습니다. ( JEP 291 ) - Young 영역 (multi thread)       \t mark and copy - Old 영역 (multi thread)\n    - mark-sweep-remark\n    - initial mark (stop-the-world) : 애플리케이스 코드에서 직접/바로 접근 가능한 객체를 판단하고 initial set을 만든다\n    - concurrent mark : initial 단계에서 만든 set의 객체에서 transitively 접근 가능한 모든 객체를 체크한다\n    - remark (stop-the-world) : concurrent mark 단계에서 변경된 객체를 다시 체크한다\n    - concurrent sweep : 표시한 객체들 삭제한다 3.5 G1 (-XX:+UseG1GC : JDK9부터 기본으로 설정됨) G1 (Garbage First) collector는 메모리가 큰 multi core 머신을 타켓으로 설계되었습니다. G1 GC는 JDK7u4부터 도입 되었고 안정화 기간 거쳐 현재 JDK9에서는 기본 GC로 채택 되었습니다. G1에서는 아래 그림과 같이 heap 메모리 영역을 작은 단위의 region으로 나눠서 관리합니다. 기본 region 개수 수치는 2K(2048)개 공간으로 나눕니다. 예를 들면 Heap Size가 8GB로 지정하면, 각 region의 크기는 4MB (ex. 8192MB/2048 = 4096)가 됩니다. - Young 영역 (multi thread)\n    - -XX:Parallel",
    "category": "java",
    "tags": [
      "gc",
      "garbage",
      "java",
      "garbage collection",
      "자바",
      "컬렉션"
    ],
    "date": "2018-10-17T00:00:00.000Z"
  },
  {
    "id": "java/자바-keystore에-ssl-인증서-import-하기",
    "slug": "java/자바-keystore에-ssl-인증서-import-하기",
    "title": "자바 keystore에 SSL 인증서 import 하기",
    "excerpt": "",
    "content": "1. 들어가며 회사에서 Zencoder API 을 사용하게 되어 자바에서 작업을 시작하려는데, 아래와 같이 SSLHandshakeException이 발생해서 뭔가 문제인지 구글링을 하게 되었습니다. 이미 아시는 분들도 많지만, 다시 한번 정리를 해봤습니다. - Zencoder API 작업 요청 주소 \\ https://app.zencoder.com/api/v2/jobs Exception 발생 화면 2. 개발 환경 실제 작성한 코드는 많지 않고 테스트를 쉽게 하려고 간단하게 유닛 테스트로 작성했습니다. github에 올린 코드를 참조해주세요. - OS : Mac OS\n- IDE: Intellij\n- Java : JDK 1.8\n- Source code : github\n- Software management tool : Maven 3. 해결책 이 문제를 해결하는 방법은 크게 2가지가 있습니다. - 코딩상에서 직접 인증서 유효성 체크 하지 않기 (비추)\n- 해당 인증서 자바 keystore에 저장하기 (추천 방식) 3.1 코드 상에서 직접 인증서 유효성 체크 하지 않기 자바 코드로 인증서 체크를 하지 않도록 HttpsConnection의 설정을 변경하는 방식입니다. 아래 코드에 대한 자세한 설명은 생략하도록 하겠습니다. 3.2 해당 인증서 자바 keystore에 저장하기 (추천 방식) 자바 keystore에 인증서를 등록하는 방법에는 크게 2가지가 있습니다. 명령어 창에서 하던지 아니면 Portecle GUI 프로그램을 사용해도 상관없습니다. 3.2.1 Portecle GUI 사용하기 자바 keystore에 인증서를 등록하기전에 유닛 테스트를 실행하면, SSLHandshakeException이 발생합니다. Portecle 은 keystore를 관리해주는 자바로 짠 GUI 프로그램입니다. 자바로 짜여 있어서 플롯폼 상관없이 어디서든 실행할 수 있습니다. 1. 다운로드 한후 압축 풀기 아래 링크에서 프로그램을 다운로드한 후 압축을 원하는 폴더에 풀어줍니다. https://sourceforge.net/projects/portecle/files/latest/download 2. Portecle 실행 인증서 등록 후 저장 시 root 권한 필요하므로 sudo로 프로그램을 실행합니다. 3. 접속 사이트에서 인증서를 다운로드합니다. 메뉴에서 Examine > Examine SSL/TSL Connection… 을 클릭하고 접속하려는 사이트 주소를 입력 후 OK 버튼 을 클릭합니다. 클릭 후에 인증서를 볼 수 있습니다. 이 내용을 저장하려면 PEM Encoding 버튼 클릭 후 Save 버튼 을 눌려 저장합니다. 4. 자바 keystore에 등록하기 원하는 자바 버전의 \\$JAVAHOMElibsecurity/cacerts 파일을 열어서 새로운 인증서를 추가하고 저장하면 끝납니다. 설치된 자바 홈 폴더를 확인하고 싶으면 javahome 명령어 로 확인할 수 있습니다. 메뉴에서 열기 버튼을 클릭해서 cacerts 파일을 찾아 오픈하면 암호를 입력하게 되어 있습니다. 디폴트 암호 값은 changeit 입니다. 현재 등록된 인증서 목록입니다. 새로운 인증서를 추가하기 위해 메뉴 임포트 버튼을 클릭하고 다운로드한 인증서를 선택합니다. 파일 선택 이후 여러 질문에 Yes 버튼 을 클릭하면 새로운 인증서가 추가된 것을 목록에서 확인할 수 있습니다. 다시 유닛 테스트를 실행하면 Exception 없이 잘 실행되는 것을 확인할 수 있습니다. 자 그면, 명령어 창에서 등록하는 방법을알아보겠습니다. 3.2.2 명령어창에서 자바 keystore에 인증서 임포트하기 명령어 창에서도 인증서를 다운로드하고 등록할 수 있습니다. 1. 인증서 다운로드하기 2. 자바 keystore에 새로운 인증서 추가하기 입력이후 질문이 나오면 yes 를 입력하면 등록이 완료됩니다. 4. 참고 - Java의 keystore에 SSL 인증서 import 하기\n    - https://www.lesstif.com/pages/viewpage.action?pageId=12451848\n    - https://stackoverflow.com/questions/2893819/accept-servers-self-signed-ssl-certificate-in-java-client\n- Certificate 다운로드 방법\n    - https://www.lesstif.com/pages/viewpage.action?pageId=16744456\n    - https://stackoverflow.com/questions/33284588/error-when-connecting-to-url-pkix-path-building-failed",
    "category": "java",
    "tags": [
      "ssl",
      "keystore",
      "import",
      "java",
      "certificate",
      "인증서",
      "자바"
    ],
    "date": "2019-01-09T00:00:00.000Z"
  },
  {
    "id": "java/자바-자료구조-priority-queue-우선순위-큐",
    "slug": "java/자바-자료구조-priority-queue-우선순위-큐",
    "title": "자바 자료구조 - Priority Queue (우선순위 큐)",
    "excerpt": "",
    "content": "1.Priority Queue (우선순위 큐)란? 자바에서 제공하는 여러 자료구조 중에 에 대해서 알아보자. 우리가 잘 알고 있는  자료구조와 같이  (First-In-First-Out) 알고리즘으로 동작하지만, 추가로 우선순위가 있는  (Best-In-First-Out) 알고리즘으로 동작한다고 보면 된다. 기본 자료구조에서는 Heap (Min, Max) 자료구조로 보면 이해가 쉽다. PriorityQueue 클래스는 큐 인터페이스를 구현한 구현체이고 이제 어떻게 사용하는지 예제를 통해서 알아보자. > 클래스 다이어그램은 GeeksforGeeks에서 발취함 1.1 기본 메서드 및 복잡도  클래스에서 제공하는 기본 메서드와 복잡도에 대한 설명이다. | 메서드 이름              | 복잡도     | 설명                                             |\n| ------------------------ | ---------- | ------------------------------------------------ |\n|  | O(N log N) | 우선순위에 따라서 큐에 삽입한다.                 |\n|           | O(1)       | 큐의 first 아이템을 제거하지 않고 확인할 수 있다 |\n|           | O(1)       | 큐의 first 아티엠을 제거하고 데이터를 반환한다   | 메서드에 대한 더 자세한 내용은 Oracle Java API를 참고해주세요. 2. Priority Queue 사용법 2.1 Min Heap  객체를 추가 인자 없이 생성하면 Min Heap으로 동작한다. 예제에서 보면 데이터 추가 이후 데이터를 추출하면 값이 가장 낮은 값이 반환된다. 2.2 Max Heap  객체 생성시 생성자에 Comparator 를 넘겨주어 데이터에 대한 우선순위를 지정할 수 있다. 값 추출시 Integer의 Max 값이 반환되도록 하기 위해 Collections 에서 기본적으로 제공하는  메서드로 지정해준다. 2.3 Min Heap - Student 객체 기본 Integer 값 외에도 객체에서 특정 값이 우선순위를 가지도록 설정할 수 있다. 예제에서는 Student 객체 Age의 가장 낮은 값이 먼저 추출되도록 설정하였다. Comparator를 인자로 넘겨주기 위해  메서드를 사용하였다.  메서드는 인자로 필드 값 기준으로 Comparator 메서드 함수를 반환해주어 쉽게 Comparator를 생성할 수 있다. 3. 마무리 은 우선순위를 가지는 Queue 자료구조이다. 개발 시 를 사용하면 쉽게 Min+Max Heap 자료구조로 사용할 수 있다. 예제 코드는 github를 참고해주세요. 4. 참고 - Priority Queue\n    - http://asuraiv.blogspot.com/2015/11/java-priorityqueue.html\n    - https://coding-factory.tistory.com/603\n    - https://woovictory.github.io/2020/05/13/PriorityQueue/\n    - https://lottogame.tistory.com/996\n    - https://www.webucator.com/how-to/how-use-the-comparatorcomparing-method-java-8.cfm\n    - https://medium.com/@logishudson0218/java-collection-framework-896a6496b14a\n- Heap\n    - https://ko.wikipedia.org/wiki/%ED%9E%99(%EC%9E%90%EB%A3%8C%EA%B5%AC%EC%A1%B0)\n- Comparator.comparing\n    - https://www.webucator.com/how-to/how-use-the-comparatorcomparing-method-java-8.cfm",
    "category": "java",
    "tags": [
      "java",
      "queue",
      "priority",
      "priority queue",
      "heap",
      "자바",
      "자료구조",
      "우선순위 큐",
      "큐",
      "힙"
    ],
    "date": "2020-09-20T00:00:00.000Z"
  },
  {
    "id": "java/자바-커스텀-어노테이션-만들기",
    "slug": "java/자바-커스텀-어노테이션-만들기",
    "title": "자바 커스텀 어노테이션 만들기",
    "excerpt": "",
    "content": "1. 어노테이션이란 스프링 프레임워크를 사용하면 어노테이션을 자주 사용하게 됩니다. 아래는 스프링 웹 MVC를 사용한 예로 GET HTTP 요청(/helloworld)이 있으면 “Hello World”를 담아서 뷰에 전달되는 코드입니다. 이런 어노테이션은 내부적으로 어떻게 코드화되어 사용되는지 알아봅시다. 자바 어노테이션은 JDK5부터 추가된 기능입니다. 어노테이션은 자바 소스코드에 추가적인 정보를 제공하는 메타데이터입니다. 어노테이션은 클래스, 메서드, 변수, 인자에 추가할 수 있습니다. 메타 데이타이기 때문에 비즈니스 로직에 직접적인 영향을 주지 않지만, 이 메타데이터 정보에 따라서 실행 흐름을 변경할 수 있는 코딩이 가능하여 단지 어노테이션 추가만으로 더 깔끔한 코딩이 가능해집니다. 위 예제를 어노테이션 없이 코딩하려면 코딩량도 길어지고 가독성도 많이 떨어지겠죠. 자, 기본적인 어노테이션 선언과 어디에 사용할 수 있는지 간단하게 알아봅시다. 본 포스팅에 작성한 예제 코드는 github 에 작성되어 있습니다. 2. 어노테이션의 기본 사용 2.1 어노테이션 타입 어노테이션은 3가지 타입이 존재합니다. - 마커 어노테이션 (Maker Annotation)\n    - @NewAnnotation\n- 싱글 값 어노테이션 (Single Value Annotation)\n    - @NewAnnotation(id=10)\n- 멀티 값 어노테이션 (Multi Value Annotation)\n    - @NewAnnotation(id=10, name=“hello”, roles= {“admin”, “user\"}) 2.1.1 마커 어노테이션 @Override 나 @Deprecated와 같은 어노테이션처럼 표시만 해두는 어노테이션입니다. 메서드없이 선언하면 마커 어노테이션이 됩니다. 추가 정보 없이 클래스나 메서드위에 추가합니다. 2.1.2 싱글 값 어노테이션\n하나의 값만 입력받을 수 있는 어노테이션입니다. 어노테이션 선언에 하나의 메서드만 있으면 싱글 값 어노테이션으로 선언됩니다. 2.1.3 멀티 값 어노테이션\n어노테이션에 여러 값들을 지정할 수 있습니다. 여러 값을 입력받으려면 여러 메서드를 선언하면 됩니다. 추가로 default 키워드로 기본값을 선언할 수 있습니다. 위 코드 라인 #4번에 user와 roles을 지정하지 않아 기본값으로 name = “users”, roles = {“anonymous”}로 지정되었습니다. 2.2 어노테이션 배치하는 곳\n아래 예제처럼 어노테이션은 클래스, 필드 변수, 메서드 인자, 로컬변수위에 선언할 수 있습니다. 커스텀 어노테이션을 어떻게 생성하고 사용할 수 있는지 알아보기 전에 기본적으로 자바에서 제공하는 어노테이션을 한번 살펴보겠습니다. 3. 빌드인 어노테이션\n자바 언어에서 제공되는 어노테이션들입니다. - 자바 코드에 적용되는 어노테이션\n    - @Override\n        - 어버라이드되는 메서드로 표시하는 역할을 한다\n        - 어노테이션을 추가한 메서드가 부모 클래스나 인터페이스에 존재하지 않으면 컴파일 오류를 발생시킨다\n    - @Deprecated\n        - 메서드를 더 이상 사용하지 않음으로 표시한다\n        - 메서드가 사용되면 컴파일 경고를 발생시킨다\n    - @SuppressWarnings\n        - 컴파일시 발생하는 경고를 무시하도록 컴파일에게 알려주는 역할을 한다 - 자바7이후부터 추가된 어노테이션\n    - @SafeVarargs\n    - 자바7에 추가된 어노테이션이다\n        - 메서드가 가변인자인 경우에 잘 못 실행될 수 있는 경고 문구를 무시하도록 하는 어노테이션이다.\n                        어버라이드가 안되는 메서드에만 사용 가능하다\n                 final, static 메서드, 생성자, private 메서드 (자바9부터)\n             예제 코드 참조\n         @FunctionalInterface\n             자바8부터 추가된 어노테이션으로 함수 인터페이스로 선언할때 사용된다\n         @Repeatable\n             같은 어노테이션을 여러번 선언할 수 있도록 해주는 어노테이션이다\n              \n             예제 코드 참조 - 다른 어노테이션에 적용되는 어노테이션 - 메타 어노테이션(Meta Annotation)     - @Retention\n    - @Documented\n    - @Target\n    - @Inherited 위 메타 어노테이션은 커스텀 어노테이션을 작성할 때 사용하는 어노테이션입니다. 각각 어떤 역할을 하는지는 다음 섹션에서 알아보도록 하겠습니다. 4. 커스텀 어노테이션 4.1 메사 어노테이션 2개의 String 값을 받을 수 있는 간단한 커스텀 어노테이션 예제입니다. 각각의 메타 어노테이션이 어떤 의미를 가지고 있는지 알아볼게요. - @Target\n    - 이 어노테이션은 선언한 어노테이션이 적용될 수 있는 위치를 결정한다\n    - ElementType Enum에 선언된 값\n        - TYPE : class, interface, enum에 적용된다.\n        - FIELD : 클래스 필드 변수\n        - METHOD : 메서드\n        - PARAMETER : 메서드 인자\n        - CONSTRUCTOR : 생성자\n        - LOCALVARIABLE : 로컬 변수\n        - ANNOTATIONTYPE : 어노테이션 타입에만 적용된다\n        - PACKAGE : 패키지\n        - TYPEPARAMETER : 자바8부터 추가된 값으로 제네릭 타입 변수에 적용된다. (ex. MyClass<T>)\n        - TYPEUSE : 자바8부터 추가된 값으로 어떤 타입에도 적용된다 (ex. extends, implements, 객체 생성시등등)\n        - 자바8 타입 어노테이션\n        - MODULE : 자바9부터 추가된 값으로 모듈에 적용된다\n- @Retention\n    - 어노테이션이 어느레벨까지 유지되는지를 결정짓는다.\n    - RetentionPolicy Enum에 선언된 값\n    - SOURCE : 자바 컴파일에 의해서 어노테이션은 삭제된다\n    - CLASS : 어노테이션은 .class 파일에 남아 있지만, runtime에는 제공되지 않는 어노테이션으로 Retention policy의 기본 값이다 \\ RUNTIME : runtime에도 어노테이션이 제공되어 자바 reflection으로 선언한 어노테이션에 접근할 수 있다\n- @Inherited\n    - 이 어노테이션을 선언하면 자식클래스가 어노테이션을 상속 받는다\n- @Documented\n    - 이 어노테이션을 선언하면 새로 생성한 어노테이션이 자바 문서 생성시 자바 문서에도 포함시키는 어노테이션이다.\n- @Repeatable\n    - 자바8에 추가된 어노테이션으로 반복 선언을 할 수 있게 해준다 4.2 커스텀 어노테이션 생성 커스텀 어노테이션을 이용해 생성한 예제들입니다. 예제1 - 클래스에 선언 예제2 - 클래스 필드에 선언 예제3 - 메서드에 선언 4.3 자바 리플렉션으로 커스텀 어노테이션 사용해보기\n프로그램 실행 시 커스텀 어노테이션을 사용한 곳과 지정한 값들을 얻어오려면 자바 리플렉션을 사용해야 합니다. 자바 리플렉션을사용해서 선언한 어노테이션 값을 얻어오는 건 비슷비슷해서 예제 3번으로만 설명하도록 하겠습니다. 커스텀 어노테이션을 어떻게 생성하고 사용하는지 간단하게 알아보았습니다. 커스텀 어노테이션을 생성해서 사용하면 반복적으로 코딩해야 하는 부분들도 많이 줄일 수 있고 더 비즈니스로직에 집중할 수 있는 장점이 있습니다. 스프링에서도 자주 사용되고 또한 요사이 많이 뜨고 있는 롬보크( Lombok )도 여러 어노테이션을 많이 지원하는 라이브러리입니다. 지금 개발하는 프로젝트가 있다면 한번 커스컴 어노테이션으로 적용해보는 것도 좋을 것 같습니다. 5. 참고 - 자바 어노테이션\n    - https://ko.wikipedia.org/wiki/%EC%9E%90%EB%B0%94\\%EC%96%B4%EB%85%B8%ED%85%8C%EC%9D%B4%EC%85%98\n    - http://tutorials.jenkov.com/java/annotations.html\n    - https://jdm.kr/blog/216\n    - https://www.javatpoint.com/custom-annotation\n    - https://elfinlas.github.io/2017/12/14/java-annotation/\n    - https://howtodoinjava.com/java/annotations/complete-java-annotations-tutorial/\n- SafeVarargs 어노테이션\n    - https://beginnersbook.com/2018/05/java-9-safevarargs-annotation/\n- Repeable 어노테이션\n    - https://dzone.com/articles/repeatable-annotations-in-java-8-1\n    - https://www.javabrahman.com/java-8/java-8-repeating-annotations-tutorial/\n- 사용예\n    - http://www.nextree.co.kr/p5864/\n    - https://examples.javacodegeeks.com/core-java/java-9-annotations-example/\n    - https://elfinlas.github.io/2017/12/14/java-custom-anotation-01/\n    - http://tutorials.jenkov.com/java-reflection/annotations.html\n    - https://elfinlas.github.io/2017/12/14/devnote01/",
    "category": "java",
    "tags": [
      "java",
      "annotation",
      "자바",
      "어노테이션"
    ],
    "date": "2018-11-18T00:00:00.000Z"
  },
  {
    "id": "java/자바8-compable-comparator-차이점",
    "slug": "java/자바8-compable-comparator-차이점",
    "title": "자바 Comparable과 Comparator의 차이점",
    "excerpt": "",
    "content": "자바에서 객체 정렬 시 사용되는 Comparator와 Comparable 인터페이스 간의 차이점을 알아보겠습니다. 예제로 작성한 코드는 github java-compare 모듈을 참고해주세요. Comparable vs. Comparator 두 인터페이스 모두 컬렉션을 정렬할 때 정렬 규칙을 설정하는 데 사용되는데, 차이점은 아래와 같습니다. - Comparable\n    - 정렬할 대상 객체에 Comparable 인터페이스를 implements해서 compareTo() 메서드를 구현해야 한다\n    - compareTo() 메서드는 this와 o 객체의 차이점에 따라서 int 값을 반환하는 메서드이고 이 반환 값에 따라서 정렬이 이루어진다\n        - 1 : this > o , this가 o보다 큰 경우\n        - -1 : this < o this가 o보다 작은 경우\n        - 0 : this == o, 두 객체 같은 경우 - Comparator     - 정렬할 대상 객체를 직접 수정할 수 없는 경우에 Comparator 인터페이스를 사용해서 정렬할 수 있다\n        - compare() 메서드는 o1, to2 객체의 차이점에 따라서 int값을 반환하는 메서드이고 이 반환 값에 따라서 정렬이 이루어진다\n            - 1 : o1 > o2 첫번째 객체가 큰 경우\n            - -1 : o1 < o2 첫번째 객체가 작은 경우\n            - 0 :  o1 == o2 두 객체가 같은 경우 1. 정렬할 대상 객체 Comparable의 경우에는 객체 자체에 Comparable 인터페이스를 구현해야 해서 Comparator와 구분하기 위해서 각각 다른 객체로 생성했습니다. 1.1 Comparable 1.2 Comparator 2. 객채 정렬하기 2.1 Collections.sort()로 정렬하기 Collections의 sort() 메서드는 2가지 메서드를 제공합니다. Comparable 객체의 List를 정렬할 수 있는 메서드와 Comparator 객체를 인자로 넘겨서 정렬할 수 있는 메서드를 제공합니다. Comparable 객체의 리스트를 정렬하고 isSorted() 메서드로 정렬이 되었는지 확인할 수 있습니다. 정렬 규칙을 Comparator로 넘겨서도 List를 정렬할 수 있습니다. 2.2 Stream의 sorted()로 정렬하기 Stream에서는 sorted() 메서드를 사용해서 정렬할 수 있습니다. sorted() 메서드는 comparator 인터페이스를 구현한 객체를 인자로 받습니다. List를 stream으로 변환해서 sorted() 메서드를 사용해서 정렬한 예제입니다. 정렬 시 객체의 여러 값을 기준으로도 정렬을 할 수 있습니다. Score가 같은 경우 발생 시에는 name 기준으로 정렬하는 예제입니다. ComparatorPlayer 객체에 compareByScoreThenName() 메서드가 있어서 method reference로 더 간결하게 작성할 수도 있습니다. 2.3 자바8 Comparators 자바8에서는 comparing() 팩토리 메서드가 추가되어 Comparator를 쉽게 정의하도록 도와줍니다. Comparator.comparing() 메서드는 항목을 비교하는데 사용할 객체의 필드 메서드를 인자로 넘겨주면 일치하는 Comparator 인스턴스를 반환해줍니다. 즉, 비교할 필드 메서드만 제공하면 알아서 Comparator 구현체를 반환해주는 것입니다. sortbyscorethenname 예제에서 score와 name 두 값을 비교해서 정렬한 것처럼 comparing() 메서드와 thenComparing() 메서드를 사용하면 동일하게 정렬할 수 있습니다. 더 코드가 간결해졌습니다. 참고  https://www.baeldung.com/java-8-sort-lambda\n https://www.daleseo.com/java-comparable-comparator/\n https://www.baeldung.com/java-comparator-comparable\n https://gmlwjd9405.github.io/2018/09/06/java-comparable-and-comparator.html\n https://www.leveluplunch.com/java/tutorials/007-sort-arraylist-stream-of-objects-in-java8/\n https://velog.io/@agugu95/Java-%EA%B0%9D%EC%B2%B4-%EC%A0%95%EB%A0%AC%EA%B3%BC-%EB%B9%84%EA%B5%90",
    "category": "java",
    "tags": [
      "java",
      "java8,",
      "comparator",
      "comparing",
      "sort",
      "자바",
      "자바8",
      "정렬"
    ],
    "date": "2020-04-15T00:00:00.000Z"
  },
  {
    "id": "java/자바8-hashmap-보다-간결하고-효과적으로-작성하기",
    "slug": "java/자바8-hashmap-보다-간결하고-효과적으로-작성하기",
    "title": "자바8 HashMap 보다 간결하고 효과적으로 작성하기",
    "excerpt": "",
    "content": "자바8부터 에 여러 메서드들이 추가되었고 이런 메서드를 사용해서 을 조금 더 간결하면서 효율적으로 사용하는 방법에 대해서 알아보겠습니다. - \n- \n- \n- \n- \n-  작성된 코드는 java8-hashmap을 참고해주세요. 1. putIfAbsent() vs. computeIfAbsent() 2가지 메서드의 공통점은 key의 존재 여부에 따라서 새로운 key와 value 값을 추가하는 메서드입니다. putIfAbsent putIfAbsent는 2개의 인자를 받습니다. - key : Map의 key 값\n- value :  value 값\n- 반환 값\n    - key 값이 존재하는 경우\n        - Map의 value 값을 반환한다\n    - key 값이 존재하지 않는 경우\n        - key와 value를 Map에 저장하고 null을 반환하다 computeIfAbsent  2개의 인자를 받습니다. - key : Map의 키 값\n- mappingFunction\n    -  람다 함수는 key 값이 존재하지 않을 때만 실행된다\n- 반환값\n    - key 값이 존재하는 경우\n        - map안에 있는 value을 반환한다\n    - key 값이 존재하지 않는 경우\n        - Map에 새로운 key와 value( 람다 함수를 실행한 결과) 값을 저장한다 2. compute() vs. computeIfPresent() vs merge() 3개의 메서드들은 모두 Map의 value 값을 업데이트할 때 사용됩니다. compute  key와  인자로 받고 key가 존재해야, value값을 인자로 넘겨준  람다 함수의 결과로 업데이트가 됩니다. key 값이 존재하지 않는 경우에는  발생합니다. computeIfPresent 결과 값 - key 값이 존재하는 경우\n    -  람다 함수 실행 결과로 value 값이 업데이트가 된다\n- key가 존재하지 않는 경우\n    - null을 반환한다 merge 결과 값 - key 값이 존재하는 경우     - Case 1 :  람다 함수의 결과가 null 아니면\n        -  람다 함수 실행 결과로 value 값이 업데이트가 된다\n    - Case 2 :  람다 함수의 결과가 null 이면\n        - map에서 해당 key를 삭제한다 - key가 존재하지 않는 경우     - Map에 key, value값이 추가된다 3. getOrDefault()  가 반환하는 값은 아래와 같습니다. - key 값이 존재하는 경우\n    - Map의 value값을 반환한다\n- key 값이 존재하지 않는 경우\n    - 을 반환한다 참고  http://tech.javacafe.io/2018/12/03/HashMap/\n https://www.baeldung.com/java-hashmap\n https://docs.oracle.com/javase/8/docs/api/java/util/Map.html",
    "category": "java",
    "tags": [
      "java",
      "java8",
      "map",
      "hashmap",
      "compute",
      "자바",
      "자바8",
      "맵",
      "putIfAbsent",
      "computeIfPresent",
      "compute",
      "putIfAbsent"
    ],
    "date": "2020-03-01T00:00:00.000Z"
  },
  {
    "id": "java/자바8-optional이란",
    "slug": "java/자바8-optional이란",
    "title": "자바8 Optional이란",
    "excerpt": "",
    "content": "1. Optional이란 Optional은 null을 대신하기 위해 만들어진 새로운 코어 라이브러리 데이터 타입입니다. Optional 클래스는 null이나 null이 아닌 값을 담을 수 있는 클래스입니다. 이미 다른 언어(ex. Scala)에 존재하는 기능으로 자바에서는 JDK8에 포함 되었습니다. 기존에 null을 사용하면 어떤 문제점이 있고 Optional을 통해서 어떻게 코드가 개선되는지 알아봅시다. 자바로 개발하다 보면 NullPointerException(NPE)을 자주 만나게 됩니다. 아래 코드와 같이 객체가 null이고 null이 된 값을 쓰게 되면 NPE가 발생합니다. NPE를 해결하기 위해서는 null에 대한 조건문을 추가해야 합니다. Null은 값이 없음을 나타내려는 의도로 개발되었지만, null를도입함으로써 코드의 가독성이 많이 떨어지고 유지보수가 어렵다는 문제점만 가지게 되었습니다. Optional 클래스를 사용하면 코드가 어떻게 달라지는지 확인해보겠습니다. Null 체크하는 조건문이 없어지면서 코드가 훨씬 깔끔해집니다. 2. Optional 사용해보기 2.1 Optional 객체 생성하기 Optional 클래스에서는 3가지 정적 팩토리 메서드를 제공합니다. - Optional.empty() : 빈 Optional 객체 생성한다\n- Optional.of(value) : value값이 null이 아닌 경우에 사용한다\n- Optional.ofNullable(value) : value값이 null인지 아닌지 확실하지 않은 경우에 사용한다 1. Optional.empty() Optional.empty()는 empty Optional 객체를 생성합니다. 2. Optional.of(value) Optional.of()는 null이 아닌 객체를 담고 있는 Optional 객체를 생성합니다. null이 아닌 객체를 생성하기 때문에 null을 넘겨서 생성하면 NPE이 발생하게 됩니다. 3. Optional.ofNullable(value) null인지 아닌지 확신할 수 없는 객체를 담고자 할 때Optional.ofNullable()를 통해서 Optional 객체를 생성합니다. null이 넘어 올 경우에는 empty Optional 객체를 생성합니다. 2.3 Optional이 담고 있는 객체에 접근 및 사용방법 Optional이 담고 있는 객체에 접근하는 여러 메서드의 사용방법에 대해서 알아보겠습니다. - ifPresent(함수)\n- null인 경우에 default 값 반환\n    - orElse(T other) : 비어 있으며 인자를 반환한다\n    - orElseGet(Supplier<? extends T> other) : 함수형 인자의 반환값을 반환한다\n- null인 경우에 예외를 throw\n    - orElseThrow(Supplier<? extends X> exceptionSupplier) : 인자 함수에서 생성된 예외를 던진다 1. IfPresent(함수)\nOptional 객체가 non-null이 경우에 인자로 넘긴 함수를 실행하는 메서드입니다. Optional 객체가 null이면 인자로 넘긴 함수는 실행되지 않습니다. 2. orElse\nOptional에 담고 있는 객체가 null이 경우에 대신할 수 있는 값을 반환하는 메서드입니다. 3. orElseGet\norElse와 유사하지만, 다른 점은 메서드를 인자로 받고 함수에서 반환한 값을 반환하게 되어 있습니다. 4. orElse와 orElseGet의 차이점\n결과적으로 사용하는데 orElse와 orElseGet의 차이점이 없어 보이지만, 아래 코드를 보면 orElseGet()의 경우에는 null인 경우에만 인자로 넘긴 함수가 실행되는 것을 알 수 있습니다. orElse 메서드 사용 시에만 주의하면 됩니다. 5. orElseThrow\nnull인 경우에 기본 값을 반환하는 대신 예외를 던질 수 있습니다. 3. Stream에서 Optional 사용법 3.1 filter(Predicate) : 조건에 따라 요소들 필터링하기 filter()는 인자로 받은 Predicate 함수의 결과가 true인 모든 요소만을 새로운 스트림으로 반환하는 Stream API입니다. 즉, 조건에 맞는 것만 필터링한다고 이해하시면 됩니다. Optional 없이 구현한 버전과 Optional을 사용한 예제입니다. Stream의 여러 API를 더 잘 이해하기 위해서는 실제 구현 코드를 보면 람다 함수가 내부적으로 어떻게 호출되는지 이해하기가 더 쉽습니다. filter는 Predicate 함수를 인자로 넘겨주고 스트림에서Predicate으로 넘긴 함수를 실행하고 그 결과가 true이면 스트림의 this를 넘기고 아닌 경우에는 실제 반환하는 결과는 Optional 타입임을 확인할 수 있습니다. 참고로 Predicate<? super T>의 의미는 Predicate의 유형 매개변수가 T 또는 T의 상위유형이어야 한다는 의미이다. T에 들어갈 수 있는 lower bound가 정해집니다. 예를 들면, List<? super Integer>인 경우에는 List<Integer>, List<Number>, List<Object>가 포함됩니다. Integer 클래스는 Number와 Object를 상속받은 자식 클래스입니다. 제네릭에 대한 더 자세한 내용은 StackOverflow 를 참조해주세요. 3.2 map() : 요소들의 값 형태 변환하기 map(Function<? super T,? extends R> mapper) map()은 요소 값의 형태를 변환하는 Stream API입니다. 아래 코드에서는 String 리스트를 Optional로 랩핑해서 map으로 size를 얻어올 수 있습니다. null인 경우에는 orElse() 메서드로 default 값을 반환합니다. filter() 함수의 경우에는 Predicate 함수 인터페이스이기 때문에 결과를 boolean을 반환해야 하지만, map() 함수는 Function 함수 인터페이스를 사용하기 때문에 여러 형태의 결과로 반환할 수 있습니다. 코드에서는 List를 int 형태로 변환해서 반환합니다. 3.3 flatMap() : 요소를 flat하고나서 map으로 값 형태를 반환하기 flatMap의 경우에는 요소가 Primitive 타입이 아니라 여러 Optional<Optional>>으로 된 타입인 경우에는 flatMap을 사용해야 합니다. Optional의 외에 Stream인 경우에도 [[1,2,3], [2,3,4]]이런 형태의 데이터는 flatMap을 사용해서 처리합니다. 더 자세한 설명은 아래 링크를 참조해주세요 (Map Vs. FlatMap) 4. 자바9에 추가된 Optional 메서드 JDK9에서 3가지 메서드가 추가되었습니다. 각각 기존 사용을 알아봅시다. - or() : Optional이 empty인 경우에 다른 Optional을 반환한다 - ifPresentOrElse() : Optional에 값이 존재하면 action 수행하고 아닌 경우에는 else 부분을 수행한다 - stream() : Optional 객체를 Stream 객체로 변환하기 위해 사용된다 4.1 or() : Optional이 empty인 경우에 다른 Optional을 반환 JDK9 이전에는 Optional 객체가 empty이면 orElse()나orElseGet()을 사용해서 default 값을 반환했습니다. JDK9부 터는 or() 메서드로 Optional이 empty이면 값 대신 다른 Optional을 반환하는 메서드가 추가되었습니다. 예제를 보면 더 쉽게 이해할 수 있습니다. 4.2 ifPresentOrElse() : Optional에 값이 존재하면 action 수행하고 아닌 경우에는 else 부분을 수행 JDK9이전에는 IfPresent 메서드만 있었다면 JDK9부터는 Optional이 empty인 경우에 Else로 추가되어 많이 편리해졌습니다. 4.3 stream() : Optional 객체를 Stream 객체로 변환하기 위해 사용 JDK8에 추가된 Stream은 여러 API를 통해 collection을 함수형 방식으로 쉽게 조작이 가능하게 하는 기능입니다. Optional에 stream()을 추가함으로써 기존의 Stream API를 사용 할 수 있게 되었습니다. 이 예제에서는 Optional을 Stream으로 변경한이후 Stream의 함수를 사용한 예제입니다. 5. 참고 포스팅 작성된 소스 코드는 github 에서 확인가능합니다. - 함수형 인터페이스 - Scala에서 Option\n    - https://danielwestheide.com/blog/2012/12/19/the-neophytes-guide-to-scala-part-5-the-option-type.html\n- Optional 사용법\n    - http://www.daleseo.com/java8-optional-after/\n    - https://softwareengineering.stackexchange.com/questions/364211/why-use-optional-in-java-8-instead-of-traditional-null-pointer-checks\n    - https://www.baeldung.com/java-optional\n- Streams\n    - https://m.blog.naver.com/PostView.nhn?blogId=2feelus&logNo=220695347170&proxyReferer=https%3A%2F%2Fwww.google.co.kr%2F\n- Generis syntax\n    - https://stackoverflow.com/questions/2827585/what-is-super-t-syntax\n- Map vs FlatMap\n    - https://code.i-harness.com/ko-kr/q/1972c92\n    - https://jaepils.github.io/java/2018/06/27/java-time-Instant.html\n- JDK9 optional\n    - https://www.baeldung.com/java-9-optional\n    - https://aboullaite.me/java-9-enhance",
    "category": "java",
    "tags": [
      "java",
      "java10",
      "jdk",
      "optional",
      "openjdk",
      "자바",
      "자바8"
    ],
    "date": "2018-07-29T00:00:00.000Z"
  },
  {
    "id": "java/자바8-stream-api-사용해서-list-of-object-생성하기",
    "slug": "java/자바8-stream-api-사용해서-list-of-object-생성하기",
    "title": "자바8 Stream API 사용해서 List of Object 생성하기",
    "excerpt": "",
    "content": "자바8에 도입된 스트림 API에 조금 더 익숙해지기 위해 loop으로 자주 사용하던 코딩을 스트림 API로 변환해보자. 1. Loop 사용해서 객체 리스트 생성하기 - 자바8 이전 자바8 전 버전에서는 아래와 같은 방식으로 for loop을 사용해서 작성한다. 2. Stream API 사용해서 객체 리스트 생성하기 - 자바8 이후 를 사용해서 for문을 대체할 수 있다. 그리고 는 원시 스트림을 객체 스트림으로 변환해준다. 여기서는 Student 객체를 생성해서 반환하고 최종적으로 List 형으로 생성한다. 정리 for문을 대신하여 Stream API를 사용할 수 있는 간단한 예제를 같이 보았다. 다음 포스팅에도 다양한 스트림 방식으로 코딩하는 방법에 대해서 알아보자. 참고  http://jtuts.com/2017/04/21/create-list-range-integers-using-java-8/\n https://stackoverflow.com/questions/22649978/java-8-lambda-can-i-generate-a-new-arraylist-of-objects-from-an-intstream",
    "category": "java",
    "tags": [
      "java",
      "java8",
      "stream",
      "object",
      "lambda",
      "람다",
      "자바",
      "스트림",
      "객체"
    ],
    "date": "2020-06-29T00:00:00.000Z"
  },
  {
    "id": "java/자바8-스트림-사용해서-list-map-형태로-변환하는-방법",
    "slug": "java/자바8-스트림-사용해서-list-map-형태로-변환하는-방법",
    "title": "자바8 스트림 사용해서 List -> Map 형태로 변환하는 방법",
    "excerpt": "",
    "content": "1. 들어가며 객체 를  형태로 변환할 때 아래와 같이 loop을 돌면서 에 내용을 채운다. 자바8에 도입된 스트림을 사용해서  -> Map으로 어떻게 변환하는지 알아보자. 2. List -> Map 변환 2.1 자바8에서 스트림 사용하여 List에서 Map으로 변환하기 collect()는 스트림의 요소들을 우리가 원하는 자료형으로 변환해준다. Collectors 라는 라이브러리가 기본적인 메서드들을 제공해주는데  형태로 변환해주는 toMap()을 사용해서 List -> Map으로 변환해주면 된다. 예제에서는 Map의 key, value 값이 되는 Student의 name, age를 의 인자로 넘겨주어 실제  자료형으로 만들어 준다. 메서드 참조를 통해 람다 표현 식을 더 간결하게 작성 할 수 있다. id : Student 형태의  자료형으로 변환해주는 예제이다. 2.2 List안에 중복 데이터가 있는 경우 - 예외 발생 List -> Map 으로 변환하는 과정에서 에 중복된 키가 있는 경우에는  예외가 발생한다. 위와 같이 중복이 발생하는 경우를 대비해서 의 3번째 인자에 mergeFunction 메서드를 제공할 수 있다.  인자에는  저장시 중복을 어떻게 처리할 지 로직을 담을 수 있다. >  중복이 있는 경우에는 oldValue 값을 선택한다 3. 마무리 자바 8의 스트림을 사용해서 List -> Map 형태로 변환해주는 방식을 알아보았다. 자바 8전의 코드 스타일보다 스트림 형태로 작성 코드들이 훨씬 더 코드를 빠르게 이해할 수 있는 장점이 있는 듯하다. 전체 소스는 github를 참고해주세요 4. 참고  List -> Map 변환\n     https://mkyong.com/java8/java-8-convert-list-to-map/\n     https://www.baeldung.com/java-list-to-map\n     https://stackoverflow.com/questions/20363719/java-8-listv-into-mapk-v\n Stream\n     https://codechacha.com/ko/java8-stream-collect/",
    "category": "java",
    "tags": [
      "java",
      "java8",
      "stream",
      "list",
      "map",
      "스트림",
      "자바",
      "자바8",
      "리스트",
      "맵"
    ],
    "date": "2020-04-20T00:00:00.000Z"
  },
  {
    "id": "java/자바8-스트림-사용해서-max값-추출하기",
    "slug": "java/자바8-스트림-사용해서-max값-추출하기",
    "title": "자바8 스트림 사용해서 max, min 값 찾기",
    "excerpt": "",
    "content": "1. 들어가며 자바8의 스트림 API를 사용해서 List나 배열에서 max, min 값을 찾는 방법에 대해서 알아보자. 2. 스트림을 사용하여 max 값 찾기 2.1 숫자 List에서 Max 값 찾기 -  : 숫자 List에서 max 값을 찾기 위해서\n-  : Stream에서 Int 원시 스트림으로 변환한다\n- \n    - IntStream에서 제공하는 메서드이다\n    - 결과를 Optional로 반환한다\n- \n    - 결과가 없는 경우에는  예외를 throw한다 IntStream으로 변환하지 않고 Stream에서 제공하는 를 사용할 수도 있다. - \n    - Comparator를 인자 값으로 받는다.\n    - Comparator 쉽게 생성하기 위해  메서드를 이용했다 2.2 숫자 배열에서 Max 값 찾기 - \n    - 기본 배열을 IntStream 스트림으로 변환한다\n- \n    - 결과를 Optional 객체로 반환한다\n- \n    - int 값을 반환한다.\n    - max값이 없는 경우에는  예외가 발생할 수 있다 2.3 객체 List에서 특정 필드의 최대 값을 가진 객체 찾기 이번 예제에서는 객체에서 특정 필드의 최댓값을 찾는 방법에 대해서 알아보자. 학생 중이 나이가 가장 많은 학생을 찾는 예제이다. - Students 객체 리스트 생성\n    - for loop 대신 IntStream과  사용하여 학생 객체 List 생성한다\n- 학생 중에 나이가 많은 학생 찾기\n    - 객체에서 나이 값이 최대 큰 값을 찾기 위해  메서드에 인자 값으로 를 넘겨준다. 2.4 String 배열에서 String 길이가 가장 큰 값을 찾기 - \n    - IntStream으로 변환할 때 string의 길이로 변환하여 max 값을 찾는다 3.정리 이 포스팅에서는 자바8 스트림 API의  메서드를 사용하여 List나 Array에서 최댓값을 찾는 방법을 살펴보았다. 예제는 Github 소스를 참고해주세요. 4.참고 - https://stackoverflow.com/questions/52443550/how-to-find-max-length-in-list-of-string-using-streams-in-java\n- https://www.baeldung.com/java-collection-min-max\n- https://codechacha.com/ko/java8-stream-max-min/",
    "category": "java",
    "tags": [
      "java",
      "java8",
      "stream",
      "list",
      "max",
      "min",
      "스트림",
      "자바",
      "자바8",
      "최대값",
      "최소값"
    ],
    "date": "2020-10-25T00:00:00.000Z"
  },
  {
    "id": "java/자바에서-final에-대한-이해",
    "slug": "java/자바에서-final에-대한-이해",
    "title": "자바에서 final에 대한 이해",
    "excerpt": "",
    "content": "1. 개요 위와 같이 final 키워드를 떠올릴 때면 그냥 상수로만 생각할 때가 종종 있습니다. final을 클래스, 메서드, 변수에 선언하면 조금씩 할 수 있는 부분들이 제안됩니다. 너무 당연한 내용이지만, 시간이 지니니까 기억에서 사라져버려서 이번에 다시 한번 상기하기 위해 정리를 해보았습니다. 자바에서 final 키워드는 여러 컨텍스트에서 단 한 번만 할당될 수 있는 entity를 정의할 때 사용됩니다. ( 위키피니아 ) final 키워드는 총 3가지에 적용할 수 있습니다. 각각에 대해서 세부적으로 알아보죠. - final 변수\n    - 원시 타입\n    - 객체 타입\n    - 클래스 필드\n    - 메서드 인자\n- final 메서드\n- final 클래스 2. Final 변수 2.1 원시 타입 로컬 원시 변수에 final로 선언하면 한번 초기화된 변수는 변경할 수 없는 상수값이 됩니다. 2.2 객체 타입 객체 변수에 final로 선언하면 그 변수에 다른 참조 값을 지정할 수 없습니다. 원시 타입과 동일하게 한번 쓰여진 변수는 재변경 불가합니다. 단, 객체 자체가 immutable하다는 의미는 아닙니다. 객체의 속성은 변경 가능합니다. 2.3 메서드 인자 메서드 인자에 final 키워드를 붙이면, 메서드 안에서 변수값을 변경할 수 없습니다. 2.4 맴버 변수 클래스의 맴버 변수에 final로 선언하면 상수값이 되거나 write-once 필드로 한 번만 쓰이게 됩니다. final로 선언하면 초기화되는 시점은 생성자 메서드가 끝나기 전에 초기화가 됩니다. 하지만, static이냐 아니냐에 따라서도 초기화 시점이 달라집니다. - static final 맴버 변수 (static final int x = 1)\n    - 값과 함께 선언시\n    - 정적 초기화 블록에서 (static initialization block)\n- instance final 맴버 변수 (final int x = 1)\n    - 값과 함께 선언시\n    - 인스턴스 초기화 블록에서 (instance initialization block)\n    - 생성자 메서드에서 2.4.1 인스턴스 초기화 블록 vs 정적 초기화 블록 정적 초기화 블록과 인스턴스 초기화 블록의 차이점을 간단하게 알아봅니다. | 인스턴스 초기화 블록                                         | 정적 초기화 블록                   |\n| ------------------------------------------------------------ | ---------------------------------- |\n| 객체 생성할때마다 블록이 실행됨 <br />부모 생성자이후에 실행됨 <br />생성자보다 먼저 실행됨 | 클래스 로드시 한번만 블록이 실행됨 | 실행결과는 다음과 같습니다. 정적 초기화 블록은 클래스가 로딩되는 시점에 한 번만 호출되고 static 블록 안에서 static 맴버변수를 초기화 할 수 있습니다. 인스턴스 초기화 블록은 객체를 생성할 때마다 호출되고 자식 객체의 생성자가 호출되기 전에 그리고 부모 생성자 이후에실행됩니다. 내용이 길었네요. 다음은 메서드와 클래스에 final을 선언하면 어떤 차이가 있는지 알아봅시다. 3. Final 메서드 메서드를 final로 선언하면 상속받은 클래스에서 오버라이드가 불가능하게 됩니다. Dog 객체는 Pet의 makeSound() 메서드를 재정의할 수 없습니다. 언제 사용하면 좋을까요? 구현한 코드의 변경을 원하지 않을 때 사용합니다. side-effect가 있으면 안 되는 자바 코어 라이브러리에서 final로 선언된 부분을 많이 찾을 수 있습니다. 4. Final 클래스 클래스에 final을 선언하면 상속 자체가 안됩니다. 그냥 클래스 그대로 사용해야 합니다. Util 형식의 클래스나 여러 상수 값을 모아둔 Constants 클래스을 final로 선언합니다. Pet 클래스가 final 클래스로 선언되어 상속할 수 없음 4.1 상수 클래스 상수 값을 모아준 클래스는 굳이 상속해서 쓸 이유는 없겠죠? 4.2 Util 형식의 클래스 JDK에서 String도 final 클래스로 선언되어 있습니다. 자바의 코어 라이브러리이기 때문에 side-effect가 있으면 안 되겠죠. 다른 개발자가 상속을 해서 새로운 SubString을 만들어 라이브러리로 다른 곳에서 사용하게 되면 유지보수, 정상 실행 보장이 어려워질 수 있습니다. 여기서에 작성된 코드는 github 를 참고해주세요. 5.참고 - final\n    - https://en.wikipedia.org/wiki/Final\\(Java)\n    - https://www.baeldung.com/java-final\n    - https://djkeh.github.io/articles/Why-should-final-member-variables-be-conventionally-static-in-Java-kor/\n- 초기화 블록\n    - http://freeprog.tistory.com/198\n    - https://docs.oracle.com/javase/tutorial/java/javaOO/initial.html",
    "category": "java",
    "tags": [
      "final",
      "java",
      "자바"
    ],
    "date": "2018-09-11T00:00:00.000Z"
  },
  {
    "id": "java/자바에서-클래스의-상속-구조에서-메서드-체이닝-해보기",
    "slug": "java/자바에서-클래스의-상속-구조에서-메서드-체이닝-해보기",
    "title": "자바에서 클래스의 상속 구조에서 메서드 체이닝 해보기",
    "excerpt": "",
    "content": "1. 메서드 체이닝이란 메서드 체이닝이란 여러 메서드 호출을 연결해 하나의 실행문으로 표현하는 문법 형태를 말합니다. (위키피디아 참고 #4.1) 메서드 체이닝의 매직은 간단합니다. 체이닝으로 연결하고 싶은 메서드의 반환 값으로 this를 반환하면 됩니다. 2. 추상 클래스와 상속 관계 있는 클래스에서의 메서드 체이닝 적용하기 2.1 One Depth : 추상 클래스 <--> 자식 클래스 한 클래스에서 메서드 체이닝을 적용하기는 쉽습니다. 하지만, 상속 관계가 있는 클래스에서는 this의 반환 값이 부모 클래스이거나 자식 클래스이기 때문에 메서드 체이닝을 할 때 캐스팅(cast)을 해줘야 하는 번거로움이 생깁니다. 이상적인 방법은 아래와 같은 형식으로 메서드 체이닝이 되면 좋죠. 메서드를 호출할 때마다 자식 객체(부모 메서드를 다 포함하고 있음)가 반환되면 캐스팅을 할 필요가 없어지니까요. 위 아이디어를 실행하기 위해 제네릭과 getThis() 함수를 추가하여 해결해보죠. 부모 클래스에 getThis()를 추상화 함수로 정의하고 체이닝 함수를 원하는 메서드마다 호출하여 제네릭 타입 T를 반환하도록 합니다. 그리고 자식 클래스에서는 실제 getThis()를 구현하여 자기 자신을 반환하도록 하면 우리가 원하는 의도대로 동작할 것입니다. 실제 코드를 보고 확인해보죠. 참고로, T extends Pet의 의미는 Pet 유형(하위 클래스 포함)이면 T자리에 들어갈 수 있다는 의미입니다. 2.2 Two Depth : 추상 클래스 <—> 추상 클래스 <—> 자식 클래스 추상 클래스의 깊이(depth)가 2이상인 경우에도 1 depth인 클래스에 정의된 제네릭 부분과 크게 다르지 않습니다. Pet과 BombayCat 클래스는 1 depth인 경우와 유사하고 Cat 클래스의 경우에는 Cat 타입에 허용될 수 있는 제네릭을 정의하면 됩니다. 2.3 Two Depth : 추상 클래스 <—> 추상 클래스 <—> 자식 클래스 refactoring 새로운 Cat 클래스를 추가할때마다 getThis()의 구현체를 매번 추가해야 하는 번거로움이 생깁니다. getThis() 구현은 this를 반환하는 것밖에 없으니까, 인터페이스 함수로 빼서 default로 정의하고 구현체를 담아보면 코드가 더 깔끔해집니다. 3. 소스 예제 전체 소스 코드는 github 에서 찾을 수 있습니다. 4. 참고 - https://en.wikipedia.org/wiki/Methodchaining\n- https://stackoverflow.com/questions/1069528/method-chaining-inheritance-don-t-play-well-together\n- https://www.andygibson.net/blog/article/implementing-chained-methods-in-subclasses/\n- https://stackoverflow.com/questions/15054237/oop-in-java-class-inheritance-with-method-chaining\n- http://www.angelikalanger.com/GenericsFAQ/FAQSections/ProgrammingIdioms.html#FAQ206\n- https://www.baeldung.com/java-type-casting",
    "category": "java",
    "tags": [
      "method",
      "chaining",
      "java",
      "inheritance",
      "자바",
      "메서드",
      "체이닝"
    ],
    "date": "2018-08-19T00:00:00.000Z"
  },
  {
    "id": "java/전후처리를-위한-자바-메서드-래퍼-메서드-생성하기",
    "slug": "java/전후처리를-위한-자바-메서드-래퍼-메서드-생성하기",
    "title": "전후처리를 위한 자바 메서드 래퍼 메서드 생성하기",
    "excerpt": "",
    "content": "1.개요 코딩을 하다 보면 어떤 작업을 하기 전에 전후 처리가 필요할 때가 종종 생깁니다. 전처리(pre-processing)에서는 실제 작업을 수행하기 전에 필요한 세팅을 하고 후처리(post-processing)에서는 cleanup 정도의 작업을 하게 됩니다. 이런 전후 처리를 여러 번 할 때에는 별도의 메서드로 구현해두면 좋습니다. 몇 개의 예제를 보면서 알아보죠. 2.예제 2.1 파일에 텍스트 String 값 쓰기 파일에 쓰려면 기본적으로 파일 스트림을 열고 텍스트를 쓰고 나서 파일 스트림을 닫아야 합니다. 파일 스트림을 여닫는 부분을 전후처리로 만들어봅시다. writeLineToFile 메서드는 3개의 인자로 받습니다. 파일 이름, 스트링 값, 그리고 람다 함수를 인자로 넘겨주고 있습니다. 람다 함수로 넘겨주는 메서는 전후처리 중간에 실행될 부분입니다. 람다 함수로 넘겨줄 인터페이스를 정의합니다. writeLineToFile 메서드에서는 전처리 코드, 넘겨 받은 함수 실행, 그리고 마지막으로 후처리 코드로 마무리합니다. 2.2 Unit test에서 static final로 선언된 상수값을 변경하여 테스트하기 기존에 선언한 static final 상수 값을 변경하면서 unit test를 작성하려면 reflection을 사용해서 상수 값을 변경해줘야 합니다. static final 값을 한번 변경하면 변경된 값으로 계속 유지되기 때문에 다른 unit test의 결과가 fail로 떨어질 수 있습니다. 그래서 테스트 이후에는 기존 값으로 원복시켜줘야 합니다. 실제 작성된 코드를 보죠. static final로 선언된 상수 값입니다. Unit Test에서 상수 값을 다른 값으로 변경하고 테스트하는 코드를 짠다면, 아래 코드와 같겠죠. 매번 다른 SIZE 값으로 unit test를 작성한다면, #2.1에서 했던 것처럼 인터페이스를 정의하고 refactoring하는게 낫겠죠. 전후처리 하는 코드아래 메서드로 refactoring합니다. reflection을 이용해서 static final을 상수값을 변경하는 메서드는 dzone 사이트에서 참조하였습니다. (참고 - static final field 변경) 인터페이스 선언하는 부분입니다. 인터페이스는 인자도 없고 반환 값도 없는 void 이여서 Runnable 인터페이스를 그냥 사용해도 무방합니다. - 지금까지 작성된 코드는 github에 올려줘 있습니다.     - com.java.examples.prepost     - - Constants\n    - PrePostTest\n    - FinalFieldChangeTest 3. 참고 - 전후처리\n    - https://www.javacodegeeks.com/2013/05/a-simple-application-of-lambda-expressions-in-java-8.html\n    - https://stackoverflow.com/questions/43599406/create-generic-java-method-wrapper-for-pre-and-post-processing\n- static final field 변경\n    - https://dzone.com/articles/how-to-change-private-static-final-fields\n- 함수 인터페이스\n    - http://multifrontgarden.tistory.com/125",
    "category": "java",
    "tags": [
      "java",
      "wrapper",
      "pre processing",
      "post processing",
      "자바",
      "래퍼",
      "메서드",
      "전후처리"
    ],
    "date": "2018-08-26T00:00:00.000Z"
  },
  {
    "id": "java/추가된-log를-junit-에서-확인하는-방법",
    "slug": "java/추가된-log를-junit-에서-확인하는-방법",
    "title": "추가된 LOG를 JUnit에서 확인하는 방법",
    "excerpt": "",
    "content": "1. 들어가며 Unit Test를 작성할 때 메서드의 결과를 기본적으로 확인하여 로직을 검증합니다. void인 메서드인 경우에는 내부 메서드에서 실행하는 메서드의 실행 여부나 메서드로 넘겨진 인자 값을 가지고 확인하기도 합니다. 개발 하다 보면 로직에 변화는 없지만, 단순히 메서드에서 로그를 추가하는 경우가 생깁니다. 개인적으로는 추가한 로그를 Unit Test로 확인해야 하나 쉽지만, 확인이 가능한지 궁금증이 생겨서 알아보았습니다. StackOverflow에는 없는 게 없네요. 저만 궁금한 게 아니었나 봐요. 2. 개발 환경 소스 코드는 아래 링크를 참고해주세요.  OS : Mac OS  IDE: Intellij  Java : JDK 1.8  Source code : github  Software management tool : Maven 3. 로그 찍히는 지 확인하는 방법 개발하는 프로젝트에서 아래와 같이 예외처리가 안 된 부분에 로그를 추가하여 원하는 정보를 출력하도록 하였습니다. catch 문구에 추가된 로그를 Unit Test에서 확인해보겠습니다. ListAppender 클래스는 발생하는 Log 이벤트를 List에 저장하여 나중에 조회 가능하도록 해주는 클래스입니다. someService.requestJobId() 메서드에서 로그를 찍게 되고 로그로 출력한 내용은 listAppender에 저장됩니다. 저장한 로그의 메시지를 확인하기 위해 listAppender.list에서 List를 가져옵니다. 4. 참고  Logger Assert\n     https://stackoverflow.com/questions/1827677/how-to-do-a-junit-assert-on-a-message-in-a-logger\n     https://stackoverflow.com/questions/29076981/how-to-intercept-slf4j-with-logback-logging-via-a-junit-test\n     https://www.jvt.me/posts/2019/09/22/testing-slf4j-logs/\n ListAppender\n     http://people.apache.org/carnold/log4j/docs/jdiffreport/Version%201.3/org/apache/log4j/varia/ListAppender.html",
    "category": "java",
    "tags": [
      "java",
      "logger",
      "junit",
      "unittest",
      "자바",
      "유닛테스트",
      "로거"
    ],
    "date": "2019-11-18T00:00:00.000Z"
  },
  {
    "id": "javascript/qa-javascript-관련-질문-모음",
    "slug": "javascript/qa-javascript-관련-질문-모음",
    "title": "Q&A JavaScript 관련 질문 모음",
    "excerpt": "",
    "content": "개인적으로 모르는 부분 적어두고 알게 되는 부분에 대해서 간단하게 정리해둔 자료입니다.\n미 답변중에 알고 계신 부분 있으면 코멘트 달아주세요. 감사합니다. Q&A 전체 목록 <span style=\"color:orange\">[답변완료]</span> <span style=\"color:brown\">1.  이건 뭔가? ES6체 추가된 새로운 문자열 표기법으로 템플릿 리터럴(Template Literal)이라고 합니다.\n템플릿 리터럴은 \\ 문자 사용없이 문자열에서 줄바꿈도 허용하고 간단하게 \\${…} 문자열 인터폴레이션 표현식을 통해서 변수의 값 바로 치환되어 쉽게 사용할 수 있습니다. 참고 - https://poiemaweb.com/es6-template-literals\n- https://developer.mozilla.org/ko/docs/Web/JavaScript/Reference/Templateliterals <span style=\"color:brown\">2. var와 const, let의 차이점은? const와 let의 키워드는 ES6에 도입된 키워드입니다. - var \\ scope가 함수 단위로 동작한다\n   - const\n    - scope가 블록 단위이다\n    - 값이 바뀌지 않는 때 사용한다\n       - let\n    - scope가 블록 단위이다\n    - 값이 변경될 때 사용한다 참고 - https://velopert.com/3626 <span style=\"color:brown\">3. 람다식으로 표현된 () => ({})의 의미는 뭔가? ES6에서 람다식 문법이 추가되었습니다. () => ({}) 표현식은 function() { return { } }와 동일합니다. 참고 - http://hacks.mozilla.or.kr/2015/09/es6-in-depth-arrow-functions/ <span style=\"color:brown\">4. …은 뭔가? ES6에 추가된 문법으로 Spread나 Rest Parameter로 사용할 수 있습니다. - Spread operator\n    - iterable가능한 배열, 객체, 스트링에 대해서 단일 요소들로 확장해준다\n    - ex.\n      \n       - Rest Parameter\n    - 모든 요소를 배열로 만들어준다\n    - Rest Parameter는 맨 마지막 인자여야 한다\n       참고 - https://developer.mozilla.org/ko/docs/Web/JavaScript/Reference/Functions/restparameters\n- https://scotch.io/bar-talk/javascripts-three-dots-spread-vs-rest-operators543\n- https://jaeyeophan.github.io/2017/04/18/ES6-4-Spread-Rest-parameter/ --- <span style=\"color:orange\">[미 답변 질문]</span> - defaultProps은 언제 사용되나?",
    "category": "javascript",
    "tags": [
      "Q&A",
      "faq",
      "javascript",
      "defaultProps",
      "es6"
    ],
    "date": "2018-03-23T00:00:00.000Z"
  },
  {
    "id": "linux/raspberry-pi4-n8n-완벽한-자동화-서버-만들기",
    "slug": "linux/raspberry-pi4-n8n-완벽한-자동화-서버-만들기",
    "title": "Raspberry Pi 4 + n8n: 완벽한 자동화 서버 만들기",
    "excerpt": "",
    "content": "1. 개요 직장 동료의 소개로 를 접하게 되었고 워크플로우기반의 자동화 도구로 다양한 서비스 특히 AI와 연동해서 자동화하기 좋은 도구이다. 이번 포스팅에서는 라즈베리파이에 를 설치하고 구동하는 방법에 대해서 알아본다. ------ 2. 라즈베리파이에  설치하는 방법 2.1 사전 작업 설치된 패키지 최신화 라즈베리파이의 패키지를 최신 상태로 유지하기 위해 다음 명령어를 실행한다. Node.js 설치 및 버전 확인 은  기반으로 동작하므로, 최신 버전의 Node.js를 설치해야 한다. 설치된  버전을 확인하여 정상적으로 설치되었는지 확인한다. 설치 이제 npm을 사용하여 을 전역(global)으로 설치한다. ------ 2.2 용 system service 파일 설정 을 시스템 서비스로 등록하면 라즈베리파이가 부팅될 때 자동으로 실행된다. 서비스 파일 생성 다음 내용을 추가한다. -  → 실행 가능한 바이너리 경로를 설정하여  실행을 보장\n-  → 을 프로덕션 환경에서 실행하도록 설정\n-  → 커뮤니티에서 제공하는 패키지를 사용할 수 있도록 허용\n-  → 기본 인증 기능을 비활성화하여 누구나 접근 가능하도록 설정 (보안이 필요하면 true로 변경 필요)\n-  → 보안 쿠키 기능을 비활성화 (HTTPS 환경이 아니라면 false로 두는 것이 일반적)\n-  → 의 라이선스 활성화 키 (유료 플랜 사용 시 필요) 2.3 서비스 활성화 및 시작 서비스를 등록하고 활성화한다. 정상적으로 실행되고 있는지 확인한다. ------ 2.4  접속하기 이 정상적으로 실행되었으면, 웹 브라우저에서 로 접속하여 확인할 수 있다. ------ 3. 마무리 을 라즈베리파이에 성공적으로 설치하고, 시스템 서비스로 등록하여 안정적으로 운영할 수 있도록 설정했다. 이를 통해 자동화된 워크플로우를 구축하고 활용할 수 있다. 추가적으로 보안이 필요하다면 기본 인증을 활성화하고, HTTPS 설정을 고려해보는 것도 좋다. > 개인적으로 외부에서도 접근해서 작업하고 싶어 집 iptime 공유기에서 port forwarding 을 설정해서 사용중이다.",
    "category": "linux",
    "tags": [
      "n8n",
      "n8n 자동화",
      "raspbian",
      "raspberry pi",
      "라즈베리파이",
      "라즈비안"
    ],
    "date": "2025-03-23T00:00:00.000Z"
  },
  {
    "id": "linux/raspberry-pi4-os-설치",
    "slug": "linux/raspberry-pi4-os-설치",
    "title": "모니터 없이 Raspberry Pi 4 OS 설치",
    "excerpt": "",
    "content": "1. 들어가며 회사에서 좋은 동료로부터 라즈베리파이4를 선물로 얻게 되어 당분간 나의 개인 Toy로 사용할 수 있을 듯하다. 조금 더 적극적으로 사용하기 위해 개인적으로 운용하고 있는 명언 서비스를 AWS에서 라즈베리파이로 옮길 계획이다. 자, 라즈베리파이4에 OS를 설치하는 방법에 대해서 알아보자. 모니터로 연결해서 설치할 수도 있지만, 모니터 없이도 간단하게 OS를 설치할 수 있다. 필요한 도구는 다음과 같다. - 라즈베리파이4\n- SD card\n- SD card 리더기\n- 전원 케이블 (5V, 3000mA)\n    - 개인적으로 USB-C 케이블를 사용해서 구동하고 있음 2.라즈베리파이 OS 설치 2.1 SD card에 라즈베리파이 PI OS 설치하기 OS 설치는 아래 순서대로 진행하면 된다. 맥을 사용하고 있어 맥 기준으로 설명한다. 1. Raspberry Pi image 다운로드 및 설치\n2. SD card format\n3. SD card에 OS 쓰기 먼저 라즈베리파이 PI 사이트에 접속하여 Raspberry Pi Imager를 다운로드한다. 다운로드한 파일을 더블클릭하고 App 파일을 선택하여 Applications 폴더로 옮겨 파일을 복사하면 설치가 완료된다. Spotlight로 Raspberry Pi를 검색하여 Application을 구동시키면 다음 화면을 볼 수 있다. 다음은 SD card를 포맷시키기 위해 SD card 리더기에 SD card를 넣고 컴퓨터에 연결한다. 그리고 Raspberry Pi Imager에서 아래 옵션을 선택하여 포맷을 시킨다. - Storage > 삽입된 SD card를 선택\n- Operation System > Choose OS > Erase를 선택 포맷이 완료되면 아래와 같이 옵션 선택 이후 WRITE 버튼을 클릭하면 OS 이미지는 인터넷으로 자동 다운로드되고 SD card에 쓰이게 된다. 쓰기 완료 시점은 SD card의 사양에 따라서 다를 수 있다. - Operating System > Raspberry PI OS Full (32-BIT) 선택\n- Storage > 삽입된 SD card를 선택 2.2 로컬 환경에서 라즈베리파이에 접근하기 2.2.1 SSH로 접근하기 ssh로 접근하기 위해서는 SD card에 설정 파일을 생성하면 라즈베리파이 구동 시 바로 SSH로 접근할 수 있다. 필요한 SSH 설정은 다음과 같다. 두 파일 모두 root 폴더에 생성한다. - ssh empty 파일 생성 ()\n- wifi 설정 파일 생성 () ssh empty 파일()은 확장명 없이 root에 생성한다. 그리고 도 SD card root에 생성하고 사용하려는 wifi와 암호 정보를 작성하면 된다. SD card 리더기에서 분리하여 라즈베리파이 보드에서 rpi에 넣고 전원 케이블을 연결하여 구동시킨다. 공유기로부터 할당받은 IP 주소를 확인하기 위해 공유기 어드민에 접속하여 확인한다. 내 공유기에서는 로 할당을 받았다. IP 주소는 변경될 수 있으니  파일에 도메인 이름을 설정하여 도메인 이름을 사용하는게 좋을 듯하다. IP 주소나 등록한 도메인이름으로 접속한다. > 라즈베리파이  기본 값은  이다. 2.2.2 Remote Desktop으로 접근하기 리눅스 터미널 대신 데스크탑 환경으로 로그인하려면 VNC나 XRDP로 접속할 수 있다. 먼저 VNC 설정을 알아보자. 2.2.2.1 VNC VNC 설정은 로 터미널에서 VNC를 손쉽게 설정할 수 있다. - Interface Options > VNC > Yes를 선택 맥에서 VNC로 접속하려면 brew 명령어로 VNC Viewer를 설치한다. VNC Viewer를 구동시켜 아래와 같이 Connect 설정후 로그인을 한다. 2.2.2.2 XRDP XRDP는 로 필요한 패키지 설치후 리부팅을 시킨다. 맥에서 Microsoft Remote Desktop가 없은 경우에는 AppStore 찾아 설치하고 Connect 설정을 한다. XRDP로 로그인한 화면이다. 3. 기타 설정 3.1 암호 변경하기 기본 암호를 변경하려면  명령어로 변경한다. 3.2 VNC 구동시 \"Cannot currently show the desktop\" 오류 발생시 해결책 \"Cannot currently show the desktiop\" 오류가 발생하면 에서 아래 설정을 변경하면 해결된다. - Display Options > Resolution > DMT Mode 16 1024 x 728 참고 - https://m.blog.naver.com/elepartsblog/221828692886 4. 마무리 모니터없이 Raspberry Pi OS를 설치했다. 다음에는 라즈베리파이에 명언 서비스를 쿠버네티스로 구동하는 방법에 대해서 다루도록 한다. 5. 참고 - https://www.raspberrypi.org/software/\n- https://roboticsbackend.com/install-ubuntu-on-raspberry-pi-without-monitor/\n- https://linuxhint.com/installraspberrypioswithoutexternalmonitor/\n- https://www.tomshardware.com/reviews/raspberry-pi-headless-setup-how-to,6028.html",
    "category": "linux",
    "tags": [
      "linux",
      "pi",
      "os",
      "install",
      "monitor",
      "raspbian",
      "raspberry pi",
      "라즈베리파이",
      "라즈비안",
      "운용체제",
      "리눅스",
      "설치",
      "모니터"
    ],
    "date": "2021-06-19T00:00:00.000Z"
  },
  {
    "id": "linux/raspberry-pi에서-최신-go-버전-설치하기-apt-대신-직접-설치",
    "slug": "linux/raspberry-pi에서-최신-go-버전-설치하기-apt-대신-직접-설치",
    "title": "Raspberry Pi에서 최신 Go 버전 설치하기 – apt 대신 직접 설치!",
    "excerpt": "",
    "content": "1. 개요 라즈베리파이4에 64 OS 업그레디 하면서 다시 최신 Golang 설치하려니 기억이 나지 않아서 기록상 남겨둔다. 를 이용해서 설치를 하면 1.19 버전이 설치가 되어서 이 포스팅에서는 수동으로 최신 버전으로 설치하는 방법을 다룬다. ------ 2. 최신 Golang 설치하는 방법 2.1 라즈베리파이 OS 확인 먼저 현재 사용 중인 라즈베리파이의 OS와 아키텍처를 확인한다. 출력 결과에 따라 아키텍처를 확인할 수 있다. -  → 32비트 (ARMv7)\n-  → 64비트 (ARM64)\n-  → 32비트 (구형 모델) 2.2 설치된 architecture OS에 맞게 Golang 다운로드 Golang의 최신 버전을 공식 웹사이트에서 확인하고 다운로드한다. 1. 아래 명령어로 최신 버전을 확인한다. 최신 버전을 기반으로 다운로드 URL을 설정한다. >  부분은 이면 , 이면 로 변경한다. 예를 들어, ARM64 버전의 최신 Golang을 다운로드하려면 다음과 같이 실행한다. 2.3 Golang 수동 설치 기존의 Golang을 제거하고, 새 버전을 설치한다. 2.4 환경 변수 설정 Golang을 정상적으로 사용하려면  환경 변수를 설정해야 한다. 2.5 Golang 실행 확인 설치가 정상적으로 완료되었는지 확인한다. ------ 3. 마무리 이제 라즈베리파이에 최신 Golang이 성공적으로 설치되었다. 를 통한 기본 설치보다 최신 기능과 성능 최적화를 활용할 수 있다. 이후 Go 프로젝트를 시작하려면  설정 및 모듈 관리를 추가로 진행하면 된다. 🚀 4. 참고 - Installing the Latest Version of Golang on Your Raspberry Pi\n- Operating system images",
    "category": "linux",
    "tags": [
      "linux",
      "pi",
      "os",
      "install",
      "monitor",
      "raspbian",
      "raspberry pi",
      "라즈베리파이",
      "go",
      "golang",
      "apt"
    ],
    "date": "2025-03-23T00:00:00.000Z"
  },
  {
    "id": "mac/m1-맥북-한영-전환-버벅임-없애는-방법-karabiner",
    "slug": "mac/m1-맥북-한영-전환-버벅임-없애는-방법-karabiner",
    "title": "M1 맥북 한영 전환 버벅임 없애는 방법 - Karabiner",
    "excerpt": "",
    "content": "1. 개요 맥북을 사용하는 많은 사용자가 한영키 전환 시 버벅임을 경험한다.  매팅을 통해서 한영키 전환 문제를 해결 했었는데, Ventura 버전 이상으로 업그레이드한 후 다시 버벅임이 발생했다. 맥 OS의 어떤 변화로 다시 안되는지는 모르지만, 결론적으로 최신  버전을 업데이트하고 다시 설정해주면 해결이 된다. 기록상 블로그에 남깁니다. 1.1 사용하는 맥버전 현재 macOS Ventura 13.2.1을 사용하는 M1 맥북 에어를 사용 중이다. 2. Karabiner 설치 밍 설정 2.1 Karabiner 설치 는 강력한 키 매핑 도구로, 맥북의 키보드 동작을 사용자 정의할 수 있다. 오픈소스로 Karabiner 공식 웹사이트에 접속해 최신 버전을 다운로드한다. > 처음 실행 시 시스템 환경설정에서 접근성 권한을 요청한다. 이 권한을 부여해야 정상적으로 작동할 수 있다  >  에서 아래 두 개의 모니터링을 허용해준다. 2.2 Karabiner 설정 - 한영 전환 버벅임 없애기  를 실행하고 아래와 같이 매팅을 추가한다. 이 설정은 을 누르면 f18 키를 누른 것과 동일한 결과가 나온다는 의미이다. 맥  설정에서  >  >  클릭하고 입력 메뉴에서 다음 소스 선택의 키를 로 변경한다. 을 눌렸을 때 한영 키를 전환하도록 바꿔주게 된다. > 이제 을 눌러보세요. 문제 없이 잘 동작할 거예요. 결론적으로 위 과정을 통해서 사용자가  키 누름 → 매팅에 의해서  가 눌림 → 한영키가 잘 전환된다. 3. 참고 - [[MAC\\] Karabiner 한영키 전환 버벅임 없애기](https://www.clien.net/service/board/lecture/18250346)\n- 맥 한영전환 딜레이 해결방법 (Karabiner-Elements)\n- 맥북 m1 한영 전환 딜레이 없애는 방법 (Karabiner)",
    "category": "mac",
    "tags": [
      "m1",
      "맥북",
      "한영전화",
      "버벅임",
      "Karabiner",
      "Karabiner-Elements",
      "caps_lock",
      "키보드",
      "입력소스"
    ],
    "date": "2024-08-24T00:00:00.000Z"
  },
  {
    "id": "mac/mac-iterm2에서-한글-깨지는-이슈",
    "slug": "mac/mac-iterm2에서-한글-깨지는-이슈",
    "title": "Mac iTerm2에서 한글 깨지는 현상",
    "excerpt": "",
    "content": "1. 개요 최근 맥북을 새로 설치한 후 를 사용하는 과정에서 한글 폴더 및 파일명이 깨지는 문제가 발생했다. 이번 글에서는 에서 한글 깨짐 현상을 해결하는 방법을 소개한다. 2. 한글 깨지는 현상 -  설정 변경 Term2에서 한글 깨짐 현상은 주로 두 가지 이유로 발생한다. Unicode Normalization Format 설정과 폰트 선이다. 아래 단계를 따라 설정을 변경하면 문제를 해결할 수 있다. 2.1 Unicode Normalization Format 변경 는  설정에 따라 문자 인코딩 방식을 다르게 처리한다. 기본 설정이 한글 처리에 적합하지 않기 때문에 이를 수정해야 한다.  실행하고 Profiles > Text 탭에서 관련 설정을 변경한다. -  → 로 변경 > (Normalization Form C)는 한글 조합 문자를 처리할 때 적합한 설정으로, 한글 파일명과 폴더명을 올바르게 표시한다., 2.2 Font 변경 에서 사용하는 폰트도 한글 표시 여부에 영향을 미친다. 한글을 제대로 표시하려면  폰트를 사용하는 것이 좋다. - Font 설정에서  선택 > 만약 Font목록에 가 없을 경우, 이 링크를 통해 다운로드 및 설치할 수 있다 3. 마무리 위 단계를 통해 에서 발생하던 한글 깨짐 문제를 손쉽게 해결할 수 있었다. 4. 참고 - [[MAC] iTerm2 한글 분리 해결 / iTerm2 한글 깨짐 해결](https://passing-story.tistory.com/entry/MAC-iTerm2-%ED%95%9C%EA%B8%80-%EB%B6%84%EB%A6%AC-%ED%95%B4%EA%B2%B0-iTerm2-%ED%95%9C%EA%B8%80-%EA%B9%A8%EC%A7%90-%ED%95%B4%EA%B2%B0)",
    "category": "mac",
    "tags": [
      "m1",
      "맥북",
      "iTerm2",
      "iterm",
      "한글깨짐",
      "NFC",
      "Normalization Form"
    ],
    "date": "2024-11-30T00:00:00.000Z"
  },
  {
    "id": "mac/otp-코드-이제-수동-입력은-그만-alfred-2fa-read-code-써보자",
    "slug": "mac/otp-코드-이제-수동-입력은-그만-alfred-2fa-read-code-써보자",
    "title": "OTP 코드, 이제 수동 입력은 그만! Alfred 2FA Read Code 써보자",
    "excerpt": "",
    "content": "1. 개요 회사 VPN에 연결할 때마다 문자로 (일회용 비밀번호)를 받고, 이를 수동으로 입력하는 번거로운 작업을 매일 반복하고 있었다. 하지만 회사 동료의 추천으로 Alfred Workflow를 활용하여 이 과정을 자동화하면서 삶이 조금 더 수월해졌다. 자동화 도구로 사용한 Alfred Workflow의 를 소개하고, 이를 활용하여 에서 자동으로 2FA 코드를 추출하고 입력하는 방법을 설명한다. > Alfred Workflow는 macOS의 생산성을 높여주는  앱의 강력한 기능 중 하나로, 반복적인 작업을 자동화할 수 있도록 도와준다. 2FA Read Code란? 는 Alfred Workflow 중 하나로, 에 수신된  코드를 자동으로 찾아 복사할 수 있도록 도와준다. 이를 통해  로그인, 웹사이트 2FA 입력 등에서 손쉽게 인증을 진행할 수 있다. 2.  사용하는 방버 2.1 필요 조건 이 Workflow를 사용하려면 다음과 같은 조건을 충족해야 한다. - 아이폰과 맥북을 함께 사용하는 사용자\n-  동기화 활성화 이렇게 하면 iPhone에서 받은 2FA 문자 메시지를 Mac에서도 확인할 수 있으며, 2FA Read Codebrew2FA Read Code2FA Read CodeAlfred2FA Read Code2FA Read CodeInstall in AlfredAlfredAlfred설치Alfred2FA Read Code2FA Read CodeOTPVPNVPNOTPOTPAlfredAlfred⌘ + 스페이스2faEnteriMessageVPN⌘ + V`(붙여넣기)로 코드를 입력한다 이제 더 이상 핸드폰을 직접 확인하고 수동으로 OTP를 입력할 필요가 없다! 3. 마무리 2FA Read Code Workflow를 활용하면 OTP 코드 입력을 자동화하여 로그인 과정을 훨씬 편리하게 만들 수 있다. 특히 매일 VPN 접속이나 2FA 인증을 해야 하는 사용자라면, 이 간단한 자동화만으로도 업무 속도를 크게 향상시킬 수 있다. 맥북을 사용하면서 생산성을 높이고 싶다면, Alfred Workflow를 적극 활용해 보세요! > 다시 한번 저에게 소개해주신 회사 동료분께 감사드립니다. 4. 참고 - 2FA Read Code\n- Full Disk Access",
    "category": "mac",
    "tags": [
      "alfred",
      "2fa",
      "2fa read code",
      "OTP 문자",
      "자동 문자 입력",
      "Alfred Workflow",
      "vpn"
    ],
    "date": "2025-03-08T00:00:00.000Z"
  },
  {
    "id": "mac/맥-iterm2에서-alt-키-매핑-설정하는-방법",
    "slug": "mac/맥-iterm2에서-alt-키-매핑-설정하는-방법",
    "title": "맥 iTerm2에서 Alt 키 매핑 설정하는 방법",
    "excerpt": "",
    "content": "1. 개요 리눅스 환경에서 터미널을 사용할 때, Alt 키와 방향키(←, →)를 사용하면 Word 단위로 이동하는 기능이 기본적으로 활성화되어 있어 매우 편리한다. 그러나 맥의 iTerm2에서는 이러한 기능이 기본적으로 설정되어 있지 않으며, 대신 키 입력 시  또는  같은 값이 표시만 된다. 맥 환경에서도 이 기능을 사용할 수 있도록 iTerm2에서 Alt 키를 매핑하는 방법을 소개한다. 2. iTerm2에서 Alt 키 매핑 설정하는 방법  >  >  > 에서 아래와 같이 새로운 매핑을 추가하면 된다. -  :\n  - Action :  선택\n  - Esc +:  값 입력\n-  :\n  - Action:  선택\n  - Esc +:  값 입력 최종적으로 아래와 같이 2가지 키 매핑을 추가하면 된다.  3. 참고 - Making the Alt Key Work in iTerm2",
    "category": "mac",
    "tags": [
      "m1",
      "맥북",
      "iTerm2",
      "iterm",
      "alt mapping",
      "word jumping"
    ],
    "date": "2024-12-08T00:00:00.000Z"
  },
  {
    "id": "mac/맥환경에서-countdown-shell-script으로-집중력-강화하기",
    "slug": "mac/맥환경에서-countdown-shell-script으로-집중력-강화하기",
    "title": "맥환경에서 countdown shell script으로 집중력 강화하기",
    "excerpt": "",
    "content": "1. 들어가며 스터디나 블로그 작성을 시작하려고 노트북을 켜면 바로 스터디나 블로그 작성보다는 먼저 가는 곳이 인터넷 뉴스나 유튜브 동영상을 먼저 보게 됩니다. 그리고 30분 정도 보다 보면 이제 시작해야지 하고 에버노트를 킵니다. 그리고 집중해서 뭔가 시작하려고 하면 10분 정도 하다가도 다시 뉴스나 유튜브 동영상을 보고 있는 저 자신을 보게 됩니다. 스터디했다가 뉴스 봤다가 이걸 반복하다 보면 몇 시간이 훌쩍 지나가 있는 경우가 허다합니다. 그래서 그런지 매주 하나씩 블로그 작성하려고 하는 게 잘 안되는 게 아닌가 싶습니다. 김민식 PD님이 쓰신 <영어 책 한권 외워봤니?> 책을 보면서 뽀모드로(pomodoro) 기법에 대해서 알게 되어 이걸 적용해보기로 했습니다. 여러 책이나 인터넷에서 언급되었던 시간 관리 기법중에 하나입니다. 뽀모드로 기법 은 ‘프란체스코 시릴로’가 제안한 방식으로 타이머를 사용해서 25분간 집중해서 일을 한다음 5분간 휴식하는 방식입니다. 앱 스토어나 핸드폰에서도 관련 앱이 많이 있지만, 저는 그냥 맥 터미널에서 동작하기 쉽게 shell script로 작성하였습니다. 그리고 25분이 지나면, 맥환경에서 Popup 형식으로 notification을 받고 효과음도 같이 플레이 하도록 스크립트를 작성하였습니다. 2. 실행화면 1분 동안 키운다운후 Pop-up이 실행되는 화면입니다. 3. 스크립트 작성 및 시스템 알람 설정 맥 환경에서 작업하였고 zsh shell을 사용하였습니다. 3.1 Shell 설정 파일 수정하기 Shell 설정 파일을 텍스트 에디터로 오픈하여 아래 함수를 넣고 저장합니다. https://gist.github.com/kenshin579/1b8dc3d9db35b6fee534569ec128e62b 수정한 Shell 설정 파일을 현재 Shell에서 재로딩하려면 source 명령어로 다시 로드하면 됩니다. 3.2 효과음 파일 Library 폴더로 복사하기 효과음 파일을 사용자 Library 폴더로 복사합니다. <a href='Clock-chimes.mp3'>Clock-chimes.mp3</a> 3.3 시스템 알람 설정하기 알람을 별도로 설정하지 않으면 기본으로 배너 알람 스타일 이라서 알람이 나왔다가 자동으로 사라집니다. 그래서 다른 화면을 보고 있으면 알람 창이 뜬지도 모르는 경우가 종종 발생합니다. 알람창이 자동으로 사라지지 않고 닫기 버튼을 둘러야 사라지는 알람 스타일로 변경하는 게 좋습니다. 시스템환경 > 알림 > 스트립트 편집기 선택후 아래와 같이 알람 스타일을 변경합니다. 3.4 실행하기 1을 입력하면 1분 동안 카운트 다운이 시작되고 1분이 지난 후에는아래와 같이 Popup 창이 뜨게 됩니다. 4. 참고 - Display Notification from Mac \\ https://code-maven.com/display-notification-from-the-mac-command-line",
    "category": "mac",
    "tags": [
      "count",
      "down",
      "shell",
      "script",
      "mac",
      "timer",
      "맥",
      "스트립트",
      "뽀모도로"
    ],
    "date": "2019-02-26T00:00:00.000Z"
  },
  {
    "id": "mac/여러-페이지-있는-pdf-파일-png로-변환하기",
    "slug": "mac/여러-페이지-있는-pdf-파일-png로-변환하기",
    "title": "여러 페이지 있는 PDF 파일 PNG로 변환하기",
    "excerpt": "",
    "content": "여러 페이지가 있는 PDF를 이미지 파일로 변환해야 하는 경우가 있는데, 바로 기억을 못해서 기록상 블로그에 남깁니다.  명령어로 여러 페이지가 있는 PDF를 png 파일로 변환했다.  옵션은 man page에서 참고 가능한다. 참고 - https://superuser.com/questions/1469592/how-can-i-convert-a-pdf-into-a-series-of-images-jpgs-or-pngs-via-the-terminal\n- https://superuser.com/questions/1469592/how-can-i-convert-a-pdf-into-a-series-of-images-jpgs-or-pngs-via-the-terminal",
    "category": "mac",
    "tags": [
      "맥",
      "명령어",
      "이미지",
      "mac",
      "pages",
      "pdf",
      "jpg",
      "png",
      "cmd",
      "변환",
      "image",
      "multiple",
      "convert"
    ],
    "date": "2012-02-13T00:00:00.000Z"
  },
  {
    "id": "node/node-모듈-npm-저장소에-배포하기",
    "slug": "node/node-모듈-npm-저장소에-배포하기",
    "title": "Node.js 모듈 NPM 저장소에 배포하기",
    "excerpt": "",
    "content": "1. 들어가며 이번 포스팅에서는 Node.js 모듈을 NPM 저장소에 배포하는 방법에 대해서 알아보겠습니다. NPM은 Node Package Modules 약자로 Node.js 모듈 관리해주는 패키지 관리자입니다.\nNPM의 명령어로 쉽게 Node.js 모듈을 설치해서 js 라이브러리를 사용할 수도 있고 또한 개발한 모듈을 다른 이들이 사용할 수 있도록 레지스토리에 배포할 수도 있습니다. 2. NPM 저장소에 배포하기 배포과정은 맥 환경으로 설명하겠습니다. 2.1 NPM 설치 NPM 명령어는 Node.js 설치 시 같이 설치되는데, 노드가 아직 설치되어 있지 않다면 brew로 설치합니다. 2.2 package.json NPM 리파지토리에 배포하기 위해 간단한 파일 몇 개만 필요합니다. 그중 가장 먼저 작성하게 되는 파일이 package.json 입니다. 프로그램에 대한 기본적인 내용(ex. 이름, 버전)을 포함해서 프로그램에 필요한 dependency 그리고 실행 방법 등을 기술할 수 있습니다. npm의 init 명령어로 -y 옵션을 주어서 non-interactive 하게 기본 설정으로 package.json을 생성할 수 있습니다. 아래는 app-timer-pomodoro 의 package.json 파일입니다. 기본 값으로 설정한 것보다 더 많은 내용이 있지만, 쉽게 이해할 수 있는 수준입니다. 2.3 소스 코드 npm init으로 간단하게 생성한 예제에서는 index.js 소스 파일만 생성해서 ‘hello world’를 출력하겠습니다. 2.4 첫 배포 NPM 리파지토리에 배포하기 위해서는 NPM 사이트에 계정이 있어야 하고 명령어 창에서 사이트로 로그인 이후에 배포할 수 있습니다. 2.4.1 사용자 등록하기 NPMJS 사이트에 계정이 없는 경우에는 사이트 에 접속해서 계정을 생성합니다. 명령어로도 바로 계정을 생성할 수 있습니다. 2.4.2 리파지토로에 배포하기 모듈 배포는 간단합니다. npm publish 명령어로 프로젝트 폴더에서 실행하면 됩니다. npm-publish-test는 이미 존재하는 프로젝트이여서 package.json에서 name을 ykoh-npm-publish-test로 변경한 이후에 다시 publish를 하였습니다. 이상 없이 배포되면 NPM 사이트에서도 바로 확인할 수 있습니다. https://www.npmjs.com/package/ykoh-npm-publish-test 2.5 재배포 코드 수정이후에 모듈을 재배포하려면 버전을 업데이트해야 합니다. 버저닝은 npm version 명령어를 사용해서 쉽게 업데이트를 하면 됩니다. minor 옵션을 주면 package.json 파일에서 version의 minor 숫자를 수정해줍니다. 버전에서 patch 부분을 업데이트합니다. 소스 수정하고 테스트하고 버저닝 올리고 배포하는 명령어를 일일이 타이팅하기보다는 배포하는 전체 과정을 package.json에서 스크립트화 하면 보다 전체 재배포 과정을 수월하게 진행할 수 있습니다. app-timer-pomodoro의 package.json 파일입니다. 버저닝 이후에는 다시 npm publish로 배포하면 됩니다. 2.6 리파지토리에서 제거하기 이미 publish한 패키지도 unpublish 명령어로 리파지토리에서 제거할 수 있습니다. 2.7 기타 사항 2.7.1 .npmignore .npmignore 파일에 리스트업된 파일이나 폴더는 배포에서 제외됩니다. .npmignore 파일이 없는 경우에는 .gitignore 파일을 대신 참조하여 배포에서 제외합니다. 제외시킬 파일들은 IDE 설정파일, test 관련 파일, log 파일 등이 있습니다. 3. 참고 - npm\n    - https://blog.outsider.ne.kr/829\n    - https://medium.freecodecamp.org/how-to-create-and-publish-your-npm-package-node-module-in-just-10-minutes-b8ca3a100050\n    - https://hackernoon.com/publish-your-own-npm-package-946b19df577e",
    "category": "node",
    "tags": [
      "npm",
      "node",
      "노드",
      "배포",
      "저장소"
    ],
    "date": "2019-03-24T00:00:00.000Z"
  },
  {
    "id": "python/gitbook으로-전자책-만들어보기",
    "slug": "python/gitbook으로-전자책-만들어보기",
    "title": "Gitbook으로 전자책 만들어보기",
    "excerpt": "",
    "content": "1. 개요 요즘은 콘텐츠 시대라고 해도 과언이 아닙니다. 특정 방송 회사가 콘텐츠를 만들기보다 개인이 직접 좋은 콘텐츠를 만들어 유튜브와 같은 플랫폼에서 퍼블리쉬하는 시대로 바뀌었습니다. 리디북스 와 같은 eBook 리더기가 보급되고 점점 활성화되면서 eBook 시장에도 개인이 직접 책을 만들 수 있는 여러 도구와 플랫폼이제공되고 있습니다. - 애플\n    - iBooks Author\n- 한글과 컴퓨터\n    - 의퍼블 (WePubl)\n- 교보 문고\n    - PubPle 본 포스트에서는 마크다운 기반의 전자책 집필 시스템인 GitBook에 대해서 알아봅시다. 1.1 주요 기능 - Markdown 언어로 집필 (ex. AsciiDoc, Markdown)\n- GitHub 저장소와 프로젝트를 연동하여 저장 가능\n- GitBook Editor 지원 - 웹, GUI(legacy 버전)\n- 여러 전자책 포멧 지원 (ex, PDF, EPUB, MOBI)\n- 여러 플러그인 지원 (ex. etoc, splitter) 2. Gitbook 설치 설치 방법은 맥 OS 기반으로 작성되었습니다. GitBook을 설치 하기 위해는 NodeJS가 기본적으로 설치되어 있어야 합니다. 없는 경우에는 아래 명령어로 설치합니다. NPM를 통해 gitbook 패키지를 설치합니다. 전자책 포맷과 PDF를 생성하려면 ebook-convert 명령어가 필요합니다. \\$PATH 환경변수에 /usr/local/bin 폴더가 포함되어 있어야 합니다. 3. 사용방법 3.1 첫 GitBook 프로젝트 만들기 아래 명령어로 책의 boilerplate를 생성합니다. 기본적으로 README.md와 SUMMARY.md가 생성됩니다. 기본적인 boilerplate 대신 조금 더 구체적인 예를 보고 싶으면 아래 github에서 샘플 예제를 다운로드해서 실행해보세요. 아래 명령어로 웹사이트를 생성하여 브라우저에서도 볼 수 있습니다. 3.2 eBooks과 PDF 생성하기 여러 전자책 포맷으로 출력할 수 있습니다. 4. 플러그인 GitBook의 여러 기능을 확장해주는 플러그인을 제공합니다. 어떤 플러그인은 있는지 찾는 방법과 설치 방법을 알아봅시다. 4.1 플러그인 찾는 방법 gitbook 플러그인 사이트 에서 원하는 기능을 가진 플러그인을 검색할 수 있습니다. 4.2 플로그인 설정 및 설치 루트 디렉터리에 있는 book.json 파일에 원하는 플러그인을 추가하고 필요하면 각 플러그인에 대한 설정도 같이해줍니다. 설정이후 추가한 플러그인을 아래 명령어로 설치합니다. 4.3 추천 플러그인 - etoc : 페이지에서 내용의 Table of Content를 자동으로 만들어주는 기능\n    - https://plugins.gitbook.com/plugin/etoc\n- splitter : 메뉴와 내용 중간 spliter를 움직이도록 해주는 기능\n    - https://www.npmjs.com/package/gitbook-plugin-splitter\n- expandable-chapters-small : > icon이 추가되어 클릭하면 확장되고 다시 클릭하면 축소되는 기능\n    - https://plugins.gitbook.com/plugin/expandable-chapters-small\n    -  - toggle-chapters : 한 chapter를 클릭하면 해당 chapter는 확장되고 나머지는 축소되는 기능\n    - https://plugins.gitbook.com/plugin/toggle-chapters\n    -  5. FAQ - 루트 디렉터리에 있는 README.md은 GitHub에서도 같이 사용된다. GitBook에서 다른 README.md로 설정하려면 어떻게 해야 하나?\n    - book.json를 아래와 같이 수정한다. 6. GitBook Pages Examples 대학에서나 개인 사이트로 GitBook을 많이 사용하고 있습니다. 아래 예제들을 통해서 어떤 다양한 플러그인을 사용하고 GitBook을 어떻게 꾸몄는지 알아보면 좋을 것 같습니다. - https://www.gitbook.com/book/jackdougherty/datavizforall\n- https://typescript-kr.github.io/ 7. 참조 자료 - https://help.gitbook.com/\n- https://toolchain.gitbook.com/\n- https://lab021.gitbooks.io/lab021manual/gitbook%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EC%82%AC%EC%9A%A9%ED%95%98%EB%82%98%EC%9A%94.html\n- http://blog.appkr.kr/work-n-play/pandoc-gitbook-%EC%A0%84%EC%9E%90%EC%B6%9C%ED%8C%90/",
    "category": "python",
    "tags": [
      "gitbook",
      "pdf",
      "epub",
      "kindle",
      "git",
      "github",
      "전자책"
    ],
    "date": "2018-07-29T00:00:00.000Z"
  },
  {
    "id": "python/pipx란-pip과의-차이점부터-설치-사용법까지-한눈에-정리",
    "slug": "python/pipx란-pip과의-차이점부터-설치-사용법까지-한눈에-정리",
    "title": "pipx란? pip과의 차이점부터 설치, 사용법까지 한눈에 정리",
    "excerpt": "",
    "content": "1. 개요 pipx를 사용해야 하는 이유 Python 패키지를 설치할 때 일반적으로 를 사용하지만, 일부 CLI(Application) 패키지는 전역적으로 설치하면서도 격리된 환경에서 실행하는 것이 더 적합할 수 있다. 이를 위해 를 사용하면 다음과 같은 장점이 있다. pip와의 차이점 | 특징           |                                             |                                |\n| -------------- | ------------------------------------------------ | ------------------------------------ |\n| 기본 설치 위치 | 가상환경 없음, 시스템 전역 또는 프로젝트 폴더 내 | 개별적인 가상환경에서 격리하여 설치  |\n| CLI 앱 실행    |  혹은 직접 실행              | 로 직접 실행 가능 |\n| 패키지 관리    | 프로젝트별 의존성 관리에 적합                    | 전역 CLI 도구 설치 및 관리에 적합    | 주요 기능 - 각 패키지를 별도의 가상환경에 설치하여 시스템 Python 환경을 오염시키지 않음\n- 을 사용해 별도 설치 없이 CLI 패키지 실행 가능\n- 설치된 모든 패키지를 한 번에 업데이트하는 기능 제공\n- 에 등록 2. pipx 설치 및 기본 사용법 2.1 pipx 설치 macOS에서는 로 설치하거나 의  명령어로 설치할 수 있다. 에서는  명령어를 실행하여 자동으로 환경 변수를 설정할 수 있다. 환경 변수 설정이 완료되면, 터미널을 재시작하거나  또는  명령어를 실행해야 현재 오픈된 셀에서도 바로 사용할 수 있다. 2.2 pipx 사용법 2.2.1 패키지 설치  명령어를 사용하면 해당 패키지가 격리된 가상환경에서 설치된다. 로 설치된 패키지는 list 명령어로 확인할 수 있다. 설치 후  명령어를 사용하여 실행 경로도 확인할 수 있다. 2.2.2 패키지 실행 설치된 패키지는 바로 실행할 수 있다. 2.2.3 패키지 제거 패키지 제거는  명령어로 제거한다. 설치된 모든 패키지를 한 번에 삭제하려면 다음 명령어를 사용한다. 3. 마무리 를 사용하면 CLI 도구를 격리된 환경에서 안전하게 관리할 수 있다. 특히, , ,  같은 글로벌 CLI 도구를 관리할 때 매우 유용하다. 앞으로 전역적으로 설치할 CLI 패키지는  대신 를 활용해 보자! 🚀 4. 참고 - 파이썬 애플리케이션 배포하기: pipx 편\n- pipx: 격리된 환경의 파이썬 앱 설치 및 실행 환경",
    "category": "python",
    "tags": [
      "python",
      "pip",
      "pipx",
      "파이썬",
      "패키지"
    ],
    "date": "2025-03-26T00:00:00.000Z"
  },
  {
    "id": "python/pypi-업로드-가이드-나만의-python-패키지-배포하기",
    "slug": "python/pypi-업로드-가이드-나만의-python-패키지-배포하기",
    "title": "PyPI 업로드 가이드: 나만의 Python 패키지 배포하기",
    "excerpt": "",
    "content": "1. 개요 Python 패키지를 개발한 후 (Python Package Index)에 업로드하면,  명령어만으로 누구나 쉽게 패키지를 설치할 수 있다. > 주식 투자할 때 개인적으로 구글 시트를 사용해.  함수로 많은 정보를 얻을 수 있지만, 제공되지 않는 데이터도 있어서 사용자 정의 함수를 구현해서 사용하고 있다.\n>\n> v1 stock-api에서는 직접 한글투자 API를 호출해서 데이터를 가져왔는데, v2 stock-api에서는 이미 구현된 Python 버전을 사용하면 시간을 절약할 수 있을 것 같아서 그걸로 대체하고 있다.\n>\n> 참고: 구글 시트에서 사용자 정의 함수 구현하기 이 글에서는  해서 사용하고 있는  모듈을 패키징하고 에 업로드하는 과정을 정리해서 설명한다. 2. Python 모듈 패키지화 프로젝트 구조 설명 파일 작성 (권장) 은 Python 패키지 설정을 정의하는 표준 파일이다. 과거 버전에서는  파일로 작성을 했지만, 지금은  포멧으로 정의를 한다. 패키지 빌드 패키지를 빌드하려면  모듈을 사용한다. 빌드가 완료되면  폴더에 과  파일이 생성된다. 3. Python 모듈 설치하기 3.1 로컬에서 패키지 설치 테스트 사용하려면 모듈에서 pip install로 설치한다. PyCharm IDE에서 설치하려면 아래 왼쪽에 위치한  아이콘을 클릭하고  >  을 클릭한 다음 빌드한 모듈 위치에서  파일을 선택하면 된다. 3.2 패키지를 PyPi에 업로드하기 1) 사전 작업 - PyPi 계정 생성: https://pypi.org/account/register/ - API 토큰 생성   :    https://pypi.org/manage/account/#api-tokens   - 참고로 2FA 를 활성화 해야 한다 -  파일에 API 토큰 설정 2) 을 이용한 업로드 업로드가 성공하면 PyPi에서 패키지를 확인할 수 있다. 업로드 후 패키지 테스트 4. 배포 및 유지보수 패키지 버전 관리 () > 은 Python 패키지의 버전을 자동으로 업데이트하는 도구이다.\n>\n> 설정된 규칙(예: , , )에 따라 버전 번호를 변경하고, 관련 파일(예: , )도 함께 수정해준다. Git 태그와 커밋까지 자동화할 수 있어서 패키지 관리가 편리해져서 사용하기 좋다. 위 명령어를 실행하면 버전 수정이 필요한 파일들에서 버전을 업데이트하고 까지 만들어준다. 5. 마무리 우리가 개발한 Python 모듈을 에 업로드하는 과정에 대해서 알아보았다. 명령어로 업로드를 했지만, GitHub Actions를 이용하면 를 푸시할 때 자동으로 에 업로드할 수 있다. - github actions 으로 pypi 패키지 배포 자동화하기 6. 참고 - pypi 등록하기",
    "category": "python",
    "tags": [
      "pypi",
      "python",
      "파이썬",
      "패키지",
      "패키지화",
      "모듈 배포",
      "모듈 업로드",
      "패키지 업로드",
      "패키지 배포",
      "bumpversion",
      "twine",
      "한국투자",
      "korea-investment-stock"
    ],
    "date": "2025-03-16T00:00:00.000Z"
  },
  {
    "id": "python/python으로-api-호출에-rate-limiting-적용하기-aiolimiter와-aiometer-사용법",
    "slug": "python/python으로-api-호출에-rate-limiting-적용하기-aiolimiter와-aiometer-사용법",
    "title": "Python으로 API 호출에 Rate Limiting 적용하기: aiolimiter와 aiometer 사용법",
    "excerpt": "",
    "content": "1. 개요 Rate limiting은 API 호출이나 서버 요청을 특정 시간 단위 내에서 제한하는 기법이다. 이는 서버 과부하를 방지하고, 과도한 요청으로 인한 장애를 예방하기 위해 사용된다. 예를 들어, 특정 API에서는 \"1초당 20건\"과 같은 방식으로 호출 횟수를 제한하기도 한다. 이러한 상황에서 Python으로 개발할 때, 효율적으로 rate limit을 적용할 수 있는 방법에 대해 알아보자. > 개인적으로 korea-investment-stock API를 사용하고 있고 한국투자 API 는 1초당 20건 호출 제약이 있어서 이 부분을 해결하기 위해 스터디를 하게 되었다. 2.  사용방법 2.1 란? 는 비동기 프로그래밍을 지원하는 rate limiter 라이브러리이다. 이는 와 함께 사용되어, 비동기적으로 여러 작업을 처리하는 동안에도 요청 수를 제어할 수 있게 도와준다. 는 특히 서버나 API 호출을 비동기적으로 처리할 때 유용하다. 사용 예제 다음은 를 사용하여 API 호출에 rate limit을 적용하는 예제이다. 이 코드에서는 를 사용하여 1초당 20번만 요청을 보낼 수 있도록 설정했다. 를 사용하여 비동기적으로 API를 호출할 때, 제한된 요청 횟수 내에서 안전하게 데이터를 받아올 수 있다. 3.  사용방법 3.1 란? 는 와 비슷하지만, 여러 개의 rate limiter를 한 번에 관리할 수 있는 기능을 제공한다. 여러 API를 동시에 호출하거나 여러 가지 다른 제약을 두어야 할 때 유용하다. 사용 예제 를 사용하여 비동기 요청에 rate limit을 적용한 예시이다. 은 1초에 최대 20번의 호출만을 허용하도록 설정했고 추가로 동시 실행 개수는 5개로 제한을 두었다. 4. 마무리 두 라이브러리 모두 비동기 처리가 가능한 환경에서 매우 유용하게 사용될 수 있다. 의 경우 동시에 실행할 수 있는 작업의 최대 수를 제한할 수 있어 과도한 I/O를 방지할 수 있다는 장점이 있다. | 기능                              |   |                  |\n| --------------------------------- | ----------- | ---------------------------- |\n| 속도 제한 (Rate Limit)            | ✅ 지원      | ✅ 지원                       |\n| 동시 실행 개수 제한 (Concurrency) | ✅ 지원      | ❌ 지원 안 함                 |\n| 복잡도                            | 비교적 간단 | 직접 로 제어해야 | 5. 참고 - 파이썬 Async API Call 에 Rate Limit 적용하기",
    "category": "python",
    "tags": [
      "rate limit",
      "aiolimiter",
      "rate limiting",
      "aiometer",
      "python",
      "파이썬"
    ],
    "date": "2025-04-05T00:00:00.000Z"
  },
  {
    "id": "python/도커라이즈-파이썬-어플리케이션-dockernize-python-application",
    "slug": "python/도커라이즈-파이썬-어플리케이션-dockernize-python-application",
    "title": "도커라이즈 파이썬 어플리케이션 (Dockernize Python Application)",
    "excerpt": "",
    "content": "1.Dockernize Python Application 파이썬 어플리케이션을 도커 이미지로 생성하는 방법에 대해서 알아보자. 도 커 라이 저하는 과정은 개발언어와 상관없이 비슷한 과정을 통해서 도커 이미지를 만든다. 아래는 를 화면에 출력하는 파이썬 코드이다. 이 코드로 도커 이미지를 만들어보자. 1.1 Dockerfile 생성하기 도커는  파일을 읽어 작성된 명령어를 실행하여 이미지를 만들 수 있다. -  : 베이스 이미지로 사용할 도커 이미지를 지정\n-  : 컨테이너 작업 디렉토리 지정\n-  : 이미지 생성시 파일 추가\n- CMD : 컨테이너 시작할 때 실행할 명령어 지정 파이썬 베이스 이미는 아래 3가지를 사용할 수 있다. 각각의 차이점은 다음과 같다. -   대부분의 필요한 패키지가 설치된 버전\n-  : 표준 라이브러리를 제외하고 전부 제외된 버전\n-  : BusyBox Linux + apk 패키지 관리자가 포함된 버전  베이스 이미지는 한 버전으로 여러 개발 언어로도 제공하여 많이 사용하는 베이스 이미지이다. 간단한 파이썬의 경우에는  이미지를 사용해도 무방하지만, application에서 사용하는 libray에 따라서 alpine에서는 기존 적으로 을 지원하지 않아 별도 빌드가 필요할 수 있다고 한다. 이런 빌드 과정을 거치지 않고 바로 도커나이즈하려면 나 를 베이스 이미지로 선택하기를 추천한다. 참고 - https://nayoungs.tistory.com/m/entry/Docker-Python-%EC%8A%A4%ED%81%AC%EB%A6%BD%ED%8A%B8%EB%A5%BC-%EC%8B%A4%ED%96%89%ED%95%98%EB%8A%94-Docker-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EB%B9%8C%EB%93%9C%ED%95%98%EA%B8%B0\n- https://pythonspeed.com/articles/alpine-docker-python/\n- https://jx2lee.github.io/cloud-base-image-with-python/ Docker 이미지 빌드하기 개인 맥북이  버전이라  옵션을 추가하여 빌드한다. 매번 빌드하기 쉽게 을 위와 같이 작성하고 을 하거나 버로 도커 허브에 이미지를 업로드라고 싶은 경오  옵션으로 실행한다. 1.2 Docker Image 실행하기 으로 Hello World를 찍어보자. 2. Flask Web Application 컨솔 창에서 출력하는 버전외에 웹 어플레이션도 도커나이즈해보자. 서버 구동후 8080 포트로 접속하면 \"Flask inside Docker!!\"를 출력하는 파이쎤 코드이다. Dockerfile 작성하기 빌드한 도커 이미지를 로컬환경에서 실행하고 8080 포트로 접속하면 잘 실행되는 것을 확인할 수 있다. 웹 화면 3. 참고 - https://runnable.com/docker/python/dockerize-your-python-application\n- https://luis-sena.medium.com/creating-the-perfect-python-dockerfile-51bdec41f1c8\n- https://blog.logrocket.com/build-deploy-flask-app-using-docker/",
    "category": "python",
    "tags": [
      "python",
      "docker",
      "dockernize",
      "도커",
      "파이썬",
      "도커라이즈"
    ],
    "date": "2022-06-05T00:00:00.000Z"
  },
  {
    "id": "python/웹-스크래핑하면서-차단-방지하는-방법",
    "slug": "python/웹-스크래핑하면서-차단-방지하는-방법",
    "title": "웹 스크래핑하면서 차단 방지하는 방법",
    "excerpt": "",
    "content": "1. 소개\n스크래핑하면 사이트에 접속하여 데이터를 추출해야 해서 어떻게 작성하느냐에 따라 서버에 많은 부하를 줄 수도 있게 됩니다. 웹 서버를 담당하는 측에서는 서버에 많은 부하를 줄이기 위해 악의적으로? 접속하는 곳을 차단할 수밖에 없습니다. 이번 포스트에서는 웹 스크래핑을 하면서 사이트로부터 차단되지 않는 여러 방법에 대해서 알아보도록 하죠.  robots.txt 체크하기\n User Agents 설정하기\n 잠시 sleep해서 부하 줄이기\n IP rotation - Tor 2. 웹 스크래핑시 차단 되지 않는 여러 방법\n2.1 robots.txt 체크하기\nrobots.txt 파일은 웹 크롤러(검색봇)가 크롤링을 할 수 있고 없고 하는 웹 페이지를 정의한 파일입니다. 많은 웹 사이트(ex. 구글, 네이버)에서 robots.txt 파일을 정의해두고 있습니다.  User-agent : 대상 크롤러 (: 모든 검색봇, googlebot: 구글봇 등등)\n Allow : 허용하는 경로\n Disallow : 허용하지 않는 경로 robots.txt 파일이 존재한다면, 접근하지 말아야 하는 경로가 무엇인지 미리 파악하여 웹 스크래핑시 해당 경로는 접근하지 않도록 주의를 해야 합니다. 2.2 User Agents 설정하기\n아래 사이트는 접속하는 브라우져 속성이 웹 서버에 어떻게 보여지는 지 잘 테스트할 수 있는 사이트입니다.  http://www.whatismybrowser.com 제 컴퓨터에서 크롬 브라우져로 위 사이트에 접속을 해보면, 맥 크롬 68 버전임을 알수 있습니다. 웹 사이트에 접속할때 서버로 보내는 HTTP 헤더를 확인해보면 다양한 정보를 담아서 보냅니다. 특히 User-Agent의 정보는 현재 사용하는 브라우져를 알수 있습니다. urllib 모듈로 사이트에 접속하면 HTTP 헤더 정보에는 python-urllib로 접속했음을 알수 있고 이 정보로 서버 측 담당자들은 쉽게 해당 접속이 일반 사용자가 아님을 판단할 수 있으므로 차단을 할 확률이 높습니다. 되도록이면 웹 스크래핑할 때는 urllib모듈은 사용하지 말고 requests 모듈을 사용하여 헤더 정보를 수정해 보내는게 좋습니다. 아래는 requests 모듈로 header 정보에 크롬 브라우져의 User-Agent를 만들어서 보내는 방법입니다. 2.3 잠시 sleep해서 부하 줄이기\n사용자가 실제 사이트에 접속해서 활동하는 것보다 더 빠르게 여러 페이지에 접속하고 온라인 폼을 채워서 스크래핑 한다면 일단 사용자가 아니라는 인식을 주게 되어 차단될 수 있습니다. 또한, 반복문으로 여러 페이지를 로딩하여 처리하거나 멀티 쓰레드 프로그래밍 방식으로 처리하면 서버에 부하를 많이 줄 수 있게 됩니다. 각 페이지에 접속하고 데이터 요청을 하는 건 최소한으로 하는게 좋습니다. 그러기 위해 sleep 문으로 각 페이지에 접속 할때는 조금의 간격을 두고 접속하면 부하를 줄일 수 있습니다. 2.4 IP rotation - Tor\n토르(Tor)란 The Onion Router의 약칭 트래픽 분석이나 IP 주소 추적을 불가능케 하는 암호화 라우터 네트워크입니다. 전송되는 데이터는 토르 네트워크를 라우팅할 때 마다 모든 경로에서 암호화가 되어 중간에 패킷을 얻어 풀려고 시도해도 실제 IP 소스 주소를 알아내기란 쉽지 않습니다. 거의 불가능? 하다고 설명하고 있습니다. (책: 파이썬으로 웹 스크래핑) 토르의 실제 동작 원리는 링크(참고 #4.1)를 참고해주세요. 여기서는 실제 어떻게 토르 네트워크안에서 웹 스크래핑을 할 수 있는지 다룹니다. 토르 설치 및 실행 내용은 맥 기반으로 작성 되었습니다. 다른 OS 설치는 아래 링크(참고 #4.5)를 참조하세요. 2.4.1 Tor 설치 tor 명령어로 토르는 실행할 수 있지만, 네트워크 트래픽은 아직 토르를 통해서 전송되지 않습니다. 2.4.2 Tor 설정 및 실행\n모든 시스템 트래픽이 토르를 통해서 라우팅 되려면 시스템의 네트워크 세팅을 변경해줘야 합니다. 사용하는(ex. Wi-Fi, Ethernet) 네트워크마다 세팅을 했다가 기본 네트워크를 사용하려면 다시 세팅을 원복해야 하는 번거로움 있습니다. 네트워크 세팅을 변경해주는 부분을 쉽게 bash 스크립트로 짤 수 있습니다(kremalicious 웹사이트 참고 #4.2). 아래는 tor.sh 스크립트를 실행한 화면입니다. 토르 세팅이 잘 되었는지 확인하려면 아래 사이트에 접속하여 확인할 수 있습니다.  https://check.torproject.org/\n  | 일반 네트워크로 접속 | 토르 네트워크로 접속 |\n  | -------------------- | -------------------- |\n  |   | | 현재 사용 중인 공개 IP 주소와 Tor IP 주소 또한 명령어로 확인할 수 있습니다. 토르 네트워크 확인을 위해서는 torsocks 패키지를 설치해야 합니다. 2.4.3 토르 네트워크에서 웹 스크래핑하기\n파이썬에서 토르 네트워크로 웹 스크래핑을 하려면 SOCKS proxy client 모듈이 필요합니다. pip 명령어로 설치합니다. torsocks 명령어로 얻은 IP 주소와 위 파이썬 스크립트로 실행한 주소가 같음을 확인할 수 있습니다. 3. 웹 스크래핑 작성시 고려사항\n3.1 코드 작성시 unit test으로 작성하기\n스크립트를 작성할때 매번 웹 사이트에 접근하여 코드를 작성해야 합니다. 매번 사이트에 접속하기 보다는 필요한 HTML 부분을 크룸 브라우저에서 복사하여 파일로 저장하고 해당 파일을 unit test로 작성하면 차단될 가능도 적어집니다. 저장된 HTML를 불러오는 코드 부분입니다. 전체 코드는 github에 업로드된 파일을 참조해주세요. 4. 참고 이 포스트에서 작성된 파일은 깃허브에서 확인할 수 있습니다.  관련 책\n     Python Web Scraping\n      \n     Web Scraping with Python\n      \n Robots.txt\n     http://www.robotstxt.org/robotstxt.html\n     https://www.twinword.co.kr/blog/basic-technical-seo/\n     http://www.searchengines.co.kr/mkt-bootcamp/7807\n 웹 스크래핑 차단 방지\n     https://www.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/\n     https://hackernoon.com/web-scraping-tutorial-with-python-tips-and-tricks-db070e70e071  Tor\n     https://namu.wiki/w/Tor(%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4)\n     https://kremalicious.com/simple-tor-setup-on-mac-os-x/\n     http://act.jinbo.net/wp/4449/\n     https://gist.github.com/DusanMadar/8d11026b7ce0bce6a67f7dd87b999f6b\n     Tor 윈도우 설치\n         https://guide.jinbo.net/digital-security/communication-security/how-to-use-tor",
    "category": "python",
    "tags": [
      "scrapping",
      "web",
      "python",
      "robots.txt",
      "Tor",
      "스크래핑",
      "웹",
      "파이썬"
    ],
    "date": "2018-08-13T00:00:00.000Z"
  },
  {
    "id": "python/파이썬-딕셔너리-리스트에서-특정-키-값으로-정렬하기",
    "slug": "python/파이썬-딕셔너리-리스트에서-특정-키-값으로-정렬하기",
    "title": "파이썬 딕셔너리 리스트에서 특정 키 값으로 정렬하기",
    "excerpt": "",
    "content": "1. 들어가며 파이썬에서 딕셔너리 리스트에서 특정 키 값(ex. age)에 따라서 정렬하는 방법에 대해서 알아보자. 리스트 정렬를 위해 파이썬에서 기본적으로 와  함수를 제공한다. - \n    - 리스트를 직접 수정하여  방식으로 정렬한다\n    - list 자료구조에만 사용할 수 있다 - sorted()\n    - 이 함수의 경우에는 정렬된 새로운 리스트를 반환한다\n    - iterable한 자료구조에도 사용 가능하다 2. 딕셔너리 리스트 특정 키 값으로 정렬하기 와  함수에 인자 값으로  함수를 지정할 수 있다. 이  함수의 반환 값이 compare 되어 정렬이 된다 2.1 키 함수로 람다 사용  함수로 딕셔너리에서 age값을 반환하는 람다함수를 지정하여 나이 순으로 정렬한다. 정렬된 결과를 확인해보면 나이 순으로 잘 정렬된 것을 확인할 수 있다. 2.2 키 함수로 itemgetter 사용 자료구조에서 키 나 속성 값을 쉽게 접근하도록 이라는 모듈에서 여러 함수를 제공해준다.  모듈에서  메서드를 사용하여  함수를 지정할 수 있다. 동일하게 나이 순으로 정렬이 된다. 2.3 여러 키 값으로 정렬시키기 여러 키 값(ex. age -> name) 기준으로도 정렬을 할 수 있다. 릭셔너리에서 나이가 같은 경우에는 이름으로 정렬을 해보자. 람다 함수와  함수 사용시 아래와 같이 지정해주면 된다. 딕셔너리에서 나이 순으로 정렬하고 같은 나이에 대해서는 이름으로 정렬이 된다. 3. 정리 sorted() 함수에서  함수를 지정할 수 있어 여러 키 값의 기준으로 정렬을 쉽게 할 수 있었다. 단일 키외에도 여러 키 값을 기준으로 정렬을 할 수 있다. 4. 참고 - https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary\n- https://www.geeksforgeeks.org/ways-sort-list-dictionaries-values-python-using-lambda-function/\n- https://www.geeksforgeeks.org/ways-sort-list-dictionaries-values-python-using-itemgetter/\n- https://wayhome25.github.io/python/2017/03/07/key-function/",
    "category": "python",
    "tags": [
      "python",
      "sort",
      "key",
      "dict",
      "파이썬",
      "정렬",
      "키",
      "딕셔너리",
      "리스트"
    ],
    "date": "2020-09-03T00:00:00.000Z"
  },
  {
    "id": "python/파이썬-커맨트-라인에서-명령어-옵션들-argparse-모듈를-이용해서-쉽게-파싱하기",
    "slug": "python/파이썬-커맨트-라인에서-명령어-옵션들-argparse-모듈를-이용해서-쉽게-파싱하기",
    "title": "파이썬 : 커맨트 라인에서 명령어 옵션들 argparse 모듈를 이용해서 쉽게 파싱하기",
    "excerpt": "",
    "content": "1. argparse 모듈이란? 셀이나 리눅스 명령어를 실행할 때 많은 옵션이 존재합니다. 아래는 pip 명령어(파이썬 패키지 관리자)의 옵션 목록입니다. Flag 형태의 옵션(ex. --no-color)이나 입력값을 받을 수 있는 옵션(ex. --log <path>)도 있습니다. 이런 옵션을 파이썬에서 구현하려면 어떻게 해야 할까요? 실제 구현한다면, 실행 명령어를 인자로 받아서 parse 하는 과정이 필요합니다. 직접 구현하기는 좀 부담스럽죠. 셀이나 여러 언어에서 이런 부분들을 별도의 모듈로 제공합니다. 파이썬에서는 커맨트parsing 라이브러리는 getopt, argparse, docopt가있습니다. 이 중에서 파이썬에서 많이 사용되는 argparse 를 알아보도록 하겠습니다. 2. argparse 옵션 알아보기 argparse는 파이썬 표준 라이브러리에 포함되어 있어 별도로 설치할 필요는 없습니다. 모듈을 사용하기 위해 import로 추가만 해주면 됩니다. 2.1 help 도움말 옵션 추가하기 커맨트 라인에서 대부분의 명령어는 도움말을 제공합니다. 옵션 -h 이나 --help 으로 명령어 옵션들에 대한 설명을 확인할 수 있습니다. argparse 모듈에서는 도움말 옵션은 기본으로 추가됩니다. -h 옵션을 주면 argparse 모듈에서 알아서 명령어에 대한 기본 설명과 옵션을 잘 정리해서 보여줍니다. 2.2 Flag 형태 옵션 추가하기 커맨트 라인에서 flag 형태의 옵션이 가장 많이 사용됩니다. Flag 옵션(-v, --verbosity)이 주어졌을 때 실행하는 옵션입니다. addargument() 함수를 통해서 여러 옵션에 대해서 설정할 수 있습니다. 아래와 같이 dash 입력으로 short와 long 옵션을 지정할 수 있고 해당 옵션에 대한 설명은 help= 키워드에 지정하면 됩니다. action 키워드에 ’storetrue’로 지정 하면 해당 옵션이 명시될 때 True 값이 지정되고 없으면 False로 지정됩니다. - Short Option : single dash (-)\n- Long Option : double dash (--) 2.3 옵션에 Value 값 입력 받기 특정 옵션에 대한 value 값도 입력을 받을 수 있습니다. 예를 들면, 아래와 같이 날짜, 숫자, String 값등을 입력받을 수 있습니다. - action=’store’ : action을 store로 지정하면 입력한 값을 dest 변수에 저장한다\n- dest : 입력한 값이 저장되는 변수이다\n    - ex. message 변수에 입력한 메시지가 저장된다\n- type: 입력한 값에 대한 타입도 지정할 수 있다\n    - 입력한 값이 정수가 아닌 경우에는 ‘error: argument -n invalid int value’ 오류가 출력된다 2.4 입력한 값에 대한 유효 검사하기 int나 String과 같은 타입(ex. int)에 대해서는 기본적으로 유효 검사를 해줍니다. 하지만, 날짜와 같은 타입은 별도의 함수 구현이 필요합니다. 입력한 날짜가 YYYY-MM-DD 포맷인지는 strptime() 함수를 실행해서 포맷이 아닌 경우에는 Exception을 띄우도록 해서 체크를 할 수 있습니다. 2.5 선택 집합 세트에서 입력값을 선택하기 choices 키워드로 정해진 선택 집합 세트에서만 입력값을 줄 수 있도록 설정할 수 있습니다. - choices : list에 정해진 값중에 하나면 선택할 수 있다\n    - ex. choices=[‘apple’, ‘orange’] 이 외에도 다양한 옵션 설정이 있지만, 자주 사용되는 옵션으로만 정리해보았습니다. 소스 코드는 github 를 참조해주세요. 3. 참고 - Parsing LIbraries\n- https://realpython.com/comparing-python-command-line-parsing-libraries-argparse-docopt-click/\n- Argparse 모듈\n    - https://docs.python.org/ko/3.7/library/argparse.html     - https://docs.python.org/3/howto/argparse.html",
    "category": "python",
    "tags": [
      "python",
      "argparse",
      "command",
      "option",
      "cli",
      "파이썬",
      "명령어",
      "옵션"
    ],
    "date": "2018-11-04T00:00:00.000Z"
  },
  {
    "id": "python/파이썬에서-람다-함수-익숙해지기",
    "slug": "python/파이썬에서-람다-함수-익숙해지기",
    "title": "파이썬에서 람다 함수 익숙해지기",
    "excerpt": "",
    "content": "1. 람다란? 파이쎤에서도 이름 없는 함수인 람다 표현식을 지원한다. syntax는 아래와 같다. 파이썬에서는 bracket (ex. { })을 지원하지 않아 single line으로만 작성해야 한다. 람다에서 multi-line을 작성하려면 별도 함수로 빼서 작성하면 된다. 1.1 람다 표현식으로 함수 생성 및 호출해보기 람다 함수를 생성하고 호출해보자. 람다는 변수에 저장할 수 있다. 호출은 일반함수와 동일하다. 변수에 저장하지 않고도 람다 함수를 정의하고 인자로 넘겨주어 바로 호출할 수 있다. 람다 함수에서 2개 이상의 인자를 갖는 함수의 예제이다. 1.2. Map, Filter, Reduce 예제 파이썬에서 람다를 인자로 받는 대표적인 함수들에 대해서 알아보자. 1.2.1 map  함수는 를 의 에 적용하는 함수이고 리스트의 데이터 형태를 변경할 때 자주사용하는 함수이다. syntax는 다음과 같다. 0  4값의 리시트의 각 에 +1을 하는 예제이다. 1.2.2 filter  함수도 입력리스트의 에 입력함수를 적용하고 결과적으로 참을 반환하는 만 처리하는 함수이다. 리스트에서 5 이상인 경우의 데이터를 반환하는 예제이다. 1.2.3 reduce  함수는 리스트의 첫번째, 두번째 을 인자로 받아 하나의 값을 반환하고 최종적으로 단일 값을 반환하는 함수이다. reduce 함수를 통해서 최종 합을 구하는 예제이다. 2. 정리 파이썬에서 지원하는 람다 표현식에 대해서 알아보았다. 람다 함수는 이름 없는 함수로 별도의 함수로 정의하지 않고 간단하게 작성할 수 있는 장점이 있다. 이런 이유로 람다 함수를 인자로 넘겨주어 자주 사용한다. 3. 참고  https://wikidocs.net/64\n https://dojang.io/mod/page/view.php?id=2359\n https://book.pythontips.com/en/latest/mapfilter.html",
    "category": "python",
    "tags": [
      "python",
      "lambda",
      "anonymous",
      "function",
      "파이선",
      "람다",
      "이름없는함수",
      "익명함수"
    ],
    "date": "2020-08-30T00:00:00.000Z"
  },
  {
    "id": "python/파이썬으로-웹-스크래핑하는-방법-web-scraping",
    "slug": "python/파이썬으로-웹-스크래핑하는-방법-web-scraping",
    "title": "파이썬으로 웹 스크래핑하는 방법 - Web scraping",
    "excerpt": "",
    "content": "1. 소개 웹 정보 바다라고 할 만큼 엄청난 양의 데이터를 가지고 있습니다. 트위터, 페이스북과 같은 사이트에서는 정규화된 JSON 형태의 데이터를 API로 제공해서 쉽게 원하는 데이터에 접근할 수 있습니다. 하지만, API를 통해서 제공되는 데이터는 제한적이고 원하는 데이터를 얻지 못할 수도 있습니다. 필요한 정보를 실제 사이트에서 직접 데이터를 추출해서 데이터를 가공할 필요가 있습니다. 이런 방식을 웹 크롤링(Web Crawling), 웹 스크래핑(Web Scraping)이라고 합니다. 웹 크롤링은 웹 스파이터(spider), 봇(bot)이라고 해서 검색 엔진과 같은 여러 사이트(ex. 구글)에서 정보를 정기적으로 추출하는 방식입니다. 웹 스크래핑은 웹 사이트로에서 원하는 데이터를 추출하는 행위를 일반적으로 얘기합니다. 둘의 차이점을 정리하면 아래와 같습니다.  웹 크롤링\n     검색 엔진에서 사용되며 bot과 같이 자동으로 웹 처리됨\n     다운로드한 사이트를 index하여 사용자가 빠르게 원하는 것을 검색할 수 있도록 해줌\n 웹 스크래핑\n     웹 사이트에서 원하는 데이터를 추출함\n     추출한 데이터를 원하는 형식으로 가공함 2. 웹 스크래핑\n파이썬이 웹 스크래핑을 하는 데 가장 많이 사용됩니다. Nodejs에서도 Cheerio 모듈 을 사용해서 쉽게 원하는 데이터를 추출할 수 있지만, 본 포스트에서는 파이썬으로 웹 스크래핑하는 방법을 알아봅니다. 웹 스크래핑을 할 때는 3가지 정도의 단계를 거치게 됩니다. 1. Scraping - 데이터 가져오기\n2. Parsing - 데이터 파씽\n3. Manipulation - 데이터 가공 먼저 필요한 파이썬 모듈을 설치하고 각 모듈의 사용법을 알아봅시다. 2.1 필요한 패키지 설치 및 사용방법\n설치 방법은 맥 OS 기반으로 작성되었습니다.  Beautiful Soup\n     HTML과 XML 형식의 데이터를 보다 쉽게 파씽하고 다루는 모듈\n     현재 버전은 bs4\n urllib\n     URL를 다루는 모듈\n     파이썬에 기본적으로 내장되어 있는 모듈임\n requests\n     HTTP/1.1 요청를 보낼 수 있음\n     요청 내용에 헤더, 폼 데이터, multipart 파일과 parameter를 포함해서 보낼 수 있음 2.1.1 패키지 설치\n파이썬 패키지 관리자 명령어(pip)로 필요한 패키지를 설치합니다. 2.1.2 사용법과 예제\n먼저 파이썬에 기본적으로 내장된 urllib 모듈을 사용해서 데이터를 가져와 보고 이어서 requests 모듈로 데이터를 가져오는 예제를 작성해봅니다. 전체 예제 소스는 github 에 작정 되어 있습니다.\n위키백과 사이트에서 요즘 화제 페이지의 주요 뉴스 정보를 가져오는 예제를 같이 작성해봅시다.  https://ko.wikipedia.org/wiki/포털:요즘화제 1. 크롬의 개발자 도구를 열어 원하는 부분의 태그를 확인합니다. 2. 웹사이트에 접근하여 BeautifulSoup 로 HTML를 파씽하고 원하는 데이터를 추출합니다. 아래 코드는 urllib 모듈로 위키백과 사이트에 접근하는 방식입니다. 다음은 urllib 모듈 대신 requests 모듈을 사용하여 html을 가져오는 방식입니다. 스크립트를 작성하면 어쩔 수 없이 해당 웹 사이트에 자주 접속할 수 밖에 없는데, urlib 모듈로 접근하면 서버 로그에 urllib로 접속한다는 정보가 고스란히 남게 되고 또한 자주 접근하는 패턴으로 차단될 리스크가 있습니다. 하지만, requests 모듈은 headers에 추가 정보를 담아서 보낼 수 있어서 크롬이나 파이어폭스 브라우저가 보내는 정보를 담아서 보낼 수 있어 차단될 가능성이 적어 requests 모듈을 사용할 것을 추천합니다. 블랙 리스트에 등록되는 것을 피하는 여러 방법은 다음 포스트에 구체적으로 다루도록 하겠습니다. HTML을 가져온 다음 BeautifulSoup에서 어떻게 파씽을 하고 원하는 데이터를 추출하는지 아래 코드를 보면서 알아봅시다. 라인 1에서는 파이썬에 기본으로 들어 있는 html.parser로 HTML를 파씽합니다. lxml등과 같은 외부 파써도 사용할 수도 있습니다. (pip로 설치 필요)\nex. bsObj = BeautifulSoup(html, ‘lxml’) 라인 2부터는 주요 뉴스를 포함하고 있는 vevent 클래스의 내용 전체를 먼저 얻어오고 한 번 더 tr 부분을 추출해 옵니다. tr [0] - 메인 타이틀\ntr [1] - 뉴스 내용 얻어온 태그 내용의 텍스트 부분을 추출하려면 gettext() 함수를 이용하고 불필요한 whitespace는 strip()나 replace()함수로 제거합니다.\n실행 결과는 다음과 같습니다. 3. 추가 예제\n인터넷상에서 많은 데이터가 존재하기 때문에 웹 스크래핑 기술로 다양한 데이터를 만들어 낼 수 있습니다.  같은 한 제품의 가격에 대해 비교 할 수 있도록 스크래핑 (ex. 다나와)\n 여러 소셜 네트워크(ex. 트위터)에서 회사의 제품에 대한 feedback을 얻을 수 있도록 스크래핑 저는 개인적으로 리디북스 페이퍼(이북 리더기)에서 성경을 종종 읽고 싶어서 EPUB를 만들면 좋겠다고 생각을 했었습니다. 웹 스크래핑 기술을 익히고 나서 EPUB 포맷으로 변환하는 스크립트를 작성했습니다. 아이디어는 위 예제와는 크게 차이는 없습니다. https://github.com/kenshin579/app-korean-catholic-bible 4. 정리 웹 스크래핑을 하다 보면 접속하는 사이트로부터 차단될 수 있어서, 어떻게 하면 차단 당하지 않고 웹 스크래핑할 수 있는지 다음 포스트(웹 스크래핑하면서 차단 방지하는 방법)에서 알아보도록 하죠. 5. 참고 조금 더 아래 책을 추천드립니다.  책 : Web Scraping with Python\n     \n 스콜링 vs. 스크래핑\n     http://stophyun.tistory.com/142\n     https://ko.wikipedia.org/wiki/웹크롤러\n Nodejs로 웹 스크래핑\n     https://medium.com/@moralmk/web-scraping-with-node-js-9a289ad19558",
    "category": "python",
    "tags": [
      "web",
      "scrap",
      "python",
      "cheerio",
      "bs4",
      "soup4",
      "웹",
      "스크래핑",
      "파이썬"
    ],
    "date": "2018-08-05T00:00:00.000Z"
  },
  {
    "id": "ros/urdf를-이용한-로봇-모델링",
    "slug": "ros/urdf를-이용한-로봇-모델링",
    "title": "URDF를 이용한 로봇 모델링",
    "excerpt": "",
    "content": "1. URDF이란? (Unified Robot Description Format)는 로봇의 원, 타원, 직사각형 따위의 기하학적 모델, 관절, 센서 등의 정보를 정의하는 XML 형식의 파일이다. 이 XML 파일에서 로봇의 부분을 나타내는 링크(link)와 동적인 움직임을 갖는 조인트(joint) 등 정보를 정의해서 로봇을 모델링 한다. 모델링한 정보는 RViz(ROS Visualization), Gazebo와 같은 시각화 프로그램을 사용해서 로봇 모델을 확인하고 시뮬레이션 해볼 수 있다. 로봇의 모델링, 시뮬레이션 및 제어를 위해 외에도 아래와 같이 여러 파일 형태가 존재한다. - URDF: Unified Robot Description Format\n    - 주로 로봇의 모델링 및 시뮬레이션에 사용되고 RViz 도구에서 사용된다\n- SDF: Simulation Description Format\n    - 로봇 시뮬레이션에 사용되는 XML 형식의 파일로, Gazebo 와 같은 시뮬레이션 도구에서 사용된다\n    - Gazebo 변환 명령어를 이용해서 URDF → SDF 변환도 가능하다\n      \n- SRDF: Semantic Robot Description Format\n    - 와 함께 사용되며, 로봇의 그룹, 경로 계획, 충돌 검사 등의 정보를 정의하는 XML 형식의 파일이고 MoveIt 에서 사용된다\n    - Setup Assistant 프로그램을 통해 URDF → SRDF로 쉽게 변환 가능하다 2. 모델링 하려는 로봇 정보  - 매니플레이터 > 매니플레이터(manipulator)는 로봇의 한 유형으로 인간의 팔과 유사한 동작을 하는 로봇이다.\n>  <br>\n<br> URDF 으로 모델링할 로봇은 매니플레이터이다. 매니플레이터의 기본 구조는 다음과 같이 기저, 링크, 조인트, 말단 장치로 구성되어 있다. 매니플레이터의 기본 구조 - 기저(base): 매니퓰레이터가 고정되어 있는 부분\n    - 목적에 따라 작업 공간내 바닥이 될 수 있음\n    - 동적으로 움직이는 모바일 로봇이 될 수도 있음\n- 링크(link) : 기저, 조인트, 말단 장치를 연결해주는 강체 역할\n    - \\- 쉽게 말해 프레임이라고 보면 됨\n- 조인트(joint) : 로봇의 움직임을 만들어내는 부분\n    - 회전(revolute) 운동형과 병진(prismatic) 운동형 등의 동적 움직임을 가짐\n- 말단 장치 (end effector) : 사람의 손에 해당하는 부분 또는 장비\n    - 집거나 흡착하여 들어올리거나 페인트를 분사하는 등 로봇의 목적에 따라 달라짐 구성 요소에서 가장 중요한 부분은 링크와 조인트 부분이다. - 링크 = 링크의 이름, 외형, 무게(mass, kg), 관성 모멘트(Kg.m^2)\n    - 외형: 원통, 원뿔, 직육면체등의 간단한 모델을 쓴다. 복잡한 구조는 메쉬 (mesh)을 표현하는 , () 포맷을 사용한다\n- 조인트  = 조인트의 이름, 종류, 운동의 기준 축, 최소, 최대 조인트 값, 조인트에 부여되는 힘 / 속도\n    - 조인트는 전/후 링크와의 관계를 기술한다\n    - 아래와 같이 여러 조인트 종류로 정의할 수 있다 3. Description 패키지 생성하기 위에서 언급한 매니플레이터의 기본 정보를 URDF로 작성해보자. ROS 커뮤니티에서는 로봇의 모델링 정보를 담은 패키지를  라는 이름으로 주로 사용한다. - ROS Index에서 으로 검색하면 이미 작성된 여러 URDF을 예제로 확인해볼 수 있다 우리가 작성할 패키지를 생성하자. 3.1 전체 URDF 예시    3.2 RViz에서 렌더링된 화면 4. URDF 작성하기 URDF는 로봇을 설명하기 위한 XML 명세서로 XML 태그를 이용해서 로봇의 각 구성 요소를 기술한다. 이 명세서는 최대한 일반적으로 명시하도록 최대한 설계 되어 있지만, 이 명세서로는 모든 로봇을 설명할 수는 없다. 주요 제약 사항은 트리 구조만 표시할 수 있으며 병렬적으로 동작하는 로봇은 모델이 어렵다. 그리고 로봇이 조인트는 단단한 링크로 구성되어 있다고 가정하며 유연한 요소는 지원하지 않는다. > URDF XML에 대한 스펙은 ROS 문서에서 확인 할 수 있다. https://wiki.ros.org/urdf/XML 4.1 \n 태그는 로봇 설명 파일의  요소로 제일 먼저 선언이 되어야 한다. 로봇에 대한 설명은 링크 요소와 링크를 서로 연결하는 조인트 요소 집합으로 구성된다. 4.3 \n 태그는 일반적으로 로봇의 각 파트 (ex. 본체, 팔, 다리, 휠 등)을 표현하는데 사용되고 각 링크는 조인트를 통해 다른 링크에 연결이 된다. 매니퓰레이터의 구성 요소 그 첫 번째인 기저는 URDF에서 링크로 표현한다. 기저는 첫번째 링크와 조인트로 연결되어 있으며 이 조인트는  타입으로 설정되어 움직이지 않게 고정되어 있고 원점(0.0,0.0,0.0)에 위치해 있다. <br>\n<br> 는 시각화 , 충돌  , 관성태그로 구성되어 있다. - common tag     -  \\- 원점 좌표\n        - 좌표는  좌표계를 통해 로봇의 위치를 3차원 공간 상에서 표현을 하고  오일러 각을 통해 로봇의 방향을 표현한다\n        - 단위는  (meter),  (radian)을 사용한다\n- \n    -  태그는 origin 좌표 중심으로 표시 범위와 모양과 크기를 적는다\n        - 모델의 모양 입력은 box, cylinder, sphere 형태를 기본으로 제공한다\n        - 표현하기 어려운 모델인 경우에는 STL, DAE등의 CAD 파일을 입력할 수도 있다\n- \n    -  태그에는 링크의 간섭 범위를 나타내는 정보를 입력을 한다\n    -  와  태그는 위에서 언급한 내용과 동일한데 표시 범위가 아닌 간섭 범위로  태그의 표시 범위보다 더 크게하여 안전을 더 고려할 수도 있다\n- \n    - 이 태그로 물리적 속성인 질량과 관성 텐서 (inertial tensor) 값을 지정한다\n    -  태그는 링크의 무게 (mass, 단위: kg)\n    -  태그는 관성 모멘트 (moments of inertia, 단위:kg·m^2)는 3x3 회전 관성 행렬로 정의를 한다. 대칭이라서 아래 밑줄친 것만 정의를 한다       | ixx  | ixy  | ixz  |\n          | ---- | ---- | ---- |\n      | ixy  | iyy  | iyz  |\n      | ixz  | iyz  | izz  |   > 3D insertia tensor 값 예제 4.3.1   태그는 링크의 색상(color)이나 텍스처(texture)를 지정하는 데 사용된다. -  태그로 색상을 지정한다\n    - 빨강, 초록, 파랑에 해당하는 0.0  1.0 사이의 숫자를 각각 기입하여 설정할 수 있다\n    - 마지막 숫자는 투명도(알파)로 0.0  1.0 값을 가지며 1.0 이면 투명 옵션을 사용하지 않은 고유 색상을 그대로 표시하는 상태를 의미한다\n-  텍스쳐는 파일(ex.png)로 지정한다 4.4   태그는 로봇의 조인트를 정의하는 데 사용되고 각 조인트는 두 링크를 연결하며 어떤 방식으로 움직일 수 있는지 정의한다. URDF 에서 지원하는 조인트 종류 <br> -  (고정): 움직임이 허용되지 않는 관절이다\n-  (회전): 선풍기의 좌우 회전과 같이 일정 각도 범위를 회전하는 관절이다\n-  (연속): 자동차 바퀴처럼 연속 회전을 하는 관절이다\n-  (프리즘): 단일 축에 대해 선형으로 미끄러지는 관절로 최대, 최소 위치 제한을 가진다\n-  (자유): 6차원 이동 및 회전을 허용하는 관절이다\n-  (평면): 한 평면에 수직으로 이동 및 회전할 수 있는 관절이다 <br>\n<br> <br>\n<br> - , \n    - 연결하는 링크로는 부모 링크(parent link)와 자식 링크(child link)의 이름을 지정하는데 일반적으로 기저에 가까운 링크를 부모 링크라고 보면 된다\n- \n    - 회전 축을 정의한다. 축은 로봇의 기준 좌표를 기준으로 정의되며, 회전 운동의 방향을 결정한다.\n        -  x, y는 zero 값으로 설정하고 z 값만 설정을 해서  와  조인트의 경우에는 선풍기 목부분 z 축 기준으로 회전을 하는 설정이다\n- \n    - 조인트 동작에 대한 제한 사항을 설정한다.\n        - 속성은 최소, 최대 조인트 (lower, upper, 단위: radian), 조인트에 부여되는 힘(effort, 단위: N), 속도 (단위: rad/s)의 제한 값을 설정한다\n    - 2.617 radian는 약 150도이다 > 각 태그와 각 설정하는 값에 대해서 이해하려면 직접 값을 변경해가면서 RViz에서 어떻게 렌더링되고 로봇이 동작을 하는지 테스트를 해보면 URDF를 이해하는 데 도움이 된다 참고 - https://wiki.ros.org/urdf/XML/link\n- Adding physical and collision properties\n- https://duvallee.tistory.com/11 5. URDF 런치 파일 생성 및 실행 5.1 런치 파일 생성  나  와 같은 명령어(참고: 8. URDF 파일 validation 체크)를 통해서 작성한 URDF 의 syntax나 렌더링 된 모습을 간단하게 확인할 수 있지만, 더 자세한 모델은 RViz에서 확인하는 게 제일 베스트이다. 이를 위하여 우선 다음 예제와 같이  패키지 폴더로 이동해  파일을 생성해보자. <br>\n<br> <br>\n<br> URDF를 담은  파라미터와 ,  ,  노드로 구성되어 있다. -  노드\n    - 파라미터 서버의  파라미터에 URDF가 주어지면 이 노드는 URDF의 모든 이동 가능한 관절에 대한 기본 값을  토픽에  메시지 형태로 지속적으로 publish를 한다\n    - Published Topic\n        -  () - 시스템에 있는 모든 움직이는 조인트 상태 값\n    - \n      조인트에 명령을 주는 GUI 툴을 제공한다         -  -  노드\n    - URDF에 설정된 로봇 정보와  토픽 정보를 가지고 계산한 kinematic tree model (TF)와 3D 위치 정보를  ,  토픽에 publish를 해주는 역",
    "category": "ros",
    "tags": [
      "urdf",
      "rviz",
      "sdf",
      "srdf",
      "moveit",
      "gazebo",
      "로봇",
      "모델링",
      "ros",
      "ros2",
      "robot"
    ],
    "date": "2024-02-24T00:00:00.000Z"
  },
  {
    "id": "spring/h2-데이터베이스-사용법-및-intellij에서-database-연동",
    "slug": "spring/h2-데이터베이스-사용법-및-intellij에서-database-연동",
    "title": "H2 데이터베이스 사용법 및 Intellij에서 Database 연동하기",
    "excerpt": "",
    "content": "1. 들어가며 H2는 자바로 구현된 오픈소스 데이터베이스입니다. 인 메모리와 파일 기반의 데이터베이스 설정이 가능합니다. 자바 애플리케이션에 임베디드해서 사용하거나 서버 모드로 구동할 수 있습니다. 별도의 설치과정 없이 임베디드로 바로 사용할 수 있는 장점으로 많이 사용되는 DB입니다. 이 포스팅에서는 H2에서 제공하는 여러 모드 외에도 웹 콘솔과 Intellij에서 H2에 연동하는 방법도 같이 알아보겠습니다. - 임베디드 모드\n    - 메모리\n    - 파일\n- 서버 모드 - 여러 도구에서 같은 DB에 접속이 가능하다 2. 개발 환경 작업시 사용한 개발 환경과 소스코드입니다.  OS : Mac OS\n IDE: Intellij\n Java : JDK 1.8\n Source code : github\n Software management tool : Maven 3. Spring Boot에서 H2 DB 사용해보기 3.1 JPA 샘플 코드 작성 스프링 부트와 H2 DB간의 연동를 위해 pom.xml 파일에 H2 라이브러리를 추가해야 합니다. H2의 여러 환경 테스트를 위해서 간단한 JPA 샘플 코드를 작성해두겠습니다. JPA에서 제공하는 DDL 자동 생성 옵션(jpa.hiberate.ddl-auto)과 초기 데이터 로딩이 되도록 세팅하면 H2에 대한 세팅을 쉽게 확인할 수 있어서 관련 코드와 설정을 먼저하겠습니다. 간단하게 JPA에서 사용할 Book 엔티티를 생성합니다. 초기 데이터가 DB에 삽입되도록 src/main/resources/data.sql 파일을 생성해 둡니다. 서버 구동시 data.sql 스크립트가 실행 되면서 데이터가 로딩됩니다. API로도 호출해보기 위해서 BookController과 BookRepository 파일도 같이 생성했습니다. 두 파일은 github에 올라간 소스코드를 확인해주세요. 3.2 H2 데이터베이스 설정 3.2.1 In-Memory datasource 값은 다른 DB 설정할 때와 유사합니다. - spring.datasource.url\n    - Connection URL - 참고 : Database URL\n        - H2에서는 다양한 url 형식을 지원하여 여러 모드와 세팅을 할 수 있다\n    - 예제.\n        - 엠비디드 연결 : \n        - 인 메모리 : \n        - 서버 모드 : \n    - 추가 옵션 - 옵션에 대한 자세한 사항은  H2 홈페이지를 참고해주세요\n        - MODE\n            - H2에서는 다른 여러 DB처럼 동작 가능하도록 호환모드를 지원한다. 완벽하게 모든 기능을 지원하지는 않는다\n            - ex. MODE=mysql (ex. CREATE TABLE 구문에서 INDEX()와 KEY()를 사용할 수 있게 됨) 인 메모리는 메모리에만 데이터가 저장되기 때문에 애플리케이션 구동 시에만 존재합니다. 3.2.2 File로 설정 DataSource 을 파일로 설정하면 애플리케이션이 종료되어도 데이터를 계속 남아 있습니다. Connection URL 형식은 파일 형식으로 작성하면 파일로 저장됩니다. 3.3 스프링 부트 구동해서 API 호출해보기 이제 스프링 부트를 구동해보고 각 설정에 따라 이상이 없는지 체크해보겠습니다. Postman을 사용해서 http://localhost:8080/api/book/list API를 호출하면 DB에 추가된 데이터를 응답 값으로 확인할 수 있습니다. 4. DB Client로 접속하기 DB 관련 작업을 수월하게 하기 위해 대부분 별도의 DB client로 접속해서 작업합니다. H2 웹 콘솔과 Intellij IDE에서 접속해보겠습니다. 4.1 H2 웹 콘솔 H2에서 웹 콘솔을 제공합니다. 웹 콘솔을 사용하기 위해서는 pom.xml에 spring-boot-devtools을 추가해줘야 합니다. application.yml에서 spring.h2.console.enabled를 true로 설정해서 웹 콘솔을 활성화합니다. http://localhost:8080/h2-console로 접속하면 다음 화면을 볼 수 있습니다. JDBC URL 설정하고 연결 버튼을 클릭하면 DB에 접속하게 됩니다. 이 콘솔 안에서 query를 실행하여 데이터를 확인할 수 있습니다. 4.2 Intellij Database 도구 다음은 Intellij Database 도구로 접속해보겠습니다. Intellij IDE를 열고 오른쪽 사이드바에서 Database를 클릭합니다. Data Source를 H2로 선택하고 아래와 같이 데이터를 입력합니다. 클릭하면 데이터베이스가 이미 사용 중이라 접속이 안 된다는 경고 창이 뜹니다. 메모리와 파일인 경우에는 동시에 접근을 할 수 없습니다. 같은 DB에 여러 곳에서 연결하려면 서버 모드로 접속을 해야 합니다. 5. H2 DB 서버 모드로 접속하기 H2 DB 서버 모드로 접속하는 방법에 대해서 알아보겠습니다. 5.1 설정 파일 추가 서버 모드로 접속하기 위해서 아래 스프링 빈을 등록해줍니다. initMethod와 destroyMethod 인자로 정의된 메서드는 스프링에 의해서 해당 빈이 초기화되거나 제거될 때 각각 start, stop 메서드가 실행되어 H2 DB를 시작하고 종료시킵니다. - tcp : TCP 서버로 H2가 실행되도록 설정한다\n- tcpAllowOthers: 다른 외부에서 접속 가능하게 하는 설정이다\n- tcpPort : 포트 번호를 지정한다 5.2 Intellij Database로 연결 재시도 Intellij Database 도구에서 다시 접속을 시도하면 이상없이 로드 되는 것을 확인할 수 있습니다. 6. 정리 스프링 부트에서 H2 DB를 연동하는 방법에 알아보았습니다. 인 메모리와 파일 등으로 DB를 생성할 수 있었습니다. 단 단일 연결로는 하나만 허용되지만, 다중으로 연결하려면 서버 모드로 접속해야 하는 것도 확인하였습니다. 다음 포스팅에서는 Unit Test 실행 시 H2 DB를 활용해서 Unit Test를 실행할 방법에 대해서 알아보겠습니다. 7. 참고  H2 설치 및 사용법\n     https://en.wikipedia.org/wiki/H2(DBMS )\n     https://jojoldu.tistory.com/234\n     https://www.tutorialspoint.com/h2database/\n     http://www.h2database.com/html/cheatSheet.html\n Spring Boot\n     https://engkimbs.tistory.com/794\n 서버 모드\n     https://www.baeldung.com/spring-boot-access-h2-database-multiple-apps\n     https://jehuipark.github.io/java/springboot-h2-tcp-setup",
    "category": "spring",
    "tags": [
      "h2",
      "database",
      "spring",
      "spring boot",
      "intellij",
      "In-Memory",
      "인메모리",
      "DB",
      "데이터베이스",
      "인텔리제이"
    ],
    "date": "2019-11-20T00:00:00.000Z"
  },
  {
    "id": "spring/handler-method-argument-resolver-이란",
    "slug": "spring/handler-method-argument-resolver-이란",
    "title": "Custom Handler Method ArgumentResolver 만들어보기",
    "excerpt": "",
    "content": "1.HandlerMethodArgumentResolver란? 1.1 들어가면 에 대해서 알아보자. 아래와 같이 컨트롤러 메서드에 여러 인자 값(ex. )을 추가하여 자주 작업을 한다. 이런 인자는 에 의해서 처리가 된다. 필요에 따라서 컨트롤러 메서드에 여러 인자 값을 추가하는데 이런 인자는 에 의해서 처리가 된다. 는 어노테이션이나 타입에 따라서 실제 값을 컨트롤러에 넘겨주는 역할을 한다. 스프링에서도 기본적으로 여러 Argument Resolver가 구현되어 있다. - \n    -  어노테이션으로 선언된 인자를 처리하는 Argument Resolver이다\n- \n    -  어노테이션으로 선언된 인자의 실제 값을 지정해 준다\n- RequestHeaderMapMethodArgumentResolver\n    -  어노테이션으로 선언된 인자의 실제 값을 지정해 준다 을 사용하게 되면 중복 코드를 줄이고 공통 기능으로 빼서 사용할 수 있는 장점이 있다. 이제 Custom 를 직접 구현해보도록 하자. 2. Custom Argument Resolver 만들기 2.1 Argument Resolver 컨트롤러에 사용예제 컨트롤러 메서드에서  어노테이션이 추가된 인자를 넘겨주면 Client Ip 주소를 얻어 올 수 있는 Resolver를 만들어보자. 2.2 Argument Resolver 생성하기 Argument Resolver 인터페이스에는 2가지 메서드가 존재하고 가 참인 경우에  메서드를 실행한다. | 메서드              | 설명                                                       |\n| ------------------- | ---------------------------------------------------------- |\n|  | 현재 parameter를 resolver가 지원할지 true/false로 반환한다 |\n|    | 실제 바인딩할 객체를 반환한다                              | -  메서드에서는 인자 값에 ClientIp 어노테이션이 포함되어 있는 지 확인한다\n-  메서드에서는 실제 client Ip 주소를 request에서 얻어 오는 로직이 있다 2.3 Argument Resolver 등록하기 이제 앞에서 생성한 Resolver를  메서드에서 추가해주면 된다. 2.4 Controller 실행하기 Unit Test로 확인해보자. 컨트롤러에서 Client Ip 주소를 잘 반환해주고 있다. 3. Argument Resolver 동작 방식 Custom Argument Resolver를 구현해보았다. 이제 스프링안에 내부적으로 어떻게 Argument Resolver가 호출되는지 알아보자. Argument Resolver 동작 구조를 쉽게 보여주는 이미지(Carrey's 기술 블로그에서 참고)이다. Request 처리시 Argument Resolver가 실행되는 순서는 크게 보면 아래와 같다. 실제 실행시 debug해서 확인하면 더 쉽게 이해할 수 있다. 1. Client에서 Request 요청을 보낸다\n2. 요청은 Dispatcher Serlvet에서 처리가 된다\n3. 요청에 대한 HandlerMapping 처리\n    1. (스프링 구동시) 에서 필요한 Argument resolver를 등록한다 (#1.2.1)\n    2. (요청시) 에서 Argument resolver를 실행한다 (#1.2.2)\n        1.  ->  -> \n4. 컨트롤러 메서드 실행 1.2.1 스프링 기본 + Custom Argument Resolver은 어디서 등록이 되나?  객체가 초기화(ex. 스프링 시작시) 될 때 에서  메서드를 호출하여 기본 스프링과 Custom resolver를 등록한다. 1.2.2 supportsParameter는 어디에서 호출되나?  메서드에서  실행해서 true를 반환하면 해당 Argument Resolver를 반환한다.  메서드는 아래 여러 메서드에 의해서 호출된다. -  -> ...생략... ->  -> ...생략... ->  ->  순으로 실행되는 것을 확인할 수 있다. 4. 마무리 는 컨트롤러 메서드에서 인자 값에 대한 처리를 위해 사용된다. 이미 스프링에서 공통기능으로 많이 제공하고 있지만, 사용자용 메서드도 쉽게 작성하여 중복 로직을 많이 줄일 수 있어 용의하게 사용된다. 관련 소스는 github에 올려두어서 참고하시면 됩니다. 5. 참고 - https://webcoding-start.tistory.com/59\n- https://velog.io/@riechu3228/HandlerMethodArgumentResolver\n- http://wonwoo.ml/index.php/post/1092\n- https://sun-22.tistory.com/76\n- https://www.mscharhag.com/spring/json-schema-validation-handlermethodargumentresolver\n- https://jaehun2841.github.io/2018/08/10/2018-08-10-spring-argument-resolver/\n- https://jekalmin.tistory.com/entry/%EC%BB%A4%EC%8A%A4%ED%85%80-ArgumentResolver-%EB%93%B1%EB%A1%9D%ED%95%98%EA%B8%B0\n- https://a1010100z.tistory.com/127",
    "category": "spring",
    "tags": [
      "java",
      "spring",
      "spring boot",
      "ArgumentResolver",
      "자바",
      "스프링",
      "스프링부트",
      "리졸버"
    ],
    "date": "2020-07-20T00:00:00.000Z"
  },
  {
    "id": "spring/multi-was-환경을-위한-cluster-환경의-quartz-job-scheduler-구현",
    "slug": "spring/multi-was-환경을-위한-cluster-환경의-quartz-job-scheduler-구현",
    "title": "Multi WAS 환경을 위한 Cluster 환경의 Quartz Job Scheduler 구현",
    "excerpt": "",
    "content": "1. 들어가며 Quartz에서는 메모리 기반의 스케줄러뿐만이 아니라 DB 기반의 스케줄러도 지원합니다. DB 기반의 스케줄러의 경우에는 스케줄러 정보를 메모리가 아닌 DB에 저장하기 때문에 다중 서버 간의 스케줄링이 가능합니다. Quartz는 master-slave 형태로 서로 간의 통신을 하지 않고 단순히 DB 업데이트 정보를 기반으로 각각의 스케줄 인스턴스가 자기가 실행해야 하는 Job을 실행합니다. Cluster 환경에서 스케줄링이 가능하기 때문에 Non-Cluster 환경에 비해 여러 가지 장점이 기본적으로 제공됩니다. - 고가용성 (High Availability)\n    - 한 서버가 셧다운 되더라도 다른 서버에 의해 Job이 실행되어 다운 타임이 없다\n- 확장성 (Scalability)\n    - Quartz 설정된 서버를 구동하면 자동으로 DB에 스케줄 인스턴스로 등록된다\n    - 셧다운 된 서버는 다른 서버에 의해서 DB에서 삭제된다\n- 로드 밸런싱 (Load balancing)\n    - Cluster 구성으로 여러 Job이 여러 서버에 분산되어 실행된다\n    - 로드 밸런싱 알고리즘에는 Hashing, Round-robin 등등이 존재하지만, Quartz에서는 최소한의 구현으로 random 알고리즘만을 제공한다 2. 개발 환경 포스팅에서 작성한 코드는 아래 github 링크를 참조해주세요. - OS : Mac OS\n- IDE: Intellij\n- Java : JDK 1.8\n- Source code : github\n- Software management tool : Maven 3. 스프링 부트 기반의 Quartz Cluster 스케줄러 구축 3.1 Quartz를 위한 DB 스키마 생성 DB 스키마는 Quartz의 소스코드 에 포함되어 있어서 원하는 DB 스키마를 소스코드에서 찾습니다. 저는 MySql를 사용하겠습니다. DB에 quartz용 database를 생성하고 schema script를 실행합니다. 3.2 Maven 라이브러리 추가 Quartz Cluster 구성을 위해 스프링 부트에서 필요한 라이브러리를 추가합니다. 3.3 Quartz 관련 설정 3.3.1 DataSource 및 Quartz 속성값 설정하기 Spring Boot에서 dataSource 설정은 간단합니다. @EnableAutoConfiguration 어노테이션(@SpringBootApplication 어노테이션에 의해 포함됨)에 의해서 application.properties 내의 spring.datasource.\\ 속성은 정의하면 자동으로 인식이 됩니다. 하지만, JavaConfig로 별도의 DataSource를 구현하여 Bean을 등록했다면 spring.datasource.\\ 속성은 적용되지 않습니다. Quartz 속성에 대한 자세한 내용은 Quartz 공식 페이지 를 참고해주세요. 개인적으로 이해 안 되었던 부분은 간단하게 추가 설명을 달았습니다. - jobStore.useProperties=true\n    - 이 값이 true이면 DB에 JobDataMaps에 저장되는 값이 binary가 아닌 string 값으로 저장된다\n- jobStore.misfireThreshold=60000 (기본 값 : 1분)\n    - Job이 실행되어야 하는데 서버가 셧다운 되었거나 쓰레드가 부족한 경우에 제시간에 실행이 안될 수 있는데 이 경우를 misfire (불발) 되었다고 한다\n    - Trigger가 misfire된 것으로 간주되는 시간으로 1분이 지나면 misFire 되었다고 판단한다 3.3.2 Quartz에 dataSource 설정하기 SchedulerFactoryBean에 dataSource를 지정하면 됩니다. 간단하죠? Spring Boot + Quartz을 이용한 Job Scheduler 구현 (In-memory) 와 비교하면 어떻게 다르게 구성 되었는지 한 눈에 쉽게 파악할 수 있습니다. 파일이나 폴더를 비교하는 프로그램 중에 개인적으로 저는 Meld 라는 오픈소스를 잘 사용합니다. 한번 설치해서 비교해보세요. 블로그보다 코드만 보고도 쉽게 파악할 수 있습니다. 3.3.3 이중화 서버 구동 서버를 이중화로 구동했을 때도 이상이 없이 스케줄러가 잘 돌아가는지 확인해볼까요? 먼저 프로젝트를 복사할게요. 복사하고 나서 서버 포트를 다른 번호로 변경합니다. 서버를 각각 실행하고 나서 QRTZ\\SCHEDULERSTATE 테이블에서 2개의 인스턴스가 등록되었는지 확인합니다. Postman에서 임의로 job을 추가합니다. GET /scheduler/jobs API로 조회해보면 잘 등록된 것을 확인할 수 있습니다. 각 WAS에서 job이 실행되는 것도 터미널에서 볼 수 있습니다. WAS1 (quartz-cluster)를 셧다운 시켜보면 WAS2 (quartz-cluster2)가 job을 픽업해서 이상 없이 실행하는 것을 확인할 수 있습니다. 3.4 Quartz Cluster 설정시 주의사항 - 서버 타임 동기화\n    - Quartz는 내부적인 로직안에서 타임으로 판단하는 부분이 많아서 서버 타임 동기화는 필수이다\n- Quartz 설정 튜닝\n    - Job Workload Type에 따라서 Quartz 설정 튜닝이 필요하다\n    - Long Jobs - 장시간 실행되는 Job (ex. CPU intensive)\n    - Short Jobs - 짧게 실행되는 Job (ex. 1초마다 실행)\n    - 특히 Short Job과 같은 경우에는 1초마다 실행되는 것을 보장해야 한다. 한 서버가 셧다운 되면 실행 중인 Job이 misfire 되고 다른 서버가 바로 이어서 실행해야 하는 조건이 있다면 튜닝은 필수이다\n        - 참고로 현재 운용 중인 서버에서 Short Jobs이 많아 아래와 같이 튜닝을 했다 3.5 Job History 기능 Quartz에서는 현재 실행되는 Job에 대해서만 관리하고 Job History에 대한 내용은 기록하지 않습니다. 나중에 어드민 UI 메인 페이지에 추가로 넣으면 좋을 것 같아서 이번에 작업을 같이했습니다. 이 내용은 어드민 UI 포스팅에서 다루도록 할게요. 4. 정리 Quartz Cluster 구성은 DB에 대한 dataSource 속성과 Cluster 관련 설정만 해주면 어렵지 않게 구성할 수 있습니다. Quartz는 Cluster 환경으로 DB를 사용하고 DB에 접근할 때마다 lock을 걸고 정보를 업데이트합니다. Quartz 서버나 Short Jobs의 수가 많은 경우에는 lock이 더 많이 발생할 수 있어서 실행하려는 Job이 misfire 될 가능성이 커집니다. 이런 경우에는 Redis와 같은 다른 저장소를 사용하면 좋지 않을까 생각합니다. Quartz에서는 기본적으로 2가지 저장소만 (Memory, DB) 제공하지만, Github에 Redis나 MongoDB와 같은 저장소에 저장할 수 있도록 구현체들이 있습니다. DB 실행 시 문제가 된다면 다른 저장소로 저장해보는 것도 좋을 듯해요. 5. 참고 - Quartz Cluster\n    - https://jeroenbellen.com/configuring-a-quartz-scheduler-in-a-clustered-spring-boot-application/\n    - https://github.com/heidiks/spring-boot-quartz-cluster-environment\n    - https://medium.com/@Hronom/spring-boot-quartz-scheduler-in-cluster-mode-457f4535104d\n    - https://flylib.com/books/en/2.65.1/howclusteringworksinquartz.html\n    - https://www.baeldung.com/spring-quartz-schedule\n    - https://kingbbode.tistory.com/38\n    - https://www.callicoder.com/spring-boot-quartz-scheduler-email-scheduling-example/\n- 스프링부트에서 DataSource 설정하는 방법\n    - https://www.holaxprogramming.com/2015/10/16/spring-boot-with-jdbc/\n- JPA page관련 unit test\n    - https://velog.io/@jayjay28/JPA를-이용한-단순-게시물-처리\n- JobStore\n    - https://dzone.com/articles/using-quartz-for-scheduling-with-mongodb\n    - https://github.com/jlinn/quartz-redis-jobstore\n- Misfire\n    - https://dzone.com/articles/quartz-scheduler-misfire\n    - https://stackoverflow.com/questions/32075128/avoiding-misfires-with-quartz\n    - https://www.nurkiewicz.com/2012/04/quartz-scheduler-misfire-instructions.html\n- Short Running Jobs\n    - https://airboxlab.github.io/performance/scalability/scheduler/quartz/2017/06/20/perftuningquartz.html\n    - https://www.ebayinc.com/stories/blogs/tech/performance-tuning-on-quartz-scheduler/",
    "category": "spring",
    "tags": [
      "spring",
      "quartz",
      "java",
      "multi was",
      "job",
      "cluster",
      "자바",
      "스프링",
      "퀄츠",
      "스케줄러",
      "이중화",
      "클러스터",
      "다중서버"
    ],
    "date": "2019-10-13T00:00:00.000Z"
  },
  {
    "id": "spring/qa-spring-boot-annotation-모음",
    "slug": "spring/qa-spring-boot-annotation-모음",
    "title": "Q&A Spring Boot Annotation 모음",
    "excerpt": "",
    "content": "개인적으로 모르는 부분 적어두고 알게 되는 부분에 대해서 간단하게 정리해둔 자료입니다.\n미 답변중에 알고 계신 부분 있으면 코멘트 달아주세요. 감사합니다. Q&A 전체 목록 <span style=\"color:orange\">[답변완료]</span> 쉽게 찾기 위해서 알파벳 순으로 정리합니다. <span style=\"color:brown\">@SpringBootApplication</span> @SpringBootApplication 어노테이션은 @Configuration, @EnableAutoConfiguration, @ComponentScan 어노테이션이 뭉쳐진 어노테이션입니다.  @EnableAutoConfiguration\n     이 어노테이션은 스프링 부트에서 자동 구성(Auto-Configuration)을 활성화 시키는 어노테이션이다.\n     스프링 부트는 클래스패스, 어노테이션, 구성 파일을 보고 가장 적절한 앱에 맞는 기술을 넣어 구성을 해준다. 참고\n http://partnerjun.tistory.com/54 - - - - <span style=\"color:orange\">[미 답변 질문]</span> - @SpringBootTest와 @DataJpaTest의 차이점은 뭔가?\n https://lalwr.blogspot.com/2018/05/spring-boot-springboottest-datajpatest.html",
    "category": "spring",
    "tags": [
      "Q&A",
      "faq",
      "spring",
      "annotation",
      "스프링",
      "어노테이션"
    ],
    "date": "2019-07-03T00:00:00.000Z"
  },
  {
    "id": "spring/qa-spring-boot-관련-질문-모음",
    "slug": "spring/qa-spring-boot-관련-질문-모음",
    "title": "Q&A Spring Boot 관련 질문 모음",
    "excerpt": "",
    "content": "개인적으로 모르는 부분 적어두고 알게 되는 부분에 대해서 간단하게 정리해둔 자료입니다.\n미 답변중에 알고 계신 부분 있으면 코멘트 달아주세요. 감사합니다. Q&A 전체 목록 <span style=\"color:orange\">[답변완료]</span> <span style=\"color:brown\">1. application.properties : server.compression.enabled 속성의 의미는?</span> 스프링 부트에서 기본적으로 GZip 압축은 비활성화 되어 있습니다. 하지만, server.compression.enabled=true로 설정하면 웹 자원(ex. html, css)을 압축해서 클라이언트로 보내져서 응답 시간을 줄일 수 있는 장점이 있습니다. 참고\n https://www.callicoder.com/configuring-spring-boot-application/ <span style=\"color:brown\">2. Quartz에서 @PersistJobDataAfterExecution 어노테이션의 의미는 뭔가? </span> Job 로직에서 JobDataMap 데이터를 수정하면 실행이후에 DB에 저장이 안됩니다. 하지만, @PersistJobDataAfterExecution 어노테이션을 클래스에 추가하면 JobDataMap을 수정한 데이터를 다음 실행 때에도 반영된 데이터를 읽을 수 있습니다.  https://www.concretepage.com/scheduler/quartz/quartz-2-scheduler-pass-parameters-to-job-with-jobdatamap-using-persistjobdataafterexecution-and-disallowconcurrentexecution-example\n http://www.quartz-scheduler.org/documentation/quartz-2.1.7/examples/Example5.html <span style=\"color:brown\"> 3. @SpringBootTest와 @DataJpaTest의 차이점은 뭔가?</span> - @SpringBootTest     - 모든 빈을 읽어서 테스트할 수 있다\n    - 실행이 느리다\n- @DataJpaTest\n    - @Entity, @Repository만 스캔해서 빈으로 설정된다\n    - @Configuration, @Component, @Service를 스캔하지 않는다.\n    - DB는 인메모리 데이터 베이스를 사용하여 테스트를 수행한다\n    - @Transactional이 이미 포함되어 있다 @SpringBootTest @DataJpaTest 참고  https://lalwr.blogspot.com/2018/05/spring-boot-springboottest-datajpatest.html\n https://kok202.tistory.com/116 - - - - <span style=\"color:orange\">[미 답변 질문]</span>",
    "category": "spring",
    "tags": [
      "Q&A",
      "faq",
      "spring",
      "springboot",
      "batch",
      "스프링",
      "배치",
      "질문"
    ],
    "date": "2019-12-04T00:00:00.000Z"
  },
  {
    "id": "spring/qa-spring-jpa-annotation-모음",
    "slug": "spring/qa-spring-jpa-annotation-모음",
    "title": "Q&A Spring JPA Annotation 모음",
    "excerpt": "",
    "content": "개인적으로 모르는 부분 적어두고 알게 되는 부분에 대해서 간단하게 정리해둔 자료입니다.\n미 답변중에 알고 계신 부분 있으면 코멘트 달아주세요. 감사합니다. Q&A 전체 목록 <span style=\"color:orange\">[답변완료]</span> 쉽게 찾기 위해서 알파벳 순으로 정리합니다. <span style=\"color:brown\">@EntityScan</span> 이 어노테이션으로 엔티티 클래스를 스캔할 곳을 지정하는데 사용합니다. 메인 어플리케이션 패키지 내에 엔티티 클래스가 없는 경우 이 어노테이션을 사용해서 패키지 밖에 존재하는 엔티티를 지정할 수 있습니다. 기존적으로 @EnableAutoConfiguration 어노테이션에 의해서 지정한 곳에서 엔티티를 스캔합니다. 참고\n https://dzone.com/articles/spring-boot-entity-scan <span style=\"color:brown\">@UniqueConstraint</span> 이 어노테이션은 JPA 컬럼 2개 이상 unique하게 설정하려고 할때 사용합니다. 참고로 하나의 컬럼에 unique 한 설정을 하려면 아래와 같습니다. 참고\n https://gs.saro.me/dev?page=4&tn=499\n https://stackoverflow.com/questions/3126769/uniqueconstraint-annotation-in-java <span style=\"color:brown\">@CreatedDate, @LastModified</span> 이 어노테이션은 처음 언티티 객체가 저장될 때 생성날짜, 수정날짜를 주입해줍니다. 참고\n https://eclipse4j.tistory.com/201 <span style=\"color:brown\">@BatchSize(size=30)</span> 이 어노테이션은 JPA의 N+1을 해결할 수 있는 방법중에 하나로 연관된 엔티티를 조회할 때 지정된 size 만큼 SQL의 IN 절을 사용해서 사이즈만큰 한번에 가져와서 조회합니다. 참고  https://joont92.github.io/jpa/JPA-%EC%84%B1%EB%8A%A5-%EC%B5%9C%EC%A0%81%ED%99%94/ - - - - <span style=\"color:orange\">[미 답변 질문]</span> - @NaturalId\n하이버네티으에서 Named query 대신에 사용할 수 있는 네임 쿼리란 한번 정의하면 변경할 수 없는 정적 쿼리이다. 참고\n https://howtodoinjava.com/hibernate/hibernate-naturalid-example-tutorial/ - @EntityListeners, @EnableJpaAuditing 참고\n https://www.logicbig.com/tutorials/java-ee-tutorial/jpa/entity-listeners.html\n https://www.logicbig.com/tutorials/java-ee-tutorial/jpa/entity-audit-listener.html",
    "category": "spring",
    "tags": [
      "Q&A",
      "faq",
      "spring",
      "jpa",
      "db",
      "database",
      "스프링",
      "데이터베이스"
    ],
    "date": "2019-07-03T00:00:00.000Z"
  },
  {
    "id": "spring/quartz-job-scheduler란",
    "slug": "spring/quartz-job-scheduler란",
    "title": "Quartz Job Scheduler란?",
    "excerpt": "",
    "content": "1. 들어가며 Quartz Job Scheduler에 대한 내용은 여러 시리즈 형식으로 작성을 하였습니다. Quartz에 대한 간단한 설명을 시작으로 스프링 부트 기반에서의 Quartz Job Scheduler 구현 그리고 이중화 환경에서 많이 사용하는 Quartz Cluster 구성도 아래 튜토리얼에서 같이 보도록 하겠습니다.  1부 : Quartz Job Scheduler란?\n 2부 : Spring Boot + Quartz을 이용한 Job Scheduler 구현 (In memory)\n 3부 : Multi WAS 환경을 위한 Cluster 환경의 Quartz Job Scheduler 구현\n 4부 : Quartz에서 실행중인 Job을 Interrupt하여 Job Scheduler를 정상종료 시키는 방법 1.1 Quartz란? Quartz는 Terracotta 라는 회사에 의해 개발된 Job Scheduling 라이브러리입니다. 완전히 자바로 개발되어 어느 자바 프로그램에서도 쉽게 통합해서 개발할 수 있습니다. Quartz는 수십에서 수천 개의 작업도 실행 가능하며 간단한 interval 형식이나 Cron 표현식으로 복잡한 스케줄링도 지원합니다. 예를 들면 매주 금요일 새벽 1시 30분에 매주 실행하는 작업이나 매월 마지막 날에 실행하는 작업도 지정할 수 있습니다. 1.2 장단점 스프링을 개발하면서 Quartz를 Job Scheduler로 자주 사용하는 이유도 있지만, 단점도 존재합니다. 장점  DB 기반으로 스케줄러 간의 Clustering 기능을 제공한다\n     시스템 Fail-over와 Round-robin 방식의 로드 분산처리를 지원한다\n In-memory Job Scheduler도 제공한다\n 여러 기본 Plug-in을 제공한다\n     ShutdownHookPlugin - JVM 종료 이벤트를 캐치해서 스케줄러에게 종료를 알려준다\n     LoggingJobHistoryPlugin - Job 실행에 대한 로그를 남겨 디버깅할 때 유용하게 사용할 수 있다 단점\n Clustering 기능을 제공하지만, 단순한 random 방식이라서 완벽한 Cluster 간의 로드 분산은 안된다\n 어드민 UI을 제공하지 않는다\n 스케줄링 실행에 대한 History는 보관하지 않는다\n Fixed Delay 타입을 보장하지 않으므로 추가 작업이 필요하다 2. Quartz 아키텍처와 구성요소 Quartz Scheduler에 익숙해지기 위해서 자주 사용하는 용어들을 알아보겠습니다. 2.1 용어 정리  Job\n     Quartz API에서 단 하나의 메서드를 가진 execute(JobExecutionContext context) Job 인터페이스를 제공한다. Quartz를 사용하는 개발자는 수행해야 하는 실제 작업을 이 메서드에서 구현하면 된다.\n     Job의 Trigger가 발생하면 스케줄러는 JobExecutionContext 객체를 넘겨주고 execute 메서드를 호출한다.\n         JobExecutionContext는 Scheduler, Trigger, JobDetail 등을 포함하여 Job 인스턴스에 대한 정보를 제공하는 객체이다  JobDataMap\n     JobDataMap은 Job 인스턴스가 실행할 때 사용할 수 있게 원하는 정보를 담을 수 있는 객체이다.\n     JobDetail을 생성할 때 JobDataMap도 같이 세팅해주면 된다\n      \n     Job 실행시 스케줄러에 Job 추가시 넣었던 JobDataMap 객체를 아래와 같이 접근하여 사용할 수 있다\n        JobDetail\n     Job을 실행시키기 위한 정보를 담고 있는 객체이다. Job의 이름, 그룹, JobDataMap 속성 등을 지정할 수 있다. Trigger가 Job을 수행할 때 이 정보를 기반으로 스케줄링을 한다\n        Trigger\n     Trigger는 Job을 실행시킬 스케줄링 조건 (ex. 반복 횟수, 시작시간) 등을 담고 있고 Scheduler는 이 정보를 기반으로 Job을 수행시킨다.\n     Trigger와 Job의 관계 정리\n         1 Trigger = 1 Job\n             반드시 하나의 Trigger는 반드시 하나의 Job을 지정할 수 있다\n         N Trigger = 1 Job\n             하나의 Job을 여러 시간때로 실행시킬 수 있다 (ex. 매주 토요일날, 매시간마다)\n     Trigger는 2가지 형태로 지정할 수 있다\n         SimpleTrigger\n             특정 시간에 Job을 수행할 때 사용되며 반복 횟수와 실행 간격등을 지정할 수 있다\n              \n         CronTrigger\n             CronTrigger는 cron 표현식으로 Trigger를 정의하는 방식이다 (ex.매일 12시 - '0 0 12   ?’)\n             Cron 표현식은 SimpleTrigger와 같이 단순 반복뿐만이 아니라 더 복잡한 스케줄링(ex. 매월 마지막 금요일에 오후 3시부터 3시까지 몇분마다 실행)도 지정할 수 있다\n             Cron 표현식은 이곳 을 참고해주세요\n                Misfire Instructions\n     Misfire는 Job이 실행되어야 하는 시간, fire time을 지키지 못한 실행 불발을 의미한다\n     이런 Misfire는 Scheduler가 종료될 때나 쓰레드 풀에 사용 가능한 쓰레드가 없는 경우에 발생할 수 있다\n     Scheduler가 Misfire된 Trigger에 대해서 어떻게 처리할 지에 대한 다양한 policy를 지원한다.\n         예를 들면..\n             MISFIREINSTRUCTIONFIRENOW - 바로 실행\n             MISFIREINSTRUCTIONDONOTHING - 아무것도 하지 않음\n Listener\n     Listener는 Scheduler의 이벤트을 받을 수 있도록 Quartz에서 제공하는 인터페이스이며 2가지를 제공한다\n         JobListener\n             Job 실행전후로 이벤트를 받을 수 있다 \t\t TriggerLIstener\n\t\t\t Trigger가 발생하거나 불발이 일어날 때나 Trigger를 완료할 때 이벤트를 받을 수 있다  JobStore\n     Job과 Trigger의 정보를 2가지 방식으로 저장할 수 있다\n         RAMJobStore\n             기본 값으로 메모리에 스케줄 정보를 저장한다\n             메모리에 저장하기 때문에 성능면에서는 제일 좋지만, 시스템 문제 발생시 스케줄 데이터를 유지하지 못하는 단점이 있다\n         JDBC JobStore\n             스케줄 정보를 DB에 저장하다\n             시스템이 셧다운되더라도 스케줄 정보는 유지되어 시스템 재시작시 다시 Job 실행을 할 수 있다\n         다른 기타 JobStore\n             Quartz JobStore을 확장하여 다른 저장소(Redis, MongoDB)에도 저장할 수 있다. 실제 구현은 아래 링크를 참고해주세요\n                 RedisJobStore\n                 MongoDBJobStore 3. Quartz 구성요소 Java Articles 블로그에서 가져온 다이어그램입니다. Quartz의 전체 구조와 흐름을 잘 보여주는 그림입니다. Quartz의 세밀한 설정을 이해하는데 공식 문서도 도움이 되지만, 실제 소스코드 를 보면 Quartz의 동작과 전체 아키텍처 구조를 이해하는데 많은 도움이 됩니다. Quartz 스케줄러가 언제 시작되고 등록된 Job들이 어떤 작업들에 의해서 실행되는지 다음 튜터리얼에서 사용할 코드를 보면서 간단하게 알아보도록 하겠습니다.  질문1 - Spring에서 Quartz Scheduler가 어떻게 시작되는지?\n 질문2 - 추가된 Job은 어떻게 시작되나?\n 질문3 - Job 실행시 호출되는 JobListener와 TriggerListener는 어떻게 호출되나? 스프링 부트 구동시 콘솔에 찍히는 화면입니다. Quartz Scheduler가 초기화되고 시작된 이후에 추가하는 SimpleJob이 실행되는 것을 볼 수 있습니다. 분석한 코드를 나름 정리한다고 했는데, 이해한 코드를 쉽게 글로 정리하기가 쉽지 않네요. 디버깅을 걸어서 따라가면서 정리된 내용을 보시면 좋을 듯합니다.  SchedulerFactoryBean\n     Quartz 스케줄러는 스프링의 컨테이너의 빈 LifeCycle 관리에 의해서 scheduler관련 설정이 초기화, 시작, 종료가 된다\n      \n     1.1 ==> void afterPropertiesSet() : InitializingBean 인터페이스의 의해서 호출됨\n         prepareSchedulerFactory() 메서드는 SchedulerFactory 인스턴스인 StdSchedulerFactory를 반환해서 prepareScheduler() 인자로 넘겨주고 Scheduler instance를 초기화한다\n          \n         1.2 ==> Scheduler prepareScheduler(SchedulerFactory schedulerFactory) :\n             SchedulerFactory로 부터 Scheduler 인스턴스를 얻어온다\n              \n             1.3 ==> Scheduler createScheduler(SchedulerFactory schedulerFactory, @Nullable String schedulerName)\n                 StdSchedulerFactory.getScheduler()에서 여러 초기화 단계를 실행한 후 Scheduler 인스턴스를 반환한다\n                     관련 Quartz 클래스를 로드한다 (ex. JobFactory, SimpleThreadPool, JobStore)\n                     Da",
    "category": "spring",
    "tags": [
      "quartz",
      "spring",
      "springboot",
      "job",
      "scheduler",
      "스케줄러",
      "스케줄",
      "스프링",
      "스프링부트"
    ],
    "date": "2019-09-07T00:00:00.000Z"
  },
  {
    "id": "spring/quartz에서-실행중인-job을-interrupt하여-job-scheduler를-정상종료-시키는-방법",
    "slug": "spring/quartz에서-실행중인-job을-interrupt하여-job-scheduler를-정상종료-시키는-방법",
    "title": "Quartz에서 실행중인 Job을 Interrupt하여 Job Scheduler를 정상종료 시키는 방법",
    "excerpt": "",
    "content": "1. 들어가며 본 포스팅은 Quartz 튜터리얼에서 4번째 시리즈로 Quartz 서버를 셧다운 시킬 때 gradefully하게 처리하는 방법에 대해서 다룹니다. 셧다운 이벤트가 발생하면 실행 중인 Quartz Job에 내부 interrupt() 함수가 호출이 되고 interrupt로 노티를 받으면 개발자가 알아서 close 로직을 짜면 됩니다. 실행 쓰레드를 kill 할 수도 있고 (비추천) 실행 중인 Job을 기다리고 다음 스케줄에서 제외시킬 수도 있습니다. 2. 개발 환경  OS : Mac OS\n IDE: Intellij\n Java : JDK 1.8\n Source code : github\n Software management tool : Maven 3. Quartz에서 ShutdownHook 등록하고 기존 Job을 Interruptable Job으로 구현하기 실행 중인 Job을 gracefully하게 셧다운 하려면 2가지만 설정해주면 됩니다. 3.1 Quartz 설정에 SchedulerFactoryBean에 대한 ShutdownHook 등록하기 Quartz에서 사용하는 SchedulerFactoryBean은 SmartLifeCycle 인터페이스를 구현하고 있습니다. 스프링의 SmartLifeCycle은 콜백 인터페이스로 여러 LifeCycle에 대한 메서드를 가지고 있고 어플리케이션이 종료되거나 시작될 때 정의된 메서드가 호출됩니다. QuartzConfiguration 파일에 gracefulShutdownHookForQuartz 메서드를 빈으로 정의하여 Shutdown Hook을 등록합니다. Gracefully 하게 셧다운 해야 하기 때문에 저희가 관심 있는 메서드는 stop() 메서드입니다. 이 메서드가 호출되면 Quartz 스케줄러에서 현재 실행 중인 모든 Job을 조회하여 실행 중인 Job의 interrupt() 메서드를 호출합니다. Job에서는 어떻게 처리할 수 있는지 다음 장에서 설명할게요. 3.2 Quartz Job에 InterruptableJob 인터페이스를 implements하여 구현하기 Interrupt 가능한 Job을 구현하려면 InterrutableJob 인터페이스를 구현하고 interrupt() 메서드를 구현해주면 됩니다. 이미 짐작 하셨겠지만, 셧다운시 3.1에서 정의한 SmartLifeCycle의 stop() 메서드에 의해 호출이 되고 현재 실행 중인 Job의 쓰레드를 interrupt 시킵니다. 4. 정리 실행 중인 Job을 Gracefully 하게 셧다운 시키는 방법에 대해서 알아보았습니다. 다음 포스팅은 Quartz 튜터리얼 시리지로의 마지막으로 Quartz 어드민 UI 구현에 대해서 알아보겠습니다. 5. 참고  Servlet 시작시\n     https://karismamun.tistory.com/46\n Spring SmartLifeCyle\n     https://jjhwqqq.tistory.com/155\n     https://krksap.tistory.com/1240\n Interrupt\n     https://m.blog.naver.com/PostView.nhn?blogId=qbxlvnf11&logNo=221106055566&proxyReferer=https%3A%2F%2Fwww.google.com%2F\n     https://blog.naver.com/qbxlvnf11/220921178603?proxyReferer=http%3A%2F%2Fblog.naver.com%2FPostView.nhn%3FblogId%3Dqbxlvnf11%26logNo%3D220945432938%26parentCategoryNo%3D%26categoryNo%3D12%26viewDate%3D%26isShowPopularPosts%3Dtrue%26from%3Dsearch\n     http://blog.naver.com/PostView.nhn?blogId=qbxlvnf11&logNo=220945432938&parentCategoryNo=&categoryNo=12&viewDate=&isShowPopularPosts=true&from=search",
    "category": "spring",
    "tags": [
      "quartz",
      "spring",
      "job",
      "interrupt",
      "hook",
      "인터럽트",
      "셧다운훅",
      "스케줄러",
      "스케줄",
      "스프링",
      "스프링부트"
    ],
    "date": "2019-10-12T00:00:00.000Z"
  },
  {
    "id": "spring/simple-spring-memcached-ssm란",
    "slug": "spring/simple-spring-memcached-ssm란",
    "title": "Simple Spring Memcached(SSM)란",
    "excerpt": "",
    "content": "1. 들어가며 In-memory DB로 Memcached를 사용하면 자바에서는 simple-spring-memcached (SSM) 라이브러리를 자주 사용됩니다. SSM 어노테이션으로 메서드에 선언하면 쉽게 관련 데이터가 캐시에서 관리됩니다. 스프링에서도 버전 3.1부터는 캐시 서비스 추상화 기능이 지원되어 비즈니스 로직 변경 없이 쉽게 다양한 캐시 구현체(ex. Ehcache, Redis)로 교체가 가능하게 되었습니다. 스프링에서 제공하는 캐시 기능은 다른 포스팅에서 더 자세히 다루도록 하겠습니다. 2. 개발 환경 - OS : Mac OS\n- IDE: Intellij\n- Java : JDK 11\n- Source code : github\n- Software management tool : Maven 3. Simple Spring Memcached(SSM) 설정 및 사용법 SSM 사용에 필요한 dependency와 스프링 빈 설정 파일을 추가해야 합니다. 3.1 Maven dependency 추가 라이브러리는 maven을 사용하였습니다. Simple-spring-memcached와 Memcache provider 중에 하나를 선택해서 pom.xml에 추가해줍니다. spymemcached와 xmemcached의 차이는 아래와 같습니다. 이 포스팅에서는 xmemcached로 위주로 설명합니다. - spymemcached \\ 단순 비동기, 단일 쓰레드 memcached 자바 라이브러리\n- xmemcached (이 라이브러리 위주로 설명함) \\ 고성능 멀티 쓰레드 memcached 자바 라이브러리 xmemcached spymemcached 3.2 스프링 설정 파일 스프링 빈 설정에 Memcached 관련 설정이 포함됩니다. Memcached의 서버 정보와 캐시 설정은 ConsistentHashing 방식으로 지정되어 있습니다. > 추가 설명\n> ConsistentHashing 방식은 여러 서버가 변경되더라도 각 서버에 할당된 데이터를 재분배하지 않고 다운된 서버의 데이터만 다른 서버로 재분배하는 방식입니다. K(keys) = 10,000, N(memcache 서버 수 : 슬롯) = 5대 => 각 서버마다 2000 keys만큼을 hashing 할 수 있다. 서버 장애 (1 서버 다운 A)가 발생한 A 서버의 key만 다른 서버로 재분배하는 방식이다.\n> 더 자세한 설명은 Consistent Hashing 를 참고해주세요. simplesm-context.xml은 SSM github 소스에 포함된 설정파일이고 SSM 사용 시 필요하므로 복사해서 사용하시면 됩니다. 3.3 SSM Cache 대표 어노테이션 아래는 대표적으로 많이 사용하는 어노테이션입이다. 이외에 사용해야 하는 어노테이션은 해당 라이브러리의 Wiki 를 참고해주세요. - Read\n    - @ReadThroughAssignCache\n    - @ReadThroughSingleCache\n    - @ReadThroughMultiCache\n- Update\n    - @UpdateAssignCache\n    - @UpdateSingleCache\n    - @UpdateMultiCache\n- Invalidate\n    - @InvalidateAssignCache\n    - @InvalidateSingleCache\n    - @InvalidateMultiCache\n- Counter\n    - @ReadCounterFromCache\n    - @IncrementCounterInCache\n    - @DecrementCounterInCache 대부분의 SSM 어노테이션은 Cache Action과 Cache Type으로 구분할 수 있습니다. | Cache Action | Description                                 |\n| ---------------- | ----------------------------------------------- |\n| ReadThrough      | Cache에 저장된 key가 없는 경우 Cache에 저장한다 |\n| Update           | Cache에 저장된 key의 값을 업데이트한다          |\n| Invalidate       | Cache에 저장된 key를 삭제한다                   | | Cache Type | Description                                              |\n| -------------- | ------------------------------------------------------------ |\n| AssignCache    | 캐시 키는 assignedKey 속성으로 지정한 값이고 메서드 인자가 없는 경우에 사용된다<br/>ex. List<Person> getAllUsers()와 같은 메서드에 사용된다 |\n| SingleCache    | 캐시 키는 SSM 어노테이션으로 선언된 메서드 인자로 생성되며 인자가 하나인 경우에 사용된다.<br/>ㅁ. 인자가 List 타입인 경우에는 캐시 키로 생성해서 사용할 수 없다<br/>ex. Person getUser(int int) |\n| MultiCache     | 캐시 키는 SSM 어노테이션으로 선언된 메서드 여러 인자로 생성된다. 인자중에 한개가 List 타입 형이여야하며 반환결과도 List 타입이여야 한다. 반환된 결과 List의 각 요소는 지정된 캐시 키로 저장된다.<br/>ex. List<Person> getUserFromData(List workInfo) | SingleCache와 MultiCache인 경우 반드시 메서드 인자 중에 @ParameterValueKeyProvider 어노테이션을 지정해야 합니다. 기본 캐시 어노테이션 외에도 여러 어노테이션과 같이 사용하는 속성들이 존재하며 예제를 통해서 더 자세히 알아보도록 하겠습니다. - 기타 어노테이션 및 속성\n    - @CacheName(“QuoteApp”) : 관련 캐시를 하나로 묶을 수 있는 개념이고 클래스외에도 메서드에도 선언할 수 있다\n    - @CacheKeyMethod : 캐시의 key 값으로 이용할 메서드를 선언한다\n    - 캐시 key는 @CacheKeyMethod로 선언된 메서드가\n    - @CacheKeyMethod가 지정되지 않은 경우에는 Object.toString() 메서드가 사용된다\n    - @ParameterValueKeyProvider : 메서드 인자에 적용되며 @CacheKeyMethod로 선언된 메서드나 toString()을 이용해서 key 값을 구한다\n    - @ParameterDataUpdateContent : 메서드 인자에 어노테이션이 적용되며 새로 저장할 값을 지정한다\n    - @ReturnDataUpdateContent : 반환 값을 캐시에 저장한다\n- 속성\n    - namespace : 동일한 키 값의 이름이 있을 경우를 방지하기 위해서 사용된다\n    - expiration : key값이 만료되는 시간 (초 단위)이다\n    - assignedKey : 캐시 저장시 사용되는 키 값이다 3.3.1 Read Cache @ReadThroughAssignCache 예제 @ReadThroughAssignCache 어노테이션은 인자가 없는 메서드에 적용할 수 있습니다. 캐시 영역에서 네임스페이스 ‘area’ 이름으로 key : value (all : List<Product)가 저장이 됩니다. @ReadThroughAssignCache(namespace = \"area\", assignedKey=\"all\") 각 캐시 어노테이션마다 차이점을 쉽게 알기 위해서 유닛 테스트로 작성을 했습니다. 각 테스트마다 원하는 결과를 얻기 위해서 매 테스트마다 memcache를 flush 하도록 작성되어 있습니다. @ReadThroughAssignCache 어노테이션은 캐시에서 읽어 드릴때 없으면 저장하기 때문에 아래 코드에서는 Comment #1 때에 캐시에 저장하고 Comment #2 때는 캐시에서 데이터를 가져오는 것을 확인할 수 있습니다. Unit Test 실행 결과 watch 모니터링 결과 캐시에 저장하고 값을 확인하는 과정을 실시간으로 확인하려면 telnet으로 로그인하여 watch 명령어를 실행하면 됩니다. 더 자세한 사항은 #3.4 Memcached 유용한 명령어 모음을 참고해주세요. @ReadThroughSingleCache @ReadThroughSingleCache 어노테이션은 인자가 하나인 경우에만 사용합니다. 인자에 @ParameterValueKeyProvider 어노테이션을 선언하면 @CacheKeyMethod로 지정된 메서드는 캐시 키를 생성하는 데 사용되고 없는 경우에는 toString() 메서드가 사용됩니다. 이 어노테이션도 읽어드릴 때 캐시에 없으면 캐시에 저장하기 때문에 코드상에서 Comment #1일 때 저장하고 Comment #2일 때 캐시에서 값을 얻어오는 것을 watch 명령어로 통해서 확인할 수 있습니다. Unit Test 실행 결과 watch 모니터링 결과 @ReadThroughMultiCache MultiCache는 메서드 인자 중에 List 타입인 자가 있어야 합니다. 그리고 캐시 되는 key : value 값은 인자의 List 요소와 반환 결과 요소가 각각 key : value로 캐시에 저장됩니다.\nstats cachedump 명령어로 key 값을 확인해보면 알 수 있습니다. cachedump 결과 포맷에 대한 설명은 #3.4 Memcached 유용한 명령어 모음 을 참고해주세요. getIncrementValue로 넘겨진 nums 리스트의 각 요소가 캐시에 저장이 안 되어 있어서 각 요소를 키로 저장하는 것을 watch 명령어로 확인할 수 있습니다. 두 번째로 getIncrementValue을 호출 할 때는 결과를 캐시에서 바로 가져옵니다. 눈치채신 분도 계시겠지만, 변환된 결과는 잘못되었습니다. 인자로 넘겨준 값이 1이기 때문에 [3, 4, 5, 6]이 반환되어야 하는데, 캐시에서 가져와서 [6, 7, 8, 9]이 반환되었습니다. 캐시 데이터와 실제 데이터 간의 동기화가 중요하면 개발할때는 이런 부분",
    "category": "spring",
    "tags": [
      "ssm",
      "cache",
      "caching",
      "java",
      "spring",
      "memcached",
      "캐쉬",
      "자바",
      "멤캐시",
      "스프링"
    ],
    "date": "2019-01-01T00:00:00.000Z"
  },
  {
    "id": "spring/spring-boot-quartz을-이용한-job-scheduler-구현-In-memory",
    "slug": "spring/spring-boot-quartz을-이용한-job-scheduler-구현-In-memory",
    "title": "Spring Boot + Quartz을 이용한 Job Scheduler 구현 (In-memory)",
    "excerpt": "",
    "content": "1. 들어가며 이 포스팅은 Quartz 튜터리얼 시리즈에 한 부분으로 첫 번째의 포스팅 Quartz Job Scheduler란? 에 이어 2부 내용으로 Spring Boot 기반의 RAMJobStore을 이용한 Quartz 스케줄러 구현을 다룹니다. 기본 개념은 이미 1부에서 다루었기 때문에 여기에서는 작성한 코드 기반으로 어떻게 스프링에서 Quartz를 설정하여 사용할 수 있는지에 대해서 알아보겠습니다. 2. 개발 환경 스프링 부트에서는 Quartz을 사용하려면 spring-boot-starter-quartz 라이브러리를 추가해줘야 합니다. pom.xml 메이븐 파일에 아래 내용을 추가합니다.  OS : Mac OS\n IDE: Intellij\n Java : JDK 1.8\n Source code : github\n Software management tool : Maven 3. 스프링 부트 기반의 Quartz 스케줄러 구축 3.1 Quartz를 위한 관련 설정 3.1.1 스프링 JavaConfig 스프링의 SchedulerFactoryBean은 Bean으로 선언하여 다른 클래스에서 DI (dependency injection)해서 사용할 수 있습니다. 그리고 첫번째 포스팅 에서 언급했던 것처럼 SchedulerFactoryBean 은 ApplicationContext에서 LifeCycle 형식으로 Scheduler을 관리하고 있습니다. Listener와 Quartz 관련된 설정도 여기서 지정합니다. 3.1.2 Quartz 관련 설정 스프링 부트에서는 Quartz 관련 설정을 application.properties에서 합니다. 관련 설정이 없으면 기본 값으로 구동됩니다. ThreadPool, Scheduler Setting, JobStore 등에 관련된 많은 설정이 존재하기 때문에 Quartz Configuration 문서를 참고해주세요. 예제에서는 threadCount를 5개만 생성하고 스케줄러 쓰레드 이름의 prefix를 QuartzScheduler로 지정하였습니다. 3.2 Scheduler Controller과 ScheduleService 구현 샘플 프로젝트에서는 사용자가 정의한 Job을 쉽게 등록하고 삭제, 조회 등을 할 수 있도록 아래와 같은 API를 제공합니다.  제공할 Scheduler API\n     Job 추가 : POST schedulerjob\n     모든 등록된 Job 조회 : GET schedulerjobs\n     Job 삭제 : DELETE schedulerjob\n     Job 멈춤 : PUT schedulerjob/pause\n     Job 재시작 : PUT schedulerjob/resume 컨트롤러와 서비스단의 로직을 보면 기본 로직은 간단하기 때문에 몇개의 API만 설명하겠습니다. 실행할 Job을 먼저 보도록 하겠습니다. 3.2.1 사용자 Job 구현 Job 작업 내용은 지정한 sleep 타임에 따라서 화면에 숫자를 출력하는 것입니다. 3.2.1.1 SimpleJob loop을 돌면서 화면에 숫자를 출력하고 지정한 sleep 타임동안 쉬고 다시 반복하는 로직입니다. 3.2.1.2 CronJob CronJob 구현도 SimpleJob과 동일하고 추가로 jobId를 JobDataMap으로 받아서 화면에 출력하고 있습니다. 3.2.2 Job 추가 API 3.2.2.1 Controller Job 추가 Quartz 스케줄러에서는 SimpleJob과 CronJob 형식으로 추가할 수 있어서 cron 표현식이 있는 경우에는 CronJob으로 등록하도록 조건문을 추가했습니다. 3.2.2.2 ScheduleService Job 추가 사용자가 제공한 Job 이름, 그룹, Cron 표현 등으로 Trigger와 JobDetail을 생성하고 schedulerJob() 메서드로 job을 Quartz에 등록할 수 있습니다. 서비스단의 로직도 Unit Test로 쉽게 체크할 수 있습니다. Quartz 소스 코드를 참조해서 작성했습니다. 3.2.3 등록된 모든 Job 조회 API Scheduler에서 현재 등록된 Job 정보도 scheduler에서 제공하는 여러 메서드을 통해서 쉽게 얻어 올 수 있습니다. 개별 Job 정보외에도 간단한 통계 수치도 같이 count해서 응답 값으로 내려주고 있습니다. 아래와 같이 응답 값을 내려주고 있습니다. 3.2.4 Listeners 3.2.4.1 TriggerListener 메서드 이름으로 쉽게 알 수 있듯이 이벤트(ex. triggerFire, triggerMisfired) 발생시 호출되는 메서드들입니다. vetoJobExecution 메서드는 해당 Trigger를 veto(거부, 금지) 시킬지 결정할 수 있는 메서드로 true이면 veto를 시켜서 Job이 실행되지 않고 false이면 veto를 시키지 않아 Job을 실행시킬 수 있어서 특정 조건을 넣어서 실행 여부를 결정 짓을 수 있는 메서드입니다. 3.2.4.2 JobListener JobListener도 메서드 이름만으로 발생 이벤트시 호출 되는 메서드를 쉽게 알 수 있습니다. jobExecutionVetoed는 TriggersListener.vetoJobExecution() 메서드에서 veto를 시킨 경우 호출됩니다. 4. 정리 Quartz에서는 Scheduler의 여러 기능을 (scheduler, unschedule, pause, resume, stop) 제공하고 있어서 애플리케이션 내에 스케줄링 기능을 잘 구현할 수 있습니다. 이 포스팅에서는 스프링에서 Quartz를 어떻게 설정해서 사용할 수 있는지 알아보았습니다. RAMJobStore를 기본으로 사용해서 스케줄링 정보가 메모리에 저장되기 때문에 다중 서버 환경에서는 적합하지 않습니다. 서버 이중화를 Quartz를 어떻게 설정해야 하는지 다음 포스팅에서 알아보겠습니다. 5. 참고  Quartz 공식 사이트\n     http://www.quartz-scheduler.org\n Spring Boot Quartz Scheduler\n     https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-quartz.html\n     https://www.baeldung.com/spring-quartz-schedule\n     https://gs.saro.me/dev?tn=549",
    "category": "spring",
    "tags": [
      "quartz",
      "spring",
      "job",
      "scheduler",
      "memory",
      "메모리기반",
      "스케줄러",
      "스케줄",
      "스프링",
      "스프링부트",
      "분산환경",
      "다중서버",
      "multiple server",
      "distributed environment"
    ],
    "date": "2019-09-09T00:00:00.000Z"
  },
  {
    "id": "spring/스프링-resttemplate",
    "slug": "spring/스프링-resttemplate",
    "title": "스프링 RestTemplate",
    "excerpt": "",
    "content": "1. 들어가며 스프링 프레임워크에서는 REST 서비스의 Endpoint를 호출할 수 있도록 크게 2가지 방식인 동기, 비동기 REST Client을 제 공합니다. 이번 포스팅에서는 동기방식인 RestTemplate에 대해서 알아보겠습니다. - RestTemplate\n    - Spring 3부터 지원 되었고 REST API 호출이후 응답을 받을 때까지 기다리는 동기방식이다\n- AsyncRestTemplate\n    - Spring 4에 추가된 비동기 RestTemplate이다\n    - Spring 5.0에서는 deprecated 되었다\n- WebClient\n    - Spring 5에 추가된 논블럭, 리엑티브 웹 클리이언트로 동기, 비동기 방식을 지원한다 RestTemplate은 스프링에서 제공하는 다른 여러 Template 클래스 (ex. JdbcTemplate, RedisTemplate)와 동일한 원칙에 따라 설계되어 단순한 방식의 호출로 복잡한 작업을 쉽게 하도록 제공 합니다. RestTemplate 클래스는 REST 서비스를 호출하도록 설계되어 HTTP 프로토콜의 메서드 (ex. GET, POST, DELETE, PUT)에 맞게 여러 메서드를 제공합니다. | 메서드 | HTTP | 설명 |\n| ------------- | ------------- | ------------- |\n| getForObject  | GET | 주어진 URL 주소로 HTTP GET 메서드로 객체로 결과를 반환받는다 | \n| getForEntity | GET | 주어진 URL 주소로 HTTP GET 메서드로 결과는 ResponseEntity로 반환받는다 | \n| postForLocation | POST | POST 요청을 보내고 결과로 헤더에 저장된 URI를 결과로 반환받는다 | \n| postForObject | POST | POST 요청을 보내고 객체로 결과를 반환받는다 | \n| postForEntity | POST | POST 요청을 보내고 결과로 ResponseEntity로 반환받는다 | \n| delete | DELETE | 주어진 URL 주소로 HTTP DELETE 메서드를 실행한다 |\n| headForHeaders | HEADER | 헤더의 모든 정보를 얻을 수 있으면 HTTP HEAD 메서드를 사용한다 |\n| put | PUT | 주어진 URL 주소로 HTTP PUT 메서드를 실행한다 |\n| patchForObject | PATCH | 주어진 URL 주소로 HTTP PATCH 메서드를 실행한다 |\n| optionsForAllow | OPTIONS | 주어진 URL 주소에서 지원하는 HTTP 메서드를 조회한다 |\n| exchange | any | HTTP 헤더를 새로 만들 수 있고 어떤 HTTP 메서드도 사용가능하다 |\n| execute | any | Request/Response 콜백을 수정할 수 있다 | 2. 개발 환경 - OS : Mac OS\n- IDE: Intellij\n- Java : JDK 1.8\n- Source code : github\n- Software management tool : Maven 예제 프로젝트는 스프링 부트로 작성되어 기본 스프링 부트 의존성을 추가하면 RestTemplate 관련 의존성은 자동으로 추가됩니다. 기본 스프링을 사용 중이라면 spring-webmvc 의존성만 추가하여 작업하시면 됩니다. 실제로 RestTemplate은 spring-web 의존성에 포함된 클래스이지만, spring-webmvc 의존성에 spring-web이 포함되어 있어 같이 의존성이 포함 됩니다. 3. RestTemplate의 동작원리 RestTemplate의 동작원리에 대한 내용은 빨간색코딩 블로그에 잘 정리가 되어 있어 별도로 정리는 하지 않았습니다. 해당 링크에 들어가서 더 자세한 설명을 참조해주세요. 4. RestTemplate 메서드 예제 RestTemplate에서 자주 사용하는 메서드 위주로 알아보겠습니다. 4.1 GET 메서드 4.1.1 getForObject() getForObject() 메서드는 GET을 수행하고 HTTP 응답을 객체 타입으로 변환해서 반환해주는 메서드입니다. 이 예제에서는 Employee 객체로 반환해줍니다. Client Code - Unit Test Controller에서는 getEmployee() 호출 시 응답으로 Employee 객체를 JSON 형태로 변환합니다. 예제를 실행해 보면 추가 설정 없이 자동으로 JSON 형태로 잘 받는 것을 알 수 있습니다. 스프링 부트에서는 Controller 단에 @RestController 어노테이션을 추가하면 클래스패스에 Jackson2 (jackson-databind)가 있는 한 기본적으로 JSON 응답을 처리 합니다. spring-boot-starter-web 의존성을 추가했다면 jackson-databind은 transitive 의존성에 의해서 같이 포함됩니다. Controller Code 실행 화면 4.1.2 getForEntity() getForEntity() 메서드의 경우에는 응답을 ResponseEntity 객체로 받게 됩니다. getForObject()와 달리 HTTP 응답에 대한 추가 정보를 담고 있어서 GET 요청에 대한 응답 코드, 실제 데이터를 확인할 수 있습니다. 또한 ResponseEntity<T> 제네릭 타입에 따라서 응답을 String이나 Object 객체로 받을 수 있습니다. 예제에서는 응답 값을 JSON 스트링 형태로 받고 있습니다. Client Code - Unit Test 실행 화면 4.1.3 getForEntity()에 여러 값을 담은 params을 같이 넘겨주기 getForEntity() 메서드는 또한 여러 값을 담은 params를 인자로 받아서 GET 요청을 할 수 있습니다. 예제에서는 URL PATH에 필요한 name과 country 변수를 LinkedMultiValueMap 객체에 담아서 params로 넘겨주었습니다. Client Code - Unit Test 실행 화면 4.2 POST 다음은 POST 메서드에 대한 메서드들입니다. 4.2.1 postForObject() 해더 포함하지 않고 보내기 postForObject()는 getForObject()와 같이 POST 요청에 대해서 반환 값을 해당 객체로 반환해주는 메서드입니다. Employee 객체를 POST의 body로 해서 보냅니다. Client Code - Unit Test Controller에서는 일반적으로 Employee 객체를 POST 메세드로 받아서 서비스 로직을 실행 하겠지만, 여기에서는 간단하게 로그로 출력을 했습니다. 응답 값으로는 받은 객체와 응답 코드 HTTP 201를 ResponseEntity로 반환합니다. Controller Code 실행 화면 4.2.2 postForObject() 해더 포함해서 보내기 이번에는 헤더에 데이터를 실어서 보내 보겠습니다. Employee 객체와 custom 헤더를 인자로 넘겨 HttpEntity를 생성합니다. 생성한 HttpEntity를 postForObject에 담아서 보내면 Controller에서 @RequestHeader 어노테이션으로 값을 얻어올 수 있습니다. Client Code - Unit Test 실행 화면 4.2.3 postForEntity() postForEntity() 메서드는 ResponseEntity<T> 객체로 데이터를 받을 수 있습니다. 예제에서는 Employee 객체로 받는 대신 String으로 받는 예제여서 응답 body가 스트링 JSON 형태로 출력됩니다. Client Code - Unit Test 실행 화면 4.2.4 postFoLocation() postForLocation() 메서드는 객체를 반환하는 대신 생성된 리소스의 URI 위치를 반환합니다. Client Code - Unit Test Controller에서는 헤더에 URI location 값을 저장하여 ResponseEntity로 반환합니다. Controller Code 실행 화면 4.3 DELETE delete() 메서드는 HTTP DELETE을 수행하며 회원의 이름을 넘겨주면 매핑된 Controller의 deleteEmployeeByName 메서드가 실행됩니다. Client Code - Unit Test Controller Code 실행 화면 4.4 PUT put() 메서드도 다른 메서드(ex. postForObject)과 비슷합니다. PUT은 데이터를 업데이트하기 위해 요청을 보내기 때문에 body에 데이터를 실어서 보냅니다. 예제에서는 Address 객체를 실어서 보냅니다. Client Code - Unit Test Controller Code 실행 화면 4.5 Exchange()\n4.5.1 Exchange()로 HTTP GET 메서드 실행하기 지금까지 HTTP 메서드 별로 RestTemplate에서 제공하는 API들을 같이 봤습니다. 별도의 메서드들도 제공하지만, 일반적으로 모두 처리가 가능한 하나의 메서드인 Exchange API도 제공합니다.\n앞에서 사용했던 getForObject() 메서드 대신에 exchange() 메서드를 사용해서 값을 조회해보겠습니다. 헤더에 값을 포함해서 호출하는 예제입니다. Client Code - Unit Test HttpEntity 객체에 “Hello World” 스트링값과 헤더 설정값을 저장합니다. exchange() 메서드에 HttpMethod.GET 메서드를 지정하여 헤더 값도 같이 해당 URL로 보내고 결과를 화면에 출력합니다. Controller Code 실행 화면 4.5.2 exchange()로 객체 컬렉션을 받아보기 단순히 하나의 객체(ex. 직원) 정보를 반환하는 메서드를 Controller에서 정의하지만, 전체 직원을 조회해서 List<Employee> 형태로 반환하는 EndPoint가 필요할 때가 있습니다. Controller Code 모든 직원을 조회해서 반환하는 메서드입니다. 2명의 직원을 리스트에 넣어 List<Employee>를 반환합니다. Client Code - Unit Test RestTemplate에서 리스트 형태의 객체 목록을 얻으려면 ResponseEntity와 Parameteri",
    "category": "spring",
    "tags": [
      "spring",
      "rest",
      "template",
      "REST",
      "java",
      "스프링",
      "스프링부트",
      "자바",
      "restTemplate"
    ],
    "date": "2019-05-06T00:00:00.000Z"
  },
  {
    "id": "spring/스프링-파일-업로드-처리",
    "slug": "spring/스프링-파일-업로드-처리",
    "title": "스프링 파일 업로드 처리",
    "excerpt": "",
    "content": "1. 들어가며 이번 포스팅에서는 스프링에서 파일 업로드를 어떻게 구현할 수 있는지에 대해서 알아보도록 하겠습니다. 스프링에서는 단일 파일 업로드뿐만이 아니라 아래와 같은 여러 방법으로 파일 업로드 기능을 제공합니다. - 단일 파일 업로드\n- 다중 파일 업로드\n- 파일 업로드 + 추가 정보 by @RequestParam 개별로\n- 파일 업로드 + 추가 정보 by @ModelAttribute 한번에 클래스와 매핑 스프링은 MultipartResolver 인터페이스와 아래 2가지 구현체로 파일 업로드를 지원합니다. - Servlet 3.0 Multipart Request 사용\n    - 구현체 : StandardServletMultipartResolver\n    - 파일(XML, JavaConfig)로 설정만 하면 된다.\n- Apache Commons FileUpload API 사용\n    - 구현체 : CommonsMultipartResolver\n    - Servlet 3 환경에만 국한된 것은 아니지만, Servlet 3.x 컨테이너에서도 똑같이 작동한다 \\ pom.xml에 라이브러리를 추가해야 한다 2. 개발 환경 - OS : Mac OS\n- IDE: Intellij\n- Java : JDK 1.8\n    - JDK 9 이상 사용하면 javaxxmlbind/JAXBException 못찾는 이슈가 있다.\n    - 해결 : https://github.com/ohnosequences/sbt-s3-resolver/issues/58\n- Source code : github\n- Software management tool : Maven 3. 파일 업로드을 위한 설정 파일 업로드를 위해 스프링에서 필요한 기본 설정에 대해서 알아봅시다. 언급했던 것처럼 스프링에서는 2가지 방법으로 파일 업로드를 설정할 수 있습니다. 본 포스팅에서는 첫 번째 StandardServletMultipartResolver 리졸뷰 위주로 설명을 하도록 하겠습니다. 두 번째인 CommonsMultipartResolver 리졸뷰를 사용해도 됩니다. 스프링 설정하는 경험이 있으면 어느 것을 사용하던 큰 어려움이 없을 것으로 생각됩니다. 필요한 내용은 참고 링크들을 참조해주시면 될 것 같아요. - Servlet 3.0 Multipart Request (이 방법 위주로 설명)\n    - standardServletMultipartResolver\n- Apache Commons FileUpload API\n    - CommonsMultipartResolver 3.1 스프링 및 파일 업로드 제한 설정 3.1.1 StandardServletMultipartResolver 사용시 설정 Bean XML 정의 파일 설정\n1. StandardServletMultipartResolver 빈을 스프링에 등록 2. 서블릿 설정에서 multipart-config을 선언 Multipart-config에서 여러 옵션을 설정하여 파일 업로드를 제한할 수 있습니다. - location - 파일 업로드 시 임시로 저장하는 절대 경로이다\n    - 디폴트 값 : \"\n- maxFileSize - 파일당 최대 파일 크기이다\n    - 디폴트 값 : 제한없음\n- maxRequestSize - 파일 한 개의 용량이 아니라 multipart/form-data 요청당 최대 파일 크기이다 (여러 파일 업로드 시 총 크기로 보면 된다) \\ 디폴트 값: 제한없음\n- fileSizeThreshold - 업로드하는 파일이 임시로 파일로 저장되지 않고 메모리에서 바로 스트림으로 전달되는 크기의 한계를 나타낸다\n    - 디폴트 값: 0\n    - ex. 1024 \\ 1024 = 1MB 설정하면 파일이 1MB이상인 경우만에만 임시 파일로 저장된다 JavaConfig 설정 1. StandardServletMultipartResolver 빈을 스프링에 등록 2. Multipart-config 설정을 MultipartConfigElement 객체로 등록 2.1 (다른 방법) @MultipartConfig 어노테이션으로 파일 업로드 제한 설정\n커스텀 서블릿을 생성하고 @MultipartConfig 어노테이션으로 multipart-config을 설정할 수 있습니다. 이 설정은 자세히 다루지 않겠습니다. 추가 설명은 @MultipartConfig❲Servlet 3.x❳ 블로그 를 참고해주세요. 3.1.2 CommonsMultipartResolver 사용시 설정 StandardServletMultipartResolver와 다르게 CommonsMultipartResolver 리졸뷰 사용 시 추가로 pom.xml 파일에 commons-fileupload 라이브러리를 추가해야 합니다. Dependency 추가 스프링 설정은 Bean 정의 파일이나 JavaConfig으로 설정 가능합니다. Bean XML 정의 파일 설정 스프링 설정에 CommonsMultipartResolver 빈을 등록합니다. JavaConfig 설정 4. 파일 업로드 예제들 지금까지는 스프링 관련 설정을 다루었습니다. 이제 뷰와 컨트롤러 단에서 어떻게 파일을 업로드할 수 있는지 알아보겠습니다. 파일을 업로드하는 방법에는 여러 가지가 있습니다. 예제를 통해서 서로 다른 점도 확인하겠습니다. 4.1 단일 파일 업로드 단일 파일을 업로드하는 예제입니다. 뷰는 HTML의 input 태그 file 속성으로 작성하여 form 방식으로 파일 업로드를 할 수 있습니다. form 태그의 enctype 속성은 multipart/form-data로 세팅하여 브라우져가 파일 업로드 방식으로 동작하도록 설정합니다. enctype 속성 값 목록 | 값                                | 설명                                               |\n| --------------------------------- | -------------------------------------------------- |\n| multipart/form-data               | 파일 업로드시 사용 (인코딩 하지 않음)              |\n| applicationx-www-formurlencoded | 디폴트값으로 모든 문자를 인코딩                    |\n| text/plain                        | 공백은 + 기호로 변환함. 특수문자는 인코딩하지 안함 | 브라우져에서의 뷰 화면 컨트롤러에서 업로드한 파일은 MultipartFile 변수를 사용하여 전달받습니다. MultipartFile 클래스는 파일에 대한 정보(파일 이름, 크기등)와 파일 관련 메서드(ex. 파일 저장)를 제공합니다. 대표적으로 사용하는 메서드는 다음과 같고 더 자세한 사항은 API를 참고해주세요. - transferTo() : 파일을 저장한다\n- getOriginalFilename() : 파일 이름을 String 값으로 반환한다\n- getSize() : 파일 크기를 반환한다\n- getInputStream() : 파일에 대한 입력 스트림을 얻어온다 Request parameter로 넘겨주는 파일 이름이 mediaFile로 넘겨줘서 @RequestRaram(“mediaFile”) 어노테이션으로 지정하였습니다. 그리고 파일이 빈파일이 아니면 정해진 다운로드 경로에 저장하고 결과 메시지는 fileUploadForm 뷰에 Model 클래스를 통해서 전달합니다. > 번외 Tips\n> Rest API 구현을 할 때 데이터 형식을 JSON으로 많이 사용해서 클라이언트와 서버 간에 주고받습니다. JSON 타입으로 주고받을때 기본적으로 encoding을 해서 보냅니다. 보내는 데이터가 적은 양이면 문제가 되지 않지만, 대용량의 JSON인 경우에는 성능에 큰 영향을 주게 됩니다. 이런 경우에 JSON 데이터를 stream으로 보내고 컨트롤러에서 MultipartFile로 받으면 스트림형식으로그냥 받기 때문에 성능이 많이 좋아집니다. 프로젝트 진행 시고려해볼 만한 부분입니다. 4.2 다중 파일 업로드 여러 파일을 업로드하는 방식은 단일 파일 업로드 예제와 매우 유사합니다. 차이점은 뷰에서 multiple 속성을 추가하여야사용자가 여러 파일을 선택할 수 있도록 해야 합니다. 브라우져에서의 뷰 화면 컨트롤러에서는 MultipartFile 변수를 배열로 선언하여 여러 파일을 받을 수 있도록 합니다. 배열을 loop으로 돌면서 각 파일을 정해진 경로에 저장합니다. MultipartFile 클래스에서 제공하는 transferTo 메서드로 저장할 수도 있지만, 직접 OutpuStream 클래스로 파일을 저장할 수도 있습니다. 4.3 파일 업로드 + 추가 정보 by @RequestParam 이번 예제는 파일과 다른 입력 정보를 같이 보내는 예제입니다. 추가 입력을 받을 수 있도록 form을 수정합니다. 브라우져에서의 뷰 화면 컨트롤러에서는 각 변수에 @RequestParam 어노테이션을 선언하여 입력 데이터를 개별로 받을 수 있게 합니다. 4.4 파일 업로드 + 추가 정보 by @ModelAttribute 4.3 예제에서는 입력 데이터를 개별 변수에 저장하는 방식이었습니다. 하지만, 입력하는 데이터가 많을 때는 메서드인자가 많이 늘어나는 단점이 있습니다. @ModelAttribute어노테이션을 이용하면 입력한 데이터를 클래스로 한 번에 매핑 할 수 있습니다. 클래스 매핑을 위해 Form에서 입력하는 데이터 이름과 같은 변수를 포함하는 클래스를 아래와 같이 생성합니다. 사용자 입력 데이터를 받을 클래스 MediaVO를 인자로 선언합니다. 업로드 파일과 입력 데이터는 MediaVO에 포함되어 있어서 원하는 데이터를 getter로 가져와 로직에서 사용합니다. 지금까지 스프링을 이용해서 파일 업로드를 알아보았습니다. 다음 시간에는 스프링 컨트롤러 예외처리에 대해서 알아보겠습니다. 5. 참고 - File Upload with Spring\n    - https://www.baeldung.com/spring-file-upload\n    - http://ktko.tistory.com/entry/Spring-단일파일-다중파일-업로드하기\n   ",
    "category": "spring",
    "tags": [
      "web",
      "spring",
      "file",
      "upload",
      "springboot",
      "파일업로드",
      "스프링"
    ],
    "date": "2019-01-02T00:00:00.000Z"
  },
  {
    "id": "spring/스프링부트-기본-오류-페이지-변경하기",
    "slug": "spring/스프링부트-기본-오류-페이지-변경하기",
    "title": "스프링부트 기본 에러 페이지 변경하기 - Customize Whitelabel Error Page",
    "excerpt": "",
    "content": "1. 들어가며 존재하지 않는 API를 접속하게 되면 아래와 같은 Whitelabel Error Page를 자주 접하게 됩니다. 별도 설정을 하지 않았다면 스프링부트에서는 기본적으로 Whitelabel Error Page를 보여줍니다. <img src=\"image-20200905170917741.png\" style=\"zoom: 67%;\" /> 오류 처리 관련해서 어떤 처리가 기본적으로 되어 있는지 어떻게 변경을 할 수 있는지 알아보죠. 1.1 BasicErrorController - 기본 오류처리 컨트롤러 스프링부트에서 BasicErrorController가 이런 기본적인 오류처리를 담당합니다. 에서 를 설정하지 않았다면 가 기본 오류처리 PATH 주소로 지정됩니다. 1.1.1 Whitelabel error page 브라우져에서 접속하면 Whitelabel Error Page 보여줍니다. Request 해더에 Accept 속성 값이 인 경우에는 아래 코드가 실행되면서 오류 페이지를 뷰로 반환해주고 있습니다. 1.1.2 Json 응답 Accept 값이 인 경우에는 응답 값을 JSON 형태로도 내려줍니다. Json 응답 값은  메서드에서 처리하여 응답 값을 내려줍니다. > 응답 값은  메서드에서 채워주고 있습니다. 2. Custom Error 페이지에 대한 처리 2.1 Error 관련 Properties 서버 오류 관련 설정은 아래와 같습니다. | 키 값               | 기본 값  | 설명     |\n| -------------------------- | -------- | -------- |\n|  |   | 바인딩 오류를 포함시킬 때|\n|       |   | 예외 내용을 포함시킬 때 |\n|         |   | 오류 메시지를 포함시킬 때 |\n|      |   | stacktrace를 포함시킬 떄 |\n|                    |  | 오류 처리할 컨트롤러 패스   |\n|      |    | 브라우져에서 오류 페이지를 보여줄 지 결정한다. <br />false로 지정하면 tomcat의 오류 페이지로 로딩이 된다 | 2.2 특정 응답코드에 대한 Custom Error 페이지 만들기 Custom Error 페이지 만들어서 사용하는 건 간단합니다. 아래 폴더중에 한 곳에  형식으로 파일을 생성하면, 스프링 부트에서 Http 상태 값에 다라서 해당 파일을 로딩해줍니다. - 폴더\n    - \n    - \n- 파일\n    - \n        - 400번 대의 모드 상태 코드 발생시 이 파일로 로딩이 된다\n    - \n        - Http 상태 코드가 404인 경우에 이 파일이 로딩이 된다 본 포스팅에서는 Mustache를 View Template Engine으로 사용하였고  폴더에 404와 5xx에 해당하는 파일을 생성했습니다.  파일을 작성합니다. 브라우져에서 존재하지 않는 path로 접속을 하면 404 응답 오류가 발생하여 위 view 파일이 응답으로 처리됩니다. 2.3 별도 ErrorController 를 생성하기 위와 같이 특정 응답 코드에 대해서 뷰 파일을 생성하는 방식은 특정 로직을 수행할 수 없는 단점이 있습니다. 이런 경우에 Custom Error Controller를 생성하여  PATH에 대한 호출은 이 컨트롤러에서 처리하도록 할 수 있습니다. 에서는  뷰를 반환합니다. 404 오류가 발생하면, 별도 뷰를 보여줍니다. 4. 마치며 Whitelabel Error Page가 어떻게 로딩이 되는지 스프링부트의 내부 코드를 간단하게 살펴보았고 어떻게 오류 처리를 다르게 변경할 수 있는지도 알아보았습니다. 전체 소스 코드는 github를 참고해주세요. 5. 참고  스프링 부트 오류 처리\n     https://www.baeldung.com/spring-boot-custom-error-page\n     https://velog.io/@godori/spring-boot-error\n     https://goddaehee.tistory.com/214\n     https://supawer0728.github.io/2019/04/04/spring-error-handling/\n 스프링 부트 속성 목록\n     https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html",
    "category": "spring",
    "tags": [
      "java",
      "java8",
      "spring",
      "exception",
      "error",
      "자바",
      "자바8",
      "스프링부트",
      "스프링",
      "오류"
    ],
    "date": "2020-06-03T00:00:00.000Z"
  },
  {
    "id": "spring/자주-접하는-게-되는-http-status-code",
    "slug": "spring/자주-접하는-게-되는-http-status-code",
    "title": "자주 접하는 게 되는 Http Status Code",
    "excerpt": "",
    "content": "API 개발 시 접하게 되는 HTTP 상태 코드를 정리해보았다. 이외에도 더 많겠지만, 한 번쯤 들어보고 접한 응답 코드들이다. HTTP 상태 값에 따라 고양이 이미지를 보여주는 사이트도 존재한다. 이 개발자는 고양이를 무척 좋아하나보다. - HTTP Cats API Http Status Code 1xx (조건부 응답)\n- 개인적으로 아직까지는 접해보지 못한 응답 값이다 2xx (성공) : 이 상태 코드 값들은 요청 처리가 성공적인 경우에 응답 값을 내려준다.\n- 200 ( 성공)\n- 201 ( 생성됨)\n  - 요청이 처리되어 새로운 자원을 잘 생성한 경우에 내려준다 3xx (리다이렉션) : 이 상태 코드 값은 클라이언트가 요청을 완료하기 위해 추가 조치를 취해야 함을 나타낸다. 그리고 대부분의 3xx 상태 코드들은 URL 리다이렉션에 사용된다.\n- 301 ( 영구 이동)\n   - 요청한 페이지를 새 위치로 영구적으로 이동시킬 때 사용한다\n   - GET 또는 HEAD 요청에 대한 응답에 이 응답 값이면 클라이언트가 자동으로 새 위치로 이동시킨다\n    4xx (클라이언트 오류) : 이 상태 코드 값은 클라이언트측 오류에 의해서 발생한 경우에 내려준다\n- 400 ( 잘못된 요청)\n- 401 ( 권한 없음)\n  - 요청시 인증이 필요한 경우에 발생하고 인증이 안된 경우에 내려준다 - 403 ( 금지됨)\n  - 해당 요청이 서버에 의해서 거부된 경우이다. 요청하는 사용자에게 권한이 없어서 발생할 수 있다. - 404 ( 찾을 수 없음)\n- 409 ()\n  - 충돌이 발생하여 요청을 처리할 수 없는 경우에 응답 값으로 내려준다 5xx (서버 오류) : 이 응답 코드 값은 서버에서 요청을 처리할 수 없는 경우에 내려준다\n- 500 ( 내부 서버 오류)\n  - 일반적인 서버 오류에 사용한다 - 501 ( 구현되지 않음)\n- 502 ( 불량 게이트웨이)\n- 503 ( 서비스를 사용할 수 없음) \n  - 서버는 실행중이지만, 많은 requests로 인해서 overloaded된 경우에 사용한다\n- 504 ( 게이트웨이 시간초과) 참고 - Http Status Code\n    - https://http.cat/\n    - https://ko.wikipedia.org/wiki/HTTP%EC%83%81%ED%83%9C%EC%BD%94%EB%93%9C\n- Error 발생시\n    - https://www.baeldung.com/rest-api-error-handling-best-practices\n    - https://developer.twitter.com/en/docs/basics/response-codes",
    "category": "spring",
    "tags": [
      "http",
      "status",
      "code",
      "statuscode",
      "상태값",
      "응답값",
      "상태코드"
    ],
    "date": "2020-09-12T00:00:00.000Z"
  },
  {
    "id": "web/postman-flows-이란",
    "slug": "web/postman-flows-이란",
    "title": "Postman Flows이란",
    "excerpt": "",
    "content": "1.Postman Flows이란? Postman Flows는 여러 타입의 빌딩 블록을 서로 연결하여 작업 흐름을 정의하고 자동화하는 데 사용되는 도구이다. Flows는 코드 한 줄도 작성하지 않고 작업 흐름을 정의할 수 있는 UI를 제공해 주고 있어서 개발자 외에도 누구나? 쉽게 Flows를 사용할 수 있다. 다음은 Postman Flows 의 주요 특징 및 기능에 대해서 알아보자. 참고 - Learning Center: Postman Flows\n- Flow Snippets\n- Postman Flows - Youtube Video Playlist\n- Various Postman Flows Usecase Example > Postman에서는 다양한 형태의 문서를 제공해 주고 있어 쉽게 Postman Flows를 학습할 수 있다.\n> 1.1 Postman Flows 특징 및 기능 - 기본적으로 Postman에서 제공하는 여러 Collection, Environment 등의 값을 Flows 내에서 바로 사용할 수 있다\n- 여러 타입의 Flow Blocks을 제공한다\n    - ex. 데이터 정보 생성/필터, 조건, 반복, 실행, 결과\n    - 정보 결과를 ex. JSON, charts, table, video, images 등과 같은 형태로 보여줄 수 있다\n- Evaluate block에서 작성할 수 있는  () 을 AI prompt를 통해서 작성 가능하다\n    - 아직  버전이라 chatGPT 만큼의 성능은 보여주지 못하는 듯하다\n- Flow를 배포하여 클라우드에서 실행할 수 있어 다른 애플리케이션과도 연동할 수 있다 1.1.1 Flow Blocks Flow에서는 아래와 같은 여러 block 타입을 제공한다. - Information blocks\n    - ,  (ex. Map), , , etc\n    - \n        - JSON 결과 특정 값을 선택할 수 있는 필터 역할을 한다\n    - \n        - Flow기 실행될 때 실행되는 첫 번째 block이다 - Decision blocks\n    - \n        -   condition 값에 따라서 Data 값으로 넘겨진 값을 다음 block으로 전달한다\n    - \n        -   를 실행해서 결과를 다음 block으로 전달할 수 있다 - Repeating blocks\n    - \n        - 입력한 값만큼 반복해서실행한다. ex 입력값:  -> \n    - \n        - ,  list에 있는 데이터를 하나씩 다음 block으로 전달한다\n    - \n        - ,  의 하나의 값을 받아서 새로운 list를 생성하여 다음 block으로 전달한다\n- Action blocks\n    - \n        - 입력한 delay (ns ms) 만큼 기다렸다가 실행한다\n    - \n        - Postman Collection에 있는 request 를 실행한다\n- Output blocks\n    - \n        - Postman console에 출력된다\n    - \n        - JSON, charts, table, image, videos 타입과 같은 형태로 정보를 출력할 수 있다 참고 - Postman Flows blocks\n- Flow Snippets 1.1.2 Flows Query Language (FQL)  ()을 사용하여 JSON 데이터를 파싱하고 JSON 데이터를 변환하여 원하는 필드나 구조를 가져올 수 언어이다. - FQL으로 할 수 있는 작업\n    - 기본 값 가져오기 (ex. nested field, specific index)\n    - 조건부 데이터 선택 (ex. 필터링)\n    - 구조화된 데이터 반환 (ex. 여러 데이터를 조합해서 array 반환)\n    - 데이터 조작 (ex. string의 길이)\n- Prompt 기능 제공하여 직접 FQL를 작성하지 않고 Prompt의 도움을 받아서 작성할 수 있다\n    - Alpha 버전이라서 그런지 실제로 사용해보면 복잡한 건 제대로 작성을 하지 못한다\n- 여러 FQL 함수를 실행하려면 아래와 같이 작성해서  실행하면 순차적으로 실행할 수 있다 참고 - Introduction to Flows Query Language\n- FQL function reference\n- Advanced FQL expressions in Postman Flows 1.1.2 Organize a Flow Flow에서 block이 많아지면 복잡해져서 아래와 같은 기능을 통해서 작성된 Flow를 조금 더 쉽게 이해할 수 있도록 도와주는 듯하다. - Colors\n    - Block을 선택해서 다른 색을 지정할 수 있다\n- Annotation\n    - 텍스트를 입력해서 추가로 설명을 달 수 있다\n- Grouping\n    - 여러 block을 grouping 해주는 기능이다 1.1.3 Webhook 기능 Flow를 클라우드에 배포해서 Webhook으로 트리거하여 Flow를 실행시킬 수 있다. 아래와 같이 Flow를 Webhook으로 생성하면 API 주소가 나오고 API를 호출하면 Flow가 트리거할 수 있다. 참고 - Publish a Flow to the Postman cloud 2.Postman Flows 사용해보기 Examples - Concatenating Strings\n- Condition (If...then.else)\n- https://www.postman.com/postman/workspace/flows-snippets/flow/641784c895e5e70033f029ad 3.FAQ 3.1 Postman에서 변수는 어디에 어떻게 저장할 수 있나? Postman에서 변수는 여러 곳, Global, Collection, Environment 등에서 변수를 저장하여 사용할 수 있다. 변수 참조시 Postman에서 아래 스코프 기준으로 참조할 변수를 찾는다. 3.1.1 Postman Variables Scope - Global\n    - Global 변수는 전역 변수로 어디서나 사용 가능한다\n    - ex. Collection, Envrionment, Request, Test Script\n- Collection\n    - Collection 변수는 Collection의 Request 전체에서 사용할 수 있고 Environment과는 무관한다\n    - 환경이 하나인 경우에는 Collection 변수를 사용하는 게 적합하다\n- Environment\n    - Environment 변수르르 사용하면 로컬 개발 환경, 테스트, 프로덕션 환경 등 다양한 환경으로 작업 범위를 지정할 수 있다\n- Data\n    - Data 변수는 newman 이나 Collection Runner를 실행할 때 사용할 수 있는 데이터 집합을 정의할 기 위해 외부 CSV, JSON 파일에서 가져온다\n    - Data  변수는 현재 값을 가지며, 요청 또는 컬렉션 실행 이후에는 지속되지 않는다\n- Local\n    - Local 변수는 Test Script에서 생성하는 임시 변수이다 참고: Store and reuse values using variables 3.2 Initial와 Current 값의 차이점은? - Initial value\n    - Initial 값은 Collection, Environment, Global에서 설정된 값이다. 이 값은 Postman의 서버와 동기화되면, 해당 요소를 공유할 때 팀과 공유가 된다\n    - 초기 값은 팀원들과 공동 작업할 때 유용할 수 있습니다.\n- Current value\n    - Request 을 보낼 때 현재 값이 사용된다. 이 값은 로컬 값이며 Postman 서버에 동기화되지 않는다\n    - 현재 값을 변경하면 원래 공유 컬렉션, 환경 또는 전역에 유지되지 않는다 3.3 Flow를 주기적으로 실행 할 수는 없나?  자체에서는 제공하지 않지만, Postman 에서 제공하는 Monitor 기능을 통해서 주기적으로 실행할 수 있다. - 작성한 Flow를 Cloud에 배포한다\n- 새로운 Collection 생성하고 Post Request를 생성하고 Cloud에 배포해서 얻은 Flow Webhook 주소를 입력한다\n- Postman Monitor에서 주기적으로 실행할 Collection을 선택한다 참고: Scheduling the Flow with a monitor 3.4 작성한 Flow를 다른 Flow에 사용할 수 없나? Postman Flow UI 상에서는 flow 간에 연결해서 사용할 수는 없습니다. 단, 특정 Flow를 Cloud에 배포해서 다른 Flow에서 API로 호출해서 다른 Flow를 호출할 수 있습니다. 3.5 Postman Flows는 언제 release 되었나? - 정확한 날짜는 확인이 안되고 Eary Access는 2022년 말쯤에 릴리스된 것으로 판단됨\n    - https://blog.postman.com/announcing-postman-flows-early-access/ 3.6 FQL는 표준화 언어인가? - 는 표준화된 쿼리 언어는 아니고 Postman 에서만 사용하는 자체 개발된 쿼리 언어이다 4.마무리 장점 - Postman에서 기본적으로 사용하는 API Collection, Environment 등을 Flows에서 바로 사용할 수 있음\n- Git 기반의 Fork를 사용해서 다른 사람이 작성한 여러 Flow, Collecttion을 Fork해서 테스트해볼 수 있어서 좋았음\n- 간단한 로직인 경우에는 비개발자가 flow를 사용해서 작성해서 자동화나 통합 테스트도 가능해짐 아쉬운 점 - UI 상으로 Flow 간의 연결해서 사용할 수가 없음\n    - 가능한 해결책은 Flow을 Cloud 배포해서 API로 호출해서 trigger하는 방법이 있음\n    - Cloud에 배포된 Flow 실행 시 디버깅이 쉽지 않음 - 로컬에서만 디버깅이 되는 듯함\n- Flow가 길어지면 전체 보기가 좀 어려움이 있음\n    - grouping 한 부분을 folding으로 숨기는 기능 같은 게 있으면 좋을 듯함\n- 로직이 좀 복잡한 경우에 flow의 FQL를 사용해도 좀 답답한 면이 있었음\n    - 아직 flow로 ",
    "category": "web",
    "tags": [
      "postman",
      "flows",
      "api",
      "automation",
      "플로우",
      "통합 테스트",
      "테스트",
      "자동화"
    ],
    "date": "2023-10-27T00:00:00.000Z"
  },
  {
    "id": "web/구글-시트에서-사용자-정의-함수-구현하기",
    "slug": "web/구글-시트에서-사용자-정의-함수-구현하기",
    "title": "구글 시트에서 사용자 정의 함수 구현하기",
    "excerpt": "",
    "content": "투자 스터디를 하면서 주식 투자 매매일지를 구글 시트로 작성하고 있는데 Google Apps Script를 유용하게 사용하게 되어 간단하게 정리합니다. Google Finance 함수란? Apps Script에 언급하기 전에 먼저 Google Finance 함수를 알아보자. 이 함수는 구글 시트에서 사용할 수 있는 내장 함수 중에 하나로, 실시간 주식 시세 데이터를 가져올 수 있다. 셀에  이렇게 입력하면 현재 애플 주식 시세를 가져와 셀로 표시해준다. 아래 그림처럼 현재 주가를 확인해서 매수/매도 할지 판단하는 데 사용하고 있다. Google Finance 함수는 대부분의 주식 세시 데이터를 제공하고 있지만, 없는 경우도 있다. 예를 들면, 금현물 시세는 제공하고 있지 않다. 사용자 정의 함수를 구현하려면? Google Finance 함수에서는 금현물 시세를 제공하지 않아 다른 API를 통해서 데이터를 가져와 셀에 입력할 수 있도록 사용자 정의 함수를 구현해야 한다. 금현물 시세 정보를 가져오는 API는 RapidAPI Stock-API 의 API를 사용한다. 1.Apps Script 작성하기 Google Apps Script는 Google 여러 서비스 (ex. Google Sheets, Docs, Gmail 등)를 자동화하고 확장 가능하도록 JavaScript 플랫폼을 제공하여 구글 서비스와 상호 작용이 가능하다. 구글 시트에서 Apps Script를 작성하려면,  >  클릭해서 실행한다. 아래 코드를 작성하고 실행 버튼을 클릭하면 최소 한번은 인증 절차를 거쳐야 한다. > '확인되지 않은 앱' 경고 창이 뜨면 \"고급 설정\" 클릭하고 \"....로 이동(안전하지 않음\")을 클릭해서 전급 허용을 해준다. 2.사용자 정의 함수 사용 구글시트에서 아래와 같이 입력을 하면 금현물 시세 값이 표시되는 것을 확인할 수 있다. 참고 - https://developers.google.com/apps-script/reference/url-fetch/url-fetch-app?hl=ko\n- https://www.youtube.com/watch?v=k0su6345KDI&t=828s\n- https://support.google.com/docs/answer/3093281?hl=en",
    "category": "web",
    "tags": [
      "google",
      "excel",
      "sheet",
      "api",
      "external",
      "구글",
      "구글시트",
      "시트",
      "엑셀",
      "apps script",
      "UrlFetchApp",
      "custom",
      "function",
      "사용자정의",
      "함수",
      "스트립트",
      "편집기"
    ],
    "date": "2023-03-05T00:00:00.000Z"
  },
  {
    "id": "web/하나의-구글-계정으로-여러-이메일-주소-사용하기",
    "slug": "web/하나의-구글-계정으로-여러-이메일-주소-사용하기",
    "title": "하나의 구글 계정으로 여러 이메일 주소 사용하기",
    "excerpt": "",
    "content": "하나의 구글 이메일 계정으로 여러 이메일 주소를 사용할 수 있다는 걸 알고 계셨나요? AWS 계정은 12월간 무료로 사용할 수 있어 개인적으로 매년 AWS 계정을 다시 생성할 때 주로 구글의 별칭 기능을 사용하고 있다. 별도의 설정도 필요 없고 아래와 같이 점이나 플러스 사인을 추가하면 끝이다. 이메일 별칭 (alias) 기능 사용하기 구글의 alias 기능은 , 을 사용자 이름에 추가해서 이메일을 보내면 원래 구글 메일 계정으로 받을 수 있다. - 구글 이메일 주소: frank@gmail.com 1. 사용자 이름에 점 (dot) 사용하기 점은 아래와 같은 형식으로 추가할 수 있다. - f.rank@gmail.com\n- fran.k@gmail.com\n- f.r.ank@gmail.com 2. 사용자 이름에 플러스 (+) 사용하기 - frank+aws1@gmail.com\n- frank+aws2@gmail.com\n- frank+us@gmail.com 위 모든 메일 주소는 모두 frank@gmail.com의 메일로 가게 된다. 참고 - https://www.101domain.com/gmailemailaliases.htm\n- https://blog.jeuke.com/67",
    "category": "web",
    "tags": [
      "google",
      "gmail",
      "mail",
      "email",
      "alias",
      "구글",
      "지메일",
      "중복",
      "aws",
      "별칭"
    ],
    "date": "2023-03-06T00:00:00.000Z"
  }
]